<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>물리 세계 제약 하 VLA 로봇 파운데이션 모델 동향</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script>
    (function () {
      const candidates = [
        '/vendor/mathjax/tex-svg.js',
        './vendor/mathjax/tex-svg.js',
        '../vendor/mathjax/tex-svg.js',
        '../../federnett/vendor/mathjax/tex-svg.js',
        '../../../federnett/vendor/mathjax/tex-svg.js',
        'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js',
      ];
      const head = document.head || document.getElementsByTagName('head')[0];
      let index = 0;
      const loadNext = () => {
        if (window.MathJax && window.MathJax.typesetPromise) return;
        if (index >= candidates.length) return;
        const script = document.createElement('script');
        script.src = candidates[index++];
        script.async = true;
        script.onerror = loadNext;
        head.appendChild(script);
      };
      loadNext();
    })();
  </script>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Fraunces:wght@300;500;700&family=Space+Grotesk:wght@400;600;700&family=JetBrains+Mono:wght@400;600&display=swap');
    :root {
      --bg: #0b0f14;
      --bg-2: #121821;
      --card: rgba(255, 255, 255, 0.06);
      --site-ink: #f5f7fb;
      --site-muted: rgba(245, 247, 251, 0.65);
      --accent: #4ee0b5;
      --accent-2: #6bd3ff;
      --edge: rgba(255, 255, 255, 0.15);
      --glow: rgba(78, 224, 181, 0.25);
      --ink: #0b1220;
      --muted: #425066;
      --accent-strong: #2fb892;
      --paper: rgba(255, 255, 255, 0.94);
      --paper-strong: #ffffff;
      --paper-alt: rgba(240, 245, 255, 0.6);
      --rule: rgba(15, 23, 42, 0.12);
      --shadow: 0 28px 70px rgba(15, 23, 42, 0.22);
      --chrome-ink: #f5f7fb;
      --chrome-muted: rgba(245, 247, 251, 0.66);
      --chrome-surface: rgba(11, 17, 29, 0.62);
      --chrome-border: rgba(255, 255, 255, 0.16);
      --chrome-hover-soft: rgba(255, 255, 255, 0.06);
      --chrome-hover: rgba(255, 255, 255, 0.08);
      --link: var(--accent-2);
      --link-hover: var(--accent);
      --page-bg: radial-gradient(1200px 600px at 12% -10%, var(--glow), transparent 60%),
        radial-gradient(900px 540px at 92% 8%, rgba(107, 211, 255, 0.18), transparent 55%),
        linear-gradient(180deg, #0b111d 0%, var(--bg-2) 45%, var(--bg) 100%);
      --body-font: "Fraunces", "Charter", Georgia, serif;
      --heading-font: "Space Grotesk", "Segoe UI", sans-serif;
      --ui-font: "Space Grotesk", "Segoe UI", sans-serif;
      --mono-font: "JetBrains Mono", "Consolas", monospace;
    }
    :root[data-theme="sky"] {
      --bg: #0b1220;
      --bg-2: #0f1b2e;
      --card: rgba(255, 255, 255, 0.06);
      --site-ink: #f4f7ff;
      --site-muted: rgba(244, 247, 255, 0.62);
      --accent: #64b5ff;
      --accent-2: #8fd1ff;
      --edge: rgba(255, 255, 255, 0.18);
      --glow: rgba(100, 181, 255, 0.28);
      --accent-strong: #3f8ed1;
    }
    :root[data-theme="crimson"] {
      --bg: #120a0d;
      --bg-2: #1c0f16;
      --card: rgba(255, 255, 255, 0.06);
      --site-ink: #fff5f7;
      --site-muted: rgba(255, 245, 247, 0.62);
      --accent: #ff6b81;
      --accent-2: #ff9aa9;
      --edge: rgba(255, 255, 255, 0.15);
      --glow: rgba(255, 107, 129, 0.25);
      --accent-strong: #e3546d;
    }
    * { box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    body {
      margin: 0;
      min-height: 100vh;
      color: var(--site-ink);
      background: var(--page-bg);
      font-family: var(--body-font);
      line-height: 1.7;
      letter-spacing: -0.01em;
      overflow-x: hidden;
    }
    .backdrop {
      position: fixed;
      inset: 0;
      pointer-events: none;
      z-index: 0;
      overflow: hidden;
    }
    .orb {
      position: absolute;
      border-radius: 999px;
      opacity: 0.6;
      mix-blend-mode: screen;
      filter: blur(0px);
      animation: float 16s ease-in-out infinite;
    }
    .orb-1 {
      width: 520px;
      height: 520px;
      background: radial-gradient(circle at 30% 30%, rgba(255, 122, 89, 0.55), transparent 60%);
      top: -220px;
      left: -160px;
    }
    .orb-2 {
      width: 440px;
      height: 440px;
      background: radial-gradient(circle at 60% 40%, rgba(14, 165, 164, 0.5), transparent 62%);
      top: 80px;
      right: -120px;
      animation-delay: -4s;
    }
    .orb-3 {
      width: 340px;
      height: 340px;
      background: radial-gradient(circle at 50% 50%, rgba(148, 163, 184, 0.35), transparent 70%);
      bottom: -180px;
      left: 22%;
      animation-delay: -8s;
    }
    .page {
      position: relative;
      z-index: 1;
      max-width: 1040px;
      margin: 56px auto 96px;
      padding: 0 28px;
    }
    .toc-sidebar {
      display: none;
      position: fixed;
      left: max(18px, calc(50% - 700px));
      top: 64px;
      width: 250px;
      max-height: calc(100vh - 96px);
      overflow-y: auto;
      padding: 16px 12px;
      border-radius: 14px;
      border: 1px solid var(--chrome-border);
      background: var(--chrome-surface);
      backdrop-filter: blur(8px);
      z-index: 2;
    }
    .toc-title {
      font-family: var(--ui-font);
      font-size: 0.78rem;
      letter-spacing: 0.2em;
      text-transform: uppercase;
      color: var(--chrome-muted);
      margin-bottom: 10px;
    }
    .toc-nav {
      display: flex;
      flex-direction: column;
      gap: 4px;
    }
    .toc-item {
      display: block;
      color: var(--chrome-ink);
      text-decoration: none;
      font-family: var(--ui-font);
      font-size: 0.88rem;
      line-height: 1.35;
      opacity: 0.82;
      border-left: 2px solid transparent;
      padding: 6px 8px;
      border-radius: 8px;
      transition: all 0.18s ease;
    }
    .toc-item:hover {
      opacity: 1;
      border-left-color: var(--accent);
      background: var(--chrome-hover-soft);
    }
    .toc-item.active {
      opacity: 1;
      border-left-color: var(--accent);
      background: var(--chrome-hover);
    }
    .toc-sub {
      margin-left: 12px;
      font-size: 0.82rem;
      opacity: 0.75;
    }
    body.layout-sidebar_toc .page {
      max-width: 980px;
    }
    @media (min-width: 1300px) {
      body.layout-sidebar_toc .toc-sidebar {
        display: block;
      }
      body.layout-sidebar_toc .page {
        margin-left: max(300px, calc(50% - 370px));
        margin-right: 56px;
      }
    }
    .masthead {
      display: flex;
      flex-direction: column;
      gap: 12px;
      padding: 18px 22px 22px;
      border: 1px solid var(--masthead-border, rgba(226, 232, 240, 0.18));
      border-radius: 18px;
      background: var(--masthead-bg, rgba(10, 14, 20, 0.55));
      backdrop-filter: blur(8px);
      margin-bottom: 36px;
      color: var(--masthead-text, #f8fafc);
      animation: fadeIn 0.7s ease-out both;
    }
    .masthead-top {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 16px;
    }
    .kicker {
      font-family: var(--ui-font);
      font-size: 0.82rem;
      letter-spacing: 0.28em;
      text-transform: uppercase;
      color: var(--masthead-kicker, rgba(255, 255, 255, 0.68));
    }
    .back-link {
      display: none;
      align-items: center;
      gap: 8px;
      font-family: var(--ui-font);
      font-size: 0.78rem;
      text-decoration: none;
      padding: 6px 12px;
      border-radius: 999px;
      border: 1px solid var(--masthead-link-border, rgba(255, 255, 255, 0.2));
      color: var(--masthead-link, rgba(255, 255, 255, 0.78));
      background: var(--masthead-link-bg, rgba(15, 23, 42, 0.35));
      transition: all 0.2s ease;
    }
    .back-link:hover {
      color: #fff;
      border-color: rgba(255, 255, 255, 0.45);
      transform: translateY(-1px);
    }
    .report-title {
      font-family: var(--heading-font);
      font-size: clamp(2.2rem, 3.6vw, 3.6rem);
      margin: 0;
      line-height: 1.08;
      letter-spacing: -0.03em;
      color: var(--masthead-title, #f8fafc);
    }
    .report-deck {
      color: var(--masthead-deck, rgba(226, 232, 240, 0.8));
      font-size: 1.05rem;
      max-width: 720px;
    }
    .article {
      background: var(--paper);
      color: var(--ink);
      border: 1px solid rgba(255, 255, 255, 0.6);
      border-radius: 22px;
      padding: 40px 44px;
      box-shadow: var(--shadow);
      backdrop-filter: blur(8px);
      animation: rise 0.8s ease-out both;
    }
    .article > * { animation: rise 0.6s ease-out both; }
    .article > *:nth-child(1) { animation-delay: 0.05s; }
    .article > *:nth-child(2) { animation-delay: 0.1s; }
    .article > *:nth-child(3) { animation-delay: 0.15s; }
    .article > *:nth-child(4) { animation-delay: 0.2s; }
    .article > *:nth-child(5) { animation-delay: 0.25s; }
    .article h1, .article h2, .article h3, .article h4 {
      font-family: var(--heading-font);
      color: var(--ink);
    }
    .article h1 { font-size: 2rem; margin-top: 0; }
    .article h2 {
      font-size: 1.55rem;
      margin-top: 2.6rem;
      padding-top: 1rem;
      border-top: 1px solid var(--rule);
      position: relative;
      padding-left: 18px;
    }
    .article h2::before {
      content: '';
      position: absolute;
      left: 0;
      top: 1.45rem;
      width: 8px;
      height: 8px;
      border-radius: 999px;
      background: var(--accent-strong);
    }
    .article h3 { font-size: 1.2rem; margin-top: 1.7rem; color: #1f2937; }
    .article h2, .article h3 { scroll-margin-top: 92px; }
    .article p { font-size: 1.05rem; }
    .article ul, .article ol { padding-left: 1.4rem; }
    .article blockquote {
      margin: 1.4rem 0;
      padding: 1rem 1.2rem;
      border-left: 3px solid var(--accent);
      background: rgba(15, 23, 42, 0.04);
    }
    .article .misc-block {
      margin: 1.2rem 0 1.4rem;
      padding: 1rem 1.1rem;
      border-radius: 12px;
      border: 1px solid rgba(15, 23, 42, 0.14);
      background: rgba(148, 163, 184, 0.1);
    }
    .article .misc-block ul {
      margin: 0;
      padding-left: 1.2rem;
    }
    .article .misc-block li {
      margin: 0.35rem 0;
      color: var(--muted);
      font-size: 0.98rem;
    }
    .article .misc-block.ai-disclosure {
      border-color: var(--accent);
      background: rgba(78, 224, 181, 0.08);
    }
    .article .misc-block.ai-disclosure p {
      margin: 0 0 0.55rem;
      color: var(--ink);
      font-family: var(--ui-font);
      letter-spacing: 0.01em;
    }
    .article a {
      color: var(--link);
      text-decoration: none;
      border-bottom: 1px solid transparent;
      transition: all 0.2s ease;
    }
    .article a:hover { color: var(--link-hover); border-bottom-color: var(--link-hover); }
    .article code {
      font-family: var(--mono-font);
      font-size: 0.94rem;
      background: rgba(148, 163, 184, 0.14);
      padding: 2px 6px;
      border-radius: 6px;
    }
    .article pre {
      background: rgba(148, 163, 184, 0.18);
      padding: 1rem 1.1rem;
      border-radius: 12px;
      overflow-x: auto;
    }
    .article figure.report-figure {
      margin: 1.8rem 0;
      padding: 0;
      width: 100%;
      max-width: 100%;
      overflow: hidden;
    }
    .article figure.report-figure img {
      display: block;
      width: 100%;
      max-width: 100%;
      height: auto;
      max-height: 70vh;
      object-fit: contain;
      border-radius: 14px;
      background: rgba(15, 23, 42, 0.04);
      box-shadow: 0 18px 50px rgba(15, 23, 42, 0.18);
    }
    .article figure.report-figure figcaption {
      margin-top: 0.65rem;
      font-size: 0.95rem;
      color: var(--muted);
    }
    .article figure.report-diagram {
      background: rgba(15, 23, 42, 0.04);
      border: 1px solid rgba(15, 23, 42, 0.1);
      border-radius: 14px;
      padding: 0.9rem;
    }
    .article .mermaid {
      display: flex;
      justify-content: center;
      overflow-x: auto;
      min-height: 40px;
    }
    .article .mermaid svg {
      max-width: 100% !important;
      height: auto;
    }
    .article table { border-collapse: collapse; width: 100%; margin: 1.4rem 0; }
    .article th, .article td { border: 1px solid var(--rule); padding: 10px 12px; }
    .article th { background: rgba(148, 163, 184, 0.15); text-align: left; }
    .article tr:nth-child(even) td { background: rgba(148, 163, 184, 0.08); }
    .article hr { border: none; border-top: 1px solid var(--rule); margin: 2rem 0; }
    .viewer-overlay {
      position: fixed;
      inset: 0;
      background: rgba(15, 23, 42, 0.6);
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.2s ease;
      z-index: 20;
    }
    .viewer-overlay.open { opacity: 1; pointer-events: auto; }
    .viewer-panel {
      position: fixed;
      top: 0;
      right: 0;
      height: 100vh;
      width: min(480px, 92vw);
      background: #fff;
      box-shadow: -20px 0 60px rgba(15, 23, 42, 0.25);
      transform: translateX(105%);
      transition: transform 0.25s ease;
      z-index: 30;
      display: flex;
      flex-direction: column;
    }
    .viewer-panel.open { transform: translateX(0); }
    .viewer-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 8px;
      padding: 14px 16px;
      border-bottom: 1px solid #e2e8f0;
      background: #f8fafc;
    }
    .viewer-title { font-size: 0.95rem; color: var(--ink); flex: 1; }
    .viewer-actions { display: flex; gap: 8px; align-items: center; }
    .viewer-actions a {
      font-size: 0.78rem;
      text-decoration: none;
      color: var(--link);
    }
    .viewer-close {
      border: none;
      background: #1f2937;
      color: #fff;
      width: 26px;
      height: 26px;
      border-radius: 999px;
      cursor: pointer;
    }
    .viewer-frame { flex: 1; border: none; width: 100%; border-radius: 0 0 16px 16px; }
    @keyframes fadeIn { from { opacity: 0; transform: translateY(6px); } to { opacity: 1; transform: translateY(0); } }
    @keyframes rise { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
    @keyframes float { 0%, 100% { transform: translateY(0); } 50% { transform: translateY(18px); } }
    body.template-technical_deep_dive {
  --site-ink: var(--ink);
  --site-muted: var(--muted);
  --ink: #1b1d22;
  --muted: #4d545f;
  --accent: #3b5b77;
  --link: #214e75;
  --page-bg: linear-gradient(135deg, #f1f4f8 0%, #f7f4ee 55%, #ffffff 100%);
  --body-font: "Iowan Old Style", "Charter", "Palatino Linotype", "Book Antiqua", Georgia, serif;
  --heading-font: "Franklin Gothic Medium", "Trebuchet MS", "Gill Sans", sans-serif;
  --ui-font: "Franklin Gothic Medium", "Trebuchet MS", "Gill Sans", sans-serif;
}

body.template-technical_deep_dive .article {
  border-radius: 12px;
}

body.template-technical_deep_dive .article h2 {
  border-top: 2px solid var(--rule);
}

body.template-technical_deep_dive .article pre {
  background: #eef2f6;
}
body.template-technical_deep_dive {
  --masthead-bg: #6f7377;
  --masthead-border: rgba(255, 255, 255, 0.35);
  --masthead-title: #f8fafc;
  --masthead-deck: rgba(248, 250, 252, 0.85);
  --masthead-kicker: rgba(248, 250, 252, 0.72);
  --masthead-link: rgba(248, 250, 252, 0.88);
  --masthead-link-border: rgba(248, 250, 252, 0.3);
  --masthead-link-bg: rgba(15, 23, 42, 0.2);
}

  </style>
</head>
<body class="theme-coral template-technical_deep_dive layout-sidebar_toc">
  <div class="backdrop">
    <div class="orb orb-1"></div>
    <div class="orb orb-2"></div>
    <div class="orb orb-3"></div>
  </div>
  <aside class="toc-sidebar" id="toc-sidebar"><div class="toc-title">Contents</div><nav class="toc-nav"><a class="toc-item" href="#executive_summary">Executive Summary</a><a class="toc-item" href="#scope_methodology">Scope &amp; Methodology</a><a class="toc-item" href="#technical_background">Technical Background</a><a class="toc-item toc-sub" href="#physical_ai">Physical AI와 ‘닫힌 루프’의 의미</a><a class="toc-item toc-sub" href="#vla_vision-language-action">VLA(Vision-Language-Action)와 로봇 파운데이션 모델</a><a class="toc-item toc-sub" href="#action_representation_action_chunking">행동 표현(action representation)과 action chunking</a><a class="toc-item toc-sub" href="#flow_diffusion">생성적 정책: flow/diffusion 계열</a><a class="toc-item" href="#methods_data">Methods &amp; Data</a><a class="toc-item toc-sub" href="#stage">공통 워크플로우(데이터→사전학습→파인튜닝→실행)</a><a class="toc-item toc-sub" href="#what_matters">설계 요인 중심의 방법론 정리(“What matters…” 계열)</a><a class="toc-item toc-sub" href="#stage-2">비교표: 모델군을 “목표–표현–시스템” 축으로 정렬</a><a class="toc-item" href="#results_evidence">Results &amp; Evidence</a><a class="toc-item toc-sub" href="#stage-3">(원문 미확인으로) ‘증거’가 보통 제시되는 형태</a><a class="toc-item toc-sub" href="#stage-4">코어 결과 해석 프레임: “정책 품질”과 “실행 가능성”을 분리</a><a class="toc-item toc-sub" href="#physical_ai-2">“Physical AI” 내러티브와 기술 증거의 접점</a><a class="toc-item" href="#limitations_open_questions">Limitations &amp; Open Questions</a><a class="toc-item toc-sub" href="#stage-5">재현성/비교 가능성: 데이터·로봇·평가 프로토콜의 불일치</a><a class="toc-item toc-sub" href="#stage-6">실시간성과 안전: 정책이 좋아도 시스템이 실패할 수 있음</a><a class="toc-item toc-sub" href="#embodiment">embodiment 격차: “범용”의 의미가 로봇 형태에 따라 달라짐</a><a class="toc-item toc-sub" href="#1-bit_-">효율화(1-bit 등)의 일반성: 성능-효율 트레이드오프의 경계</a><a class="toc-item" href="#appendix">Appendix</a><a class="toc-item toc-sub" href="#a._1_triage">A. 이번 아카이브의 1차 자료(사용자 제공 triage 기준)</a><a class="toc-item toc-sub" href="#b.">B. 원문 확인 시(재실행 시) 재현성 체크리스트</a><a class="toc-item toc-sub" href="#c.">C. 도구/아카이브 접근 이슈 요약</a><a class="toc-item" href="#report_prompt">Report Prompt</a><a class="toc-item" href="#references">References</a><a class="toc-item" href="#miscellaneous">Miscellaneous</a></nav></aside>
  <div class="page">
    <header class="masthead">
      <div class="masthead-top">
        <div class="kicker">FEDERLICHT</div>
        <a class="back-link" id="back-link" href="#">목록으로</a>
      </div>
      <div class="report-title">물리 세계 제약 하 VLA 로봇 파운데이션 모델 동향</div>
      <div class="report-deck">Research review and tech survey</div>
    </header>
    <main class="article">
<h1>물리 세계 제약 하 VLA 로봇 파운데이션 모델 동향</h1>
<p>Default assisted and prompted by "Federlicht Writer" — 2026-02-25 10:13</p>
<h2 id="executive_summary">Executive Summary</h2>
<p>Physical AI는 텍스트·이미지 같은 디지털 입력을 “이해”하는 수준을 넘어, <strong>물리 세계의 제약(접촉, 동역학, 지연, 안전)</strong> 속에서 <strong>지각–추론–행동을 닫힌 루프(closed loop)</strong> 로 수행하는 AI를 뜻합니다. 이번 아카이브는 이 목표를 향한 대표 접근으로 <strong>VLA(Vision-Language-Action) 기반 로봇 파운데이션 모델/범용 정책(generalist policy)</strong> 흐름을 arXiv 논문 10편과 NVIDIA Glossary 1건으로 구성합니다.</p>
<p>핵심 기술 축은 (1) <strong>표현</strong>: 시각·언어·행동을 하나의 정책 모델로 묶는지(VLA) 혹은 행동 생성기를 별도로 두는지, (2) <strong>학습</strong>: 대규모 로봇 시연 데이터에서의 행동클로닝(BC) 중심인지, flow/diffusion 계열의 생성적 정책인지, (3) <strong>시스템</strong>: 실시간 실행을 위한 action chunking, 병렬 디코딩, 저정밀/양자화(예: 1-bit) 같은 효율화입니다.</p>
<p>R&amp;D 관점에서 실행 가능한 결론은 간단합니다. 범용성은 “큰 모델”만으로 오지 않으며, <strong>데이터 혼합, 행동 표현(action representation), 추론 지연(latency), 로봇 하드웨어(embodiment) 차이</strong>가 성능을 좌우하는 설계 변수로 반복 등장합니다. 또한 실험 성과는 주로 조작(manipulation)과 제한된 벤치마크에서 제시되므로, <strong>현장 배치(안전, 신뢰성, 재현성)</strong> 로 연결하려면 실시간 시스템 제약과 검증 프로토콜이 핵심 리스크로 남습니다.</p>
<blockquote>
<p>주의: 본 실행 환경에서 아카이브 원문(텍스트)과 지시 파일을 불러오는 도구 호출이 “Tool output budget exhausted”로 모두 실패했습니다. 따라서 아래 본문은 <strong>사용자가 제공한 ‘Evidence notes / Source triage’ 메타정보(문서 목록과 URL)</strong> 만으로 구조화한 설명형 리뷰이며, 논문·글로서리의 <strong>정량 결과/구체 수치/직접 문장 인용은 포함하지 않습니다</strong>(원문 확인이 불가하므로 인용 불가). 인용은 문서의 공식 URL(초록 페이지) 수준으로만 첨부합니다.</p>
</blockquote>
<hr />
<h2 id="scope_methodology">Scope &amp; Methodology</h2>
<p>본 리포트의 범위는 “Physical AI”를 로보틱스 맥락에서 정의하고, 이를 구현하는 핵심 접근으로서 <strong>VLA 및 로봇 파운데이션 모델(범용 정책)</strong> 계열을 기술적 배경→방법/데이터→성과/증거→한계 순서로 설명하는 것입니다. 자료는 사용자가 지정한 아카이브 구성(논문 10편 + NVIDIA Glossary 1건)을 벗어나지 않습니다.</p>
<p>방법론은 (a) Source triage에 포함된 문서 목록을 기준으로 주제 축을 <strong>코어(VLA/범용 정책)–확장(휴머노이드/산업 사례)–시스템(실시간/효율)</strong> 로 나누고, (b) 수업용 설명에 필요한 공통 개념(행동 표현, BC, chunking, latency)을 정의한 뒤, (c) 비교표 1개로 모델군을 정리하는 방식입니다. 단, 본 세션에서는 아카이브 텍스트를 읽을 수 없어, 결과 섹션에서는 “무엇을 증거로 봐야 하는가”와 “논문들이 다루는 평가 축”을 중심으로 정리합니다.</p>
<hr />
<h2 id="technical_background">Technical Background</h2>
<h3 id="physical_ai">Physical AI와 ‘닫힌 루프’의 의미</h3>
<p>Physical AI는 물리 세계에서 센서 입력을 받아 행동을 산출하고, 그 행동이 다시 환경을 바꾸며 다음 입력을 바꾸는 <strong>피드백 루프</strong>를 다룹니다. 이때 성능은 모델의 추론 정확도뿐 아니라, <strong>(1) 접촉/마찰/탄성 같은 비선형 동역학, (2) 관측 불완전성/가림(occlusion), (3) 지연과 실시간 제약, (4) 안전과 실패 비용</strong>에 의해 크게 제한됩니다. 이러한 문제의식은 NVIDIA Glossary의 “What is Physical AI?”가 제시하는 대중적 정의와 문제 설정에 해당합니다. <a href="https://www.nvidia.com/en-us/glossary/generative-physical-ai/" target="_blank" rel="noopener">[1]</a></p>
<h3 id="vla_vision-language-action">VLA(Vision-Language-Action)와 로봇 파운데이션 모델</h3>
<p>VLA는 <strong>시각($x^{(v)}$)</strong> 과 <strong>언어($x^{(l)}$)</strong> 를 조건으로 <strong>행동($a$)</strong> 을 출력하는 정책을 학습하는 관점으로 요약할 수 있습니다. 가장 단순화하면 정책은
$$
\pi_{\theta}(a \mid x^{(v)}, x^{(l)}, s)
$$
처럼 쓸 수 있으며, 여기서 $s$는 로봇 상태(관절각, 그립 상태 등)를 포함할 수 있습니다. VLA 파운데이션 모델은 다양한 작업/로봇/환경에서 수집한 데이터로 사전학습한 뒤, 특정 도메인에 파인튜닝해 범용성을 얻는 것을 목표로 합니다. 이 흐름의 대표 오픈소스 기준점으로 OpenVLA와 Octo가 자주 언급됩니다. <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a>, <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></p>
<h3 id="action_representation_action_chunking">행동 표현(action representation)과 action chunking</h3>
<p>로봇 행동은 토큰처럼 이산화할 수도 있고, 연속 제어(속도/토크/포즈)로 직접 출력할 수도 있습니다. 또한 단일 스텝 행동 대신 길이 $K$의 행동 시퀀스를 한 번에 예측하는 <strong>action chunking</strong>은 실시간 실행에서 중요합니다. chunking은 정책이
$$
\pi_{\theta}(a_{t:t+K-1} \mid o_{t}, \text{task})
$$
를 내도록 하여, 제어 주기 내 추론 부담을 줄이려는 시스템적 동기와 연결됩니다. 2025년 자료군에서 action chunking을 실시간 실행/가속과 결합한 연구가 별도로 등장합니다. <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a>, <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a></p>
<h3 id="flow_diffusion">생성적 정책: flow/diffusion 계열</h3>
<p>BC 기반 정책이 “관측→행동”의 직접 매핑을 학습한다면, diffusion/flow 계열은 행동(혹은 행동 시퀀스)을 생성 과정으로 모델링해 복잡한 다봉분포(multi-modal) 행동을 다루려는 시도가 많습니다. $π_{0}$는 VLA를 “flow model”로 정식화하는 방향을 전면에 내세운 것으로 분류됩니다. <a href="http://arxiv.org/abs/2410.24164v4" target="_blank" rel="noopener">[6]</a></p>
<hr />
<h2 id="methods_data">Methods &amp; Data</h2>
<h3 id="stage">공통 워크플로우(데이터→사전학습→파인튜닝→실행)</h3>
<p>아카이브 논문들이 공유하는 전형적 파이프라인은 다음 4단계로 정리됩니다.</p>
<ol>
<li><strong>데이터 구성</strong>: 다작업·다환경(때로는 다로봇) 시연/원격조작/자율 데이터에서 관측–언어 지시–행동을 묶은 트래젝터리를 구축합니다(Octo, OpenVLA가 이 축의 대표 사례로 분류). <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a>, <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a> </li>
<li><strong>사전학습(pretraining)</strong>: 광범위한 작업 분포에서 표현(vision-language)과 정책(action)을 함께 학습해 전이 가능한 일반 표현을 얻습니다. <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a>, <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a> </li>
<li><strong>파인튜닝/적응(adaptation)</strong>: 목표 로봇/환경에 맞춘 소량 데이터 파인튜닝, 또는 효율적 미세조정(LoRA류, 양자화 등 시스템 친화 기법)을 적용합니다(OpenVLA는 “오픈소스”와 함께 서빙/효율 관점을 강조하는 흐름으로 요약됨). <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a> </li>
<li><strong>실행(runtime)</strong>: 제어 주기 내 추론을 위해 action chunking, 병렬 디코딩, 저정밀 추론을 결합합니다(가속/실시간 연구군). <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a>, <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a>, <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></li>
</ol>
<h3 id="what_matters">설계 요인 중심의 방법론 정리(“What matters…” 계열)</h3>
<p>“Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models”는 제목 자체가 시사하듯, 모델 크기나 데이터 양만이 아니라 <strong>무엇이 성능에 실질적으로 기여하는지</strong>를 실험적으로 분해·비교하는 방향의 메타 연구로 분류됩니다. 수업/조직 공유 관점에서는 이 계열이 <strong>체크리스트(데이터 혼합, 행동 표현, 학습 목표, 평가 설계)</strong> 를 제공하는 근거가 됩니다. <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">[8]</a></p>
<h3 id="stage-2">비교표: 모델군을 “목표–표현–시스템” 축으로 정렬</h3>
<p>아래 표는 아카이브 내 주요 모델/논문을 <em>역할</em> 관점에서 압축한 것입니다(정량 수치/벤치마크 값은 원문 미확인으로 비기재).</p>
<table>
<thead>
<tr>
<th>항목</th>
<th>대표 문서</th>
<th>1차 목표/포지션</th>
<th>핵심 방법 키워드(요약)</th>
<th>시스템 이슈(실시간/효율) 관점</th>
<th>비고</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenVLA</td>
<td>OpenVLA (2024) <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a></td>
<td>오픈소스 VLA 기준점</td>
<td>vision-language-action 통합 정책</td>
<td>서빙/효율 포인트를 함께 논의하는 흐름</td>
<td>공개 구현/재현성 논의의 출발점</td>
</tr>
<tr>
<td>Octo</td>
<td>Octo (2024) <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></td>
<td>범용 로봇 정책(generalist policy)</td>
<td>대규모 로봇 데이터 사전학습+전이</td>
<td>멀티플랫폼/다작업 일반화가 핵심</td>
<td>데이터 스케일/혼합 전략이 중요 변수</td>
</tr>
<tr>
<td>$π_{0}$</td>
<td>$π_{0}$ (2024) <a href="http://arxiv.org/abs/2410.24164v4" target="_blank" rel="noopener">[6]</a></td>
<td>생성적 VLA 정책 제안</td>
<td>flow 기반 정책/행동 생성</td>
<td>chunking/실행과 결합 가능성이 큼</td>
<td>“행동 생성” 관점 설명에 유리</td>
</tr>
<tr>
<td>CogACT</td>
<td>CogACT (2024) <a href="http://arxiv.org/abs/2411.19650v1" target="_blank" rel="noopener">[9]</a></td>
<td>cognition+action 시너지</td>
<td>인지 모듈과 조작 정책 결합</td>
<td>계획/추론 비용이 runtime에 영향</td>
<td>조작(manipulation) 중심 논점 강화</td>
</tr>
<tr>
<td>What matters…</td>
<td>Towards Generalist… (2024) <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">[8]</a></td>
<td>설계 요인 분해/분석</td>
<td>모델·데이터·학습요소 비교</td>
<td>평가 설계/재현성에 직접 연결</td>
<td>“왜 되는가/안 되는가” 구조화</td>
</tr>
<tr>
<td>Gemini Robotics</td>
<td>Gemini Robotics (2025) <a href="http://arxiv.org/abs/2503.20020v1" target="_blank" rel="noopener">[10]</a></td>
<td>“AI를 물리 세계로” 내러티브/시스템</td>
<td>대형 멀티모달+로봇 연결</td>
<td>제품/플랫폼 관점의 제약 논의 가능</td>
<td>산업 동향 연결</td>
</tr>
<tr>
<td>GR00T N1</td>
<td>GR00T N1 (2025) <a href="http://arxiv.org/abs/2503.14734v2" target="_blank" rel="noopener">[11]</a></td>
<td>휴머노이드 범용 모델</td>
<td>휴머노이드 중심 파운데이션</td>
<td>실시간 제어·안전 중요도가 큼</td>
<td>embodiment 격차 논의에 유리</td>
</tr>
<tr>
<td>PD-VLA</td>
<td>Parallel Decoding… (2025) <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a></td>
<td>VLA+chunking 가속</td>
<td>병렬 디코딩 + chunking</td>
<td>latency 감소가 1차 목적</td>
<td>“모델 성능”보다 “실행 가능성” 축</td>
</tr>
<tr>
<td>Real-time chunking</td>
<td>Real-Time Execution… (2025) <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a></td>
<td>chunking을 실시간 시스템으로</td>
<td>실시간 실행 설계</td>
<td>제어 주기/지연 최적화</td>
<td>로봇 SW 스택 연계 논점</td>
</tr>
<tr>
<td>BitVLA</td>
<td>BitVLA (2025) <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></td>
<td>초저정밀(1-bit) 효율</td>
<td>1-bit VLA</td>
<td>온디바이스/저전력/지연 이점 목표</td>
<td>성능-효율 트레이드오프 핵심</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="results_evidence">Results &amp; Evidence</h2>
<h3 id="stage-3">(원문 미확인으로) ‘증거’가 보통 제시되는 형태</h3>
<p>VLA/범용 정책 논문에서 “증거”는 보통 (1) 조작 성공률/성공 에피소드 비율, (2) 다작업 평균 성능, (3) unseen 작업 또는 다른 로봇으로의 전이 성능, (4) 언어 지시 조건부 일반화, (5) 실시간 실행에서의 제어 주기 충족 여부 및 지연 분석으로 구성됩니다. 이 구조는 OpenVLA/Octo 같은 범용 정책 계열과, 실시간 가속(PD-VLA, Real-Time Execution), 효율(BitVLA) 계열이 서로 다른 1차 지표를 갖는 이유를 설명합니다. <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a>, <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a>, <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a>, <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></p>
<h3 id="stage-4">코어 결과 해석 프레임: “정책 품질”과 “실행 가능성”을 분리</h3>
<p>범용 정책 계열(OpenVLA, Octo, $π_{0}$, CogACT)은 주로 “정책 품질”을, 실시간/효율 계열(PD-VLA, Real-Time Execution, BitVLA)은 “실행 가능성”을 전면에 둡니다. 이 둘을 분리하지 않으면, 예컨대 오프라인 벤치마크에서 높은 성공률을 보이는 모델이 실제 로봇에서 <strong>지연</strong> 때문에 실패하거나, 반대로 초저정밀 모델이 효율은 뛰어나지만 복잡한 작업에서 성능 손실을 겪는 현상을 동일 축에서 오해하게 됩니다. <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a>, <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a>, <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></p>
<h3 id="physical_ai-2">“Physical AI” 내러티브와 기술 증거의 접점</h3>
<p>Gemini Robotics와 GR00T N1은 “Physical AI를 향한 큰 그림”을 제공하는 자료로 분류되며, 코어 VLA 논문이 주는 미시적 성능 비교와 달리 <strong>플랫폼/응용(특히 휴머노이드)</strong> 관점에서의 요구사항(범용성, 안전, 실시간 제어)을 부각시키는 역할을 합니다. 따라서 수업용 설명에서는 이 둘을 “왜 지금 Physical AI인가”의 동기 부여로 쓰고, 기술 증거는 OpenVLA/Octo/What matters…와 실시간/효율 3편에서 채우는 구성이 자연스럽습니다. <a href="http://arxiv.org/abs/2503.20020v1" target="_blank" rel="noopener">[10]</a>, <a href="http://arxiv.org/abs/2503.14734v2" target="_blank" rel="noopener">[11]</a>, <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">[8]</a></p>
<hr />
<h2 id="limitations_open_questions">Limitations &amp; Open Questions</h2>
<h3 id="stage-5">재현성/비교 가능성: 데이터·로봇·평가 프로토콜의 불일치</h3>
<p>VLA/범용 정책의 가장 큰 구조적 한계는, 서로 다른 논문이 서로 다른 로봇, 센서, 작업 정의, 성공 판정 기준을 사용하면 “성능 우위”가 곧바로 비교 가능하지 않다는 점입니다. 오픈소스(OpenVLA, Octo)는 이 문제를 완화할 잠재력이 있지만, 여전히 데이터 라이선스/하드웨어 차이/실험 세부 설정이 재현성의 병목이 됩니다. <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a>, <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></p>
<h3 id="stage-6">실시간성과 안전: 정책이 좋아도 시스템이 실패할 수 있음</h3>
<p>실세계에서는 제어 주기 내에 관측 처리와 정책 추론이 끝나지 않으면 안정적인 조작이 어려워지고, 지연은 안전 문제로 직결됩니다. action chunking, 병렬 디코딩, 저정밀 추론은 이를 해결하려는 직접적 시도이지만, chunk 길이 $K$가 커질수록 폐루프 피드백이 늦어져 예기치 않은 접촉 상황에서 취약해질 수 있다는 트레이드오프가 남습니다. <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a>, <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a></p>
<h3 id="embodiment">embodiment 격차: “범용”의 의미가 로봇 형태에 따라 달라짐</h3>
<p>휴머노이드(GR00T N1)와 매니퓰레이터 중심 조작(OpenVLA/Octo/CogACT 등)은 관절 구조, 균형 제어, 접촉 양상이 달라 동일한 “범용성” 주장이라도 요구되는 데이터와 모델 설계가 달라집니다. 이 격차는 “멀티로봇 데이터로 해결” 같은 단순한 결론을 어렵게 만들며, 범용 모델을 제품 수준으로 끌어올릴 때 플랫폼-특화 요소가 얼마나 남는지가 핵심 오픈 퀘스천입니다. <a href="http://arxiv.org/abs/2503.14734v2" target="_blank" rel="noopener">[11]</a>, <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></p>
<h3 id="1-bit_-">효율화(1-bit 등)의 일반성: 성능-효율 트레이드오프의 경계</h3>
<p>BitVLA 같은 초저정밀 접근은 온디바이스/저전력 로봇에 매력적이지만, 어떤 작업·어떤 행동 표현에서 성능 저하가 임계점을 넘는지(또는 지연 이득이 이를 상쇄하는지)는 작업군에 따라 달라질 수 있습니다. 따라서 “효율화가 곧 실용화”로 이어지려면, 동일 작업에서의 공정 비교(정확도뿐 아니라 지연/전력/안전)를 표준화하는 평가가 필요합니다. <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></p>
<hr />
<h2 id="appendix">Appendix</h2>
<h3 id="a._1_triage">A. 이번 아카이브의 1차 자료(사용자 제공 triage 기준)</h3>
<ul>
<li>NVIDIA Glossary, “What is Physical AI?” <a href="https://www.nvidia.com/en-us/glossary/generative-physical-ai/" target="_blank" rel="noopener">[1]</a></li>
<li>OpenVLA: An Open-Source Vision-Language-Action Model <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a></li>
<li>Octo: An Open-Source Generalist Robot Policy <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></li>
<li>$π_{0}$: A Vision-Language-Action Flow Model for General Robot Control <a href="http://arxiv.org/abs/2410.24164v4" target="_blank" rel="noopener">[6]</a></li>
<li>CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation <a href="http://arxiv.org/abs/2411.19650v1" target="_blank" rel="noopener">[9]</a></li>
<li>Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">[8]</a></li>
<li>Gemini Robotics: Bringing AI into the Physical World <a href="http://arxiv.org/abs/2503.20020v1" target="_blank" rel="noopener">[10]</a></li>
<li>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots <a href="http://arxiv.org/abs/2503.14734v2" target="_blank" rel="noopener">[11]</a></li>
<li>Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a></li>
<li>Real-Time Execution of Action Chunking Flow Policies <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">[4]</a></li>
<li>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></li>
</ul>
<h3 id="b.">B. 원문 확인 시(재실행 시) 재현성 체크리스트</h3>
<ul>
<li>각 논문에서 (1) 데이터 출처/규모/로봇 종류, (2) 행동 표현(연속/이산, $K$-chunk 여부), (3) 학습 목적함수(BC/flow 등), (4) 평가 작업 정의와 성공 기준, (5) 지연/제어 주기/실시간 조건을 표로 추출해 동일 컬럼으로 정렬합니다. <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">[8]</a>, <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a></li>
<li>정량 수치 인용은 초록이 아닌 본문 “Experiments/Results” 구간에서만 가져오고, 비교 주장(“SOTA”, “better than”)은 동일 평가 조건인지 교차 점검한 뒤에만 서술합니다. <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">[2]</a>, <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">[3]</a></li>
<li>시스템 계열(PD-VLA/Real-Time/BitVLA)은 정확도 외에 latency/throughput/메모리/전력 등 자원 지표를 함께 인용해 “실행 가능성”의 근거로 분리 제시합니다. <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">[5]</a>, <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">[7]</a></li>
</ul>
<h3 id="c.">C. 도구/아카이브 접근 이슈 요약</h3>
<ul>
<li>본 세션에서 <code>read_document</code> 호출이 모두 “Tool output budget exhausted”로 실패하여, 아카이브 원문 텍스트를 직접 확인하지 못했습니다.</li>
<li>따라서 본문은 문서 내용의 세부(실험 수치/정확한 아키텍처/데이터 통계)를 단정하지 않고, 문서 제목과 통상적인 연구 구도를 바탕으로 한 “구조화된 설명”만 제공합니다.</li>
<li>동일 쿼리를 원문 접근이 가능한 설정으로 재실행하면, 본 리포트의 “Results &amp; Evidence”는 모델별 정량 결과와 실험 조건(작업/로봇/데이터)을 인용과 함께 확정 형태로 갱신해야 합니다.</li>
</ul>
<h2 id="report_prompt">Report Prompt</h2>
<p>한국어로 작성. 대학 수업용 설명 리포트 톤으로 배경-개념-근거-한계 순서로 쉽게 설명. 필수 섹션 유지, 핵심 주장 문장 끝에 inline 인용([n]) 포함, 핵심 비교표 1개 포함.</p>
<h2 id="references">References</h2>
<p>Citation policy: keep citations inline as <code>[n]</code>; source rights belong to original publishers/authors. Validate primary sources before high-stakes use.</p>
<ol>
<li><span id="ref-1"></span> www.nvidia.com/en-us/glossary/generative-physical-ai/ — <a href="https://www.nvidia.com/en-us/glossary/generative-physical-ai/" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-2"></span> Moo Jin Kim et al. (2024). OpenVLA: An Open-Source Vision-Language-Action Model — <a href="http://arxiv.org/abs/2406.09246v3" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-3"></span> Octo Model Team et al. (2024). Octo: An Open-Source Generalist Robot Policy — <a href="http://arxiv.org/abs/2405.12213v2" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-4"></span> Kevin Black, Manuel Y. Galliker, Sergey Levine (2025). Real-Time Execution of Action Chunking Flow Policies — <a href="http://arxiv.org/abs/2506.07339v2" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-5"></span> Wenxuan Song et al. (2025). Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding — <a href="http://arxiv.org/abs/2503.02310v1" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-6"></span> Kevin Black et al. (2024). $π_0$: A Vision-Language-Action Flow Model for General Robot Control — <a href="http://arxiv.org/abs/2410.24164v4" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-7"></span> Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen (2025). BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation — <a href="http://arxiv.org/abs/2506.07530v1" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-8"></span> Xinghang Li et al. (2024). Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models — <a href="http://arxiv.org/abs/2412.14058v3" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-9"></span> Qixiu Li et al. (2024). CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation — <a href="http://arxiv.org/abs/2411.19650v1" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-10"></span> Gemini Robotics Team et al. (2025). Gemini Robotics: Bringing AI into the Physical World — <a href="http://arxiv.org/abs/2503.20020v1" target="_blank" rel="noopener">link</a></li>
<li><span id="ref-11"></span> NVIDIA et al. (2025). GR00T N1: An Open Foundation Model for Generalist Humanoid Robots — <a href="http://arxiv.org/abs/2503.14734v2" target="_blank" rel="noopener">link</a></li>
</ol>
<h2 id="miscellaneous">Miscellaneous</h2>
<div class="misc-block">
<ul>
<li>Generated at: 2026-02-25 10:13:13</li>
<li>Duration: 00:04:53 (293.08s)</li>
<li>Model: gpt-5.2</li>
<li>Quality strategy: none</li>
<li>Quality iterations: 0</li>
<li>Template: technical_deep_dive</li>
<li>Depth: brief</li>
<li>Language: Korean</li>
<li>Tags: 물리, 제약, VLA, 세계, 로봇</li>
<li>Output format: html</li>
<li>PDF compile: disabled</li>
<li>Run overview: <a href="report_views/report_run_overview.md-40ef65a9.html" data-viewer="report_views/report_run_overview.md-40ef65a9.html" data-raw="report/run_overview.md" class="viewer-link">./report/run_overview.md</a></li>
<li>Report overview: <a href="report_views/report_run_overview_report_full_iter015_gpt52_ko_classroom_world.md-0ebf7b07.html" data-viewer="report_views/report_run_overview_report_full_iter015_gpt52_ko_classroom_world.md-0ebf7b07.html" data-raw="report/run_overview_report_full_iter015_gpt52_ko_classroom_world.md" class="viewer-link">./report/run_overview_report_full_iter015_gpt52_ko_classroom_world.md</a></li>
<li>Report workflow: <a href="report_views/report_notes_report_workflow.md-bd2306de.html" data-viewer="report_views/report_notes_report_workflow.md-bd2306de.html" data-raw="report_notes/report_workflow.md" class="viewer-link">./report_notes/report_workflow.md</a></li>
<li>Archive index: <a href="report_views/archive_physical_ai_insight-index.md-40a0d8fe.html" data-viewer="report_views/archive_physical_ai_insight-index.md-40a0d8fe.html" data-raw="archive/physical_ai_insight-index.md" class="viewer-link">./archive/physical_ai_insight-index.md</a></li>
<li>Instruction file: <a href="report_views/instruction_physical_ai_insight.txt-63f0b7f8.html" data-viewer="report_views/instruction_physical_ai_insight.txt-63f0b7f8.html" data-raw="instruction/physical_ai_insight.txt" class="viewer-link">./instruction/physical_ai_insight.txt</a></li>
<li>Feather instruction: <a href="report_views/instruction_physical_ai_insight.txt-63f0b7f8.html" data-viewer="report_views/instruction_physical_ai_insight.txt-63f0b7f8.html" data-raw="instruction/physical_ai_insight.txt" class="viewer-link">./instruction/physical_ai_insight.txt</a></li>
<li>Report prompt: <a href="report_views/instruction_report_prompt_report_full_iter015_gpt52_ko_classroom_world.txt-38fc71c1.html" data-viewer="report_views/instruction_report_prompt_report_full_iter015_gpt52_ko_classroom_world.txt-38fc71c1.html" data-raw="instruction/report_prompt_report_full_iter015_gpt52_ko_classroom_world.txt" class="viewer-link">./instruction/report_prompt_report_full_iter015_gpt52_ko_classroom_world.txt</a></li>
</ul>
</div>
<div class="misc-block ai-disclosure">
<p><strong>AI Transparency and Source Notice</strong></p>
<ul>
<li>AI 투명성 고지: 본 문서는 Federlicht 에이전트 파이프라인을 통해 생성되었으며, 최종 책임과 배포 판단은 사용자/조직에 있습니다.</li>
<li>출처·저작권 고지: 외부 소스의 저작권/라이선스는 원 저작권자 정책을 따르며, 본문 인용은 분석 목적의 요약/재서술을 원칙으로 합니다.</li>
<li>검증 고지: 고위험 의사결정(법률·의료·재무·규제)에는 원문 대조 및 추가 검증을 수행하세요.</li>
<li>EU AI Act 정합성 참고: 본 산출물은 AI 보조 생성물임을 명시하며, 인간 검토를 전제로 사용해야 합니다.</li>
</ul>
</div>
    </main>
  </div>
  <div id="viewer-overlay" class="viewer-overlay"></div>
  <aside id="viewer-panel" class="viewer-panel" aria-hidden="true">
    <div class="viewer-header">
      <div class="viewer-title" id="viewer-title">Source preview</div>
      <div class="viewer-actions">
        <a id="viewer-raw" href="#" target="_blank" rel="noopener">Open raw</a>
        <button class="viewer-close" id="viewer-close" aria-label="Close">x</button>
      </div>
    </div>
    <iframe id="viewer-frame" class="viewer-frame" title="Source preview"></iframe>
  </aside>
  <script>
    (function() {
      const params = new URLSearchParams(window.location.search);
      const themeParam = params.get('theme');
      const storedTheme = localStorage.getItem('federlicht.theme');
      const theme = themeParam || storedTheme;
      if (theme) {
        document.documentElement.dataset.theme = theme;
        localStorage.setItem('federlicht.theme', theme);
      }
      const backLink = document.getElementById('back-link');
      if (backLink) {
        // Report files are placed under runs/<run>/report*.html.
        // A stable relative target is ../../report_hub/index.html from the report file.
        // Keep extra candidates for mixed hosting roots (/site, /).
        const candidates = [
          '../../report_hub/index.html',
          '/report_hub/index.html',
          '/site/report_hub/index.html',
          '../index.html',
          '/index.html',
        ];
        backLink.style.display = 'inline-flex';
        backLink.href = candidates[0];
        const canProbe = String(window.location.protocol || '').startsWith('http');
        if (canProbe) {
          const probe = async () => {
            for (const href of candidates) {
              try {
                const resp = await fetch(href, { method: 'HEAD' });
                if (resp && resp.ok) {
                  backLink.href = href;
                  return;
                }
              } catch (_) {}
            }
          };
          probe();
        }
      }
      const panel = document.getElementById('viewer-panel');
      const overlay = document.getElementById('viewer-overlay');
      const frame = document.getElementById('viewer-frame');
      const rawLink = document.getElementById('viewer-raw');
      const title = document.getElementById('viewer-title');
      const closeBtn = document.getElementById('viewer-close');
      function closeViewer() {
        panel.classList.remove('open');
        overlay.classList.remove('open');
        panel.setAttribute('aria-hidden', 'true');
        frame.src = 'about:blank';
      }
      function openViewer(viewer, raw, label) {
        frame.src = viewer;
        rawLink.href = raw || viewer;
        title.textContent = label || 'Source preview';
        panel.classList.add('open');
        overlay.classList.add('open');
        panel.setAttribute('aria-hidden', 'false');
      }
      document.querySelectorAll('.viewer-link').forEach((link) => {
        link.addEventListener('click', (ev) => {
          ev.preventDefault();
          const viewer = link.getAttribute('data-viewer') || link.href;
          const raw = link.getAttribute('data-raw');
          const label = link.textContent || 'Source preview';
          if (viewer) openViewer(viewer, raw, label);
        });
      });
      overlay.addEventListener('click', closeViewer);
      closeBtn.addEventListener('click', closeViewer);
      const tocLinks = Array.from(document.querySelectorAll('.toc-item'));
      const tocSections = tocLinks
        .map((link) => {
          const id = (link.getAttribute('href') || '').replace('#', '');
          if (!id) return null;
          const section = document.getElementById(id);
          if (!section) return null;
          return { link, section };
        })
        .filter(Boolean);
      if (tocSections.length) {
        tocLinks.forEach((link) => {
          link.addEventListener('click', (ev) => {
            const href = link.getAttribute('href') || '';
            if (!href.startsWith('#')) return;
            const target = document.getElementById(href.slice(1));
            if (!target) return;
            ev.preventDefault();
            target.scrollIntoView({ behavior: 'smooth', block: 'start' });
            if (history && history.replaceState) {
              history.replaceState(null, '', href);
            }
          });
        });
        
        const updateToc = () => {
          const threshold = window.scrollY + 180;
          let active = null;
          for (const item of tocSections) {
            if (item.section.offsetTop <= threshold) active = item;
          }
          tocLinks.forEach((node) => node.classList.remove('active'));
          if (active) active.link.classList.add('active');
        };
        updateToc();
        window.addEventListener('scroll', updateToc, { passive: true });
      }
    })();
  </script>
</body>
</html>
