

===== PAGE 1 =====
VISION TRANSFORMERS IN PRECISION AGRICULTURE: A
COMPREHENSIVE SURVEY
A PREPRINT
Saber Mehdipour
Department of Computer Engineering
University of Guilan
Rasht, Iran
sabermehdipour@webmail.guilan.ac.ir
Seyed Abolghasem Mirroshandel
Department of Computer Engineering
University of Guilan
Rasht, Iran
mirroshandel@guilan.ac.ir
Seyed Amirhossein Tabatabaei
Department of Computer Science
University of Guilan
Rasht, Iran
amirhossein.tabatabaei@guilan.ac.ir
ABSTRACT
Detecting plant diseases is a crucial aspect of modern agriculture, as it plays a key role in maintaining
crop health and increasing overall yield. Traditional approaches, though still valuable, often rely
on manual inspection or conventional machine learning techniques, both of which face limitations
in scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as a promising
alternative, offering advantages such as improved handling of long-range dependencies and better
scalability for visual tasks. This review explores the application of ViTs in precision agriculture,
covering a range of tasks. We begin by introducing the foundational architecture of ViTs and
discussing their transition from Natural Language Processing (NLP) to Computer Vision (CV). The
discussion includes the concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive review of
recent literature, focusing on key methodologies, datasets, and performance metrics. This study
also includes a comparative analysis of CNNs and ViTs, along with a review of hybrid models and
performance enhancements. Technical challenges such as data requirements, computational demands,
and model interpretability are addressed, along with potential solutions. Finally, we outline future
research directions and technological advancements that could further support the integration of ViTs
in real-world agricultural settings. Our goal with this study is to offer practitioners and researchers a
deeper understanding of how ViTs are poised to transform smart and precision agriculture.
Keywords Vision Transformers · Precision agriculture · Transformer models · ViT · Deep learning · Plant disease
detection · Plant disease classification
1
Introduction
Plant diseases pose a significant challenge to global agriculture, affecting crop yields, food security, and the economic
well-being of farmers. Early and accurate detection is essential to effectively manage these diseases and minimize their
impact [11]. Traditionally, the diagnosis of plant diseases has depended on labor-intensive manual inspections by experts
or on conventional Machine Learning (ML) techniques, both of which have inherent limitations. Manual inspection
is not only time-consuming but also subject to human error and inconsistency, making it impractical for large-scale
agricultural operations [58, 26]. Conventional ML methods are largely dependent on manual feature extraction, which
limits their capacity to generalize across diverse environmental conditions, such as variations in lighting, background,
and plant species. Moreover, these techniques are unable to effectively capture complex spatial patterns directly from
arXiv:2504.21706v4  [cs.CV]  14 Aug 2025


===== PAGE 2 =====
S.MEHDIPOUR ET AL.
raw image data [24, 60]. To address these limitations, researchers are increasingly turning to automated solutions that
harness recent advances in ML and CV.
CNN, a widely used traditional Deep Learning (DL) method, have shown some success in automating the disease
detection process [91, 53]. This has created a growing demand for more robust, scalable, and intelligent systems
for plant disease detection. In response, several studies have explored the potential of DL to meet these challenges.
Some existing surveys provide general overviews of DL approaches, highlighting their advantages over traditional
methods and outlining the progress made in applying these techniques to plant disease detection tasks. For example,
one study [86] reviews how DL and ML techniques have been employed for plant disease detection from image data.
The study describes the transition from handcrafted features with the traditional DL methods to deep models like
CNNs and segmentation models. It also discusses the implementation of object detection algorithms like YOLO and
Faster R-CNN, and segmentation architectures like U-Net and Mask R-CNN, for pixel-level accuracy-based detection
quality improvement. Another work [43] discusses the application of ML and DL techniques in the early detection and
classification of plant diseases, highlighting their importance in improving agricultural yield and reducing crop loss. It
presents an extensive list of ML techniques such as Naïve Bayes, k-Nearest Neighbors, Decision Trees, Support Vector
Machines, Random Forests, and Multi-layer Perceptrons, and explores their applications in plant disease classification
using environmental and image inputs. The study also discusses preprocessing techniques and image segmentation
methods, demonstrating how ML has enabled the automation of disease diagnosis and soil testing from models trained
with crop data. The contrast between classical ML and DL methods shows that DL outperforms ML in terms of accuracy
and scalability. This study emphasizes the importance of addressing current deficiencies and proposes DL as a promising
direction for creating automatic, systematic, and scalable plant disease diagnostic systems that support both scientists
and farmers in agricultural decision-making. Another overview of studies [3] explores the use of DL techniques for plant
disease detection and management in agriculture. It addresses key challenges including dataset quality, imaging sensors,
model generalizability, estimation of severity, and performance comparison with human capabilities. Commonly used
datasets are categorized, along with their limitations in practical scenarios. The study outlines various DL techniques,
with a particular focus on CNN-based models, employed for image classification, object detection, and semantic
segmentation. It highlights limitations in the ability of existing models to generalize to field conditions, primarily
due to overfitting and imbalanced data. Additionally, significant research areas are highlighted, such as universal
severity estimation, multi-disease detection, and model generalization across diverse conditions. A separate study [54]
explores the application of DL in plant disease recognition and classification, emphasizing its critical role in agricultural
productivity and food security. It illustrates how DL improves upon traditional image processing techniques through
automated feature extraction and enhanced objectivity and efficiency. The paper discusses foundational concepts,
model performance criteria, and the evolution of DL in plant disease recognition. It also addresses the need for data
augmentation to manage limited datasets and the use of visualization methods to improve model interpretability. The
study concludes with future challenges, such as the need for more varied datasets, enhanced model robustness, and the
integration of hyperspectral imaging for early detection.
Recent advancements in DL have demonstrated the potential of CNNs and their variants in accurately detecting
and classifying plant diseases. However, despite their success, CNN-based models often struggle to capture global
contextual information due to their inherent inductive biases and locality constraints. The introduction of the Trans-
former architecture has transformed the field of NLP, enabling significant advancements in machine translation, text
summarization, and language modeling, and setting new performance benchmarks across a wide range of language
understanding tasks [70]. By leveraging parallel processing and self-attention mechanisms, Transformers excel at
capturing long-range dependencies and contextual relationships. This breakthrough has inspired researchers to explore
the application of Transformers in other domains, including CV. It was soon recognized that the same properties
that made Transformers effective in NLP, namely their ability to model long-range dependencies and contextually
relevant information, could also be advantageous for image analysis tasks [21]. Unlike CNNs, which rely on inductive
biases such as local connectivity and spatial hierarchies, Transformers offer a more flexible architecture with fewer
assumptions about input structure. This flexibility enables them to learn rich, complex patterns directly from visual
data, making them a promising alternative for various CV applications, including automated plant disease detection.
The use of ViTs in agriculture is a novel and rapidly emerging research area. To the best of our knowledge, there are
a few works whose focus is on ViTs; however, most of them mention it only briefly and do not provide a comprehensive
analysis of their applications in precision agriculture. For example, a review [64] categorizes DL techniques applied to
plant disease detection, organizing the literature into three tasks: classification, detection, and segmentation of plant
diseases from leaf images. The review evaluates a broad range of DL models, with a strong focus on CNNs such as
ResNet [33], VGG [88], and EfficientNet [94], alongside growing interest in ViTs. Another study [104] examines the
use of DL and CV in plant disease detection, systematically analyzing the transition from traditional image processing
to modern DL-based approaches. It assesses the suitability and performance of image processing techniques like
thresholding, edge detection, and region-based segmentation in real-world agriculture. The study categorizes various
DL architectures, such as CNNs, Generative Adversarial Network (GAN), ViTs, and Vision-Language Models, and
highlights their scalability and potential for practical implementation. It covers advanced models like YOLO and Faster
2


===== PAGE 3 =====
S.MEHDIPOUR ET AL.
R-CNN for real-time object detection, and discusses the emerging relevance of ViTs for capturing global context in
image analysis. However, while ViTs are introduced, the review lacks a deep evaluation of specific architectures or
performance benchmarks in agricultural contexts.
In this work, we examine the use of ViT in the agriculture domain with the objective of identifying how this
architecture has been used and adapted for agricultural applications such as crop classification, disease detection,
yield prediction, and precision agriculture. With ViT’s success in CV across numerous domains, its application to
transforming agricultural image analysis holds much promise. By analyzing the existing literature, we aspire to extract
current trends, evaluate model performance, and identify challenges specific to agricultural environments and datasets.
This initial exploration will lay the groundwork for future studies and build a sound body of knowledge within this
emerging interface of DL and agriculture.
In summary, the key contributions of this work are as follows:
• An overview of the Transformer and ViT architectures and their functionality is provided.
• We have comprehensively addressed the concept of inductive biases and their role in ML models, especially
CNNs and ViTs.
• A comprehensive survey is presented on the application of ViTs in the context of precision agriculture,
systematically analyzing 44 high-impact peer-reviewed studies.
• We critically analyze the current challenges and unresolved issues in the field, highlight emerging research
trends, formulate key open challenges, and suggest promising directions for future investigation.
The structure of this paper is organized as follows: Section 2 provides an overview of the Transformer architecture,
followed by a discussion on ViTs. Section 3 explores the concept of inductive biases in ML, with a particular focus on
how these biases manifest in CNNs and ViTs. In Section 4, we review the applications of ViTs in precision agriculture,
categorizing the models into two groups: pure and hybrid models. Section 5 highlights key findings and outlines the
open challenges identified in the reviewed literature. Finally, Section 6 presents the conclusions of the paper.
2
Background
2.1
Transformer
The Transformer model [105] brought a groundbreaking shift to NLP by introducing the self-attention mechanism,
which allows the model to evaluate the importance of different words in a sentence during sequence encoding and
generation. This innovation overcomes the limitations of earlier architectures, such as Recurrent Neural Networks
(RNNs) and CNNs, which often struggled with long-range dependencies and parallelization. By processing all tokens
in a sequence simultaneously, self-attention captures long-range relationships more effectively, making the Transformer
architecture both efficient and highly scalable for a wide range of sequence modeling tasks. In the following subsection,
we examine its main components.
2.1.1
Encoder-Decoder structure
The Transformer architecture comprises an encoder and a decoder, each consisting of multiple layers of self-attention
and Feed-Forward Neural Networks. The encoder processes the input sequence into a contextual representation, while
the decoder generates the output sequence based on both the encoder’s output and previously generated tokens. This
encoder–decoder structure is commonly used in tasks such as machine translation, whereas certain variants, such as
GPT [10], employ only the decoder for autoregressive language modeling. Figure 1 illustrates the overall architecture
of the Transformer’s encoder–decoder structure.
Each encoder layer consists of two main sublayers:
1. Self-Attention Mechanism
2. Feed-Forward Neural Network
Each sublayer is followed by a residual connection and layer normalization.
Each decoder layer consists of three sublayers:
1. Masked Self-Attention Mechanism
2. Encoder-Decoder Attention Mechanism
3. Feed-Forward Neural Network
Similar to the encoder, each sublayer is followed by a residual connection and layer normalization.
3


===== PAGE 4 =====
S.MEHDIPOUR ET AL.
Figure 1: The Encoder-Decoder architecture of the Transformer model [105]. The encoder processes the input sequence
to generate contextualized representations, which are then passed to the decoder to produce the output sequence. This
structure enables efficient parallel processing and captures long-range dependencies through self-attention mechanisms,
allowing for better modeling of relationships between distant elements in the sequence.
2.1.2
Self-Attention mechanism
The self-attention mechanism is the core innovation of the Transformer architecture, allowing the model to dynami-
cally focus on different parts of the input sequence. As shown in Figure 2, this attention is computed using the following
steps:
1. Input Matrices: The inputs consist of queries (Q), keys (K), and values (V), all derived from the input
embeddings.
2. Attention Scores: Calculate the dot products of the query (Q) with all keys (K). This results in a matrix of raw
attention scores, which determines how much attention each query should pay to the different keys.
3. Scaling: Scale the attention scores by the square root of the dimension of the keys, √dk, to avoid large values
that could result in very small gradients during training.
4. Softmax: Apply the softmax function to the scaled attention scores to normalize them, turning them into
probabilities (attention weights) that sum to 1.
5. Weighted Sum: Compute the weighted sum of the values (V) using the attention weights. The result is a new
set of embeddings (or representations), where each one is a mixture of the original values weighted by how
much attention was paid to the corresponding key.
2.1.3
Multi-Head Attention
To allow the model to focus on different parts of the sequence simultaneously, the Transformer uses multi-head
attention, which involves multiple parallel attention layers (heads). The process can be summarized as follows:
1. Linear Projections: Linearly project the queries, keys, and values h times using different learned projection
matrices for each head, resulting in h distinct sets of queries, keys, and values.
4


===== PAGE 5 =====
S.MEHDIPOUR ET AL.
Figure 2: The Attention mechanism [105]. The attention computation uses queries (Q), keys (K), and values (V) to
generate weighted representations, allowing the model to focus on relevant parts of the input sequence based on learned
attention scores.
2. Parallel Attention Heads: Apply the scaled dot-product attention mechanism independently to each set of
queries, keys, and values in parallel. This allows the model to attend to different parts of the input sequence
from different subspaces.
3. Concatenation: Concatenate the outputs of all h attention heads, forming a single vector of combined
information.
4. Final Linear Projection: Apply a final linear transformation to the concatenated output, resulting in the final
multi-head attention output that will be passed to subsequent layers.
Figure 3 provides a schematic representation of the multi-head attention mechanism.
Figure 3: multi-head attention [105]. It runs multiple Self-Attention operations in parallel, each with different learned
projections of Q, K, and V, allowing the model to capture diverse contextual relationships from different representation
subspaces.
2.2
Vision Transformer
Inspired by the success of Transformers in NLP, A. Dosovitskiy et al. [21] adapted the Transformer architecture for
CV tasks, leading to the creation of ViT. Before this breakthrough, visual attention mechanisms were typically combined
with convolutional networks or used to enhance specific components, while retaining the overall CNN structure. Early
5


===== PAGE 6 =====
S.MEHDIPOUR ET AL.
attention-based architectures had shown promise in CV [41], but their implementation on hardware accelerators often
required complex and specialized engineering, as these systems were primarily optimized for convolutional operations.
The key insight of ViT was to show that convolutional networks are not essential: a simple Transformer applied directly
to sequences of image patches can achieve strong results in image classification. In ViT, images are divided into
fixed-size patches, each treated as a token, similar to words in NLP. These patches are linearly embedded, combined with
positional embeddings, and processed through multiple Self-Attention layers. This design enables ViTs to effectively
capture long-range dependencies and contextual relationships within images, leading to performance that often surpasses
traditional CNNs on various classification benchmarks. By leveraging the Self-Attention mechanism of Transformers,
ViTs have revolutionized the field of CV. The following section delves into the architecture of ViTs, explaining their
key components and operations in detail. Figure 4 shows the structure of ViT. The architecture of ViT consists of the
following key components: Patch Embedding, Positional Embedding, Transformer Encoder, and Classification Head.
Figure 4: Vision Transformer architecture [21]. The image is split into fixed-size patches, which are linearly embedded,
combined with positional embeddings, and fed into a Transformer encoder. The model captures global image context
through Self-Attention across all patches.
2.2.1
Patch Embedding
In ViTs, the input image is divided into fixed-size patches. Each patch is then flattened and projected into a lower-
dimensional space.
Given an image X ∈RH×W ×C with height X, width W, and C channels, it is divided into patches of size P × P
Thus, the number of patches N is HW
P 2 .
Each patch is then linearly transformed into an embedding vector using a trainable projection matrix E ∈R(P 2·C)×D
where D is the dimensionality of the embedding space.
• The embedding dimension D in a ViT plays a crucial role in transforming the flattened image patches into a
suitable format for processing by the Transformer architecture.
• An embedding is a vector representation of the input data. For image patches, it means converting the flattened
pixel values into a high-dimensional vector that captures more meaningful information.
• Directly using the raw pixel values of image patches may not be efficient or effective for the Transformer
model. Embedding helps transform these raw values into a more abstract, dense representation.
• The choice of the embedding dimension D in a ViT is a crucial design decision that balances the model’s
representational power and computational efficiency.
Let zP
0 ∈RD denote the initial embedding of the P th image patch. It is obtained by flattening the patch and
projecting it into a D-dimensional space using a learnable linear transformation:
zP
0 = Flatten(xP )E,
for P = 1 to N
(1)
where zP
0 ∈RD is the embedding of the P th patch at Transformer layer-0 (before any attention layers), xP is the
P th image patch before projection and E ∈R(P 2.C)×D is a learnable weight matrix that projects each flattened image
patch into a D-dimensional embedding space.
6


===== PAGE 7 =====
S.MEHDIPOUR ET AL.
2.2.2
Positional Embedding
To retain the spatial information, the positional embedding P ∈RN×D is added to the patch embeddings:
z0 = [x1E ; x2E ; . . . ; xNE] + P
(2)
where z0 is the full embedded input sequence after adding positional information at Transformer layer-0 (before any
attention layers), xi ∈RP 2.C is the ith flattened image patch, E ∈R(P 2.C)×D is the patch embedding projection
matrix and P ∈RN×D is the positional embeddings. The positional embeddings added to the patch embeddings can be
either learned or fixed.
2.2.3
Transformer Encoder
The Transformer encoder consists of multiple layers, each composed of a multi-head self-attention mechanism and a
feed-forward neural network.
1-Multi-Head Self-Attention (MHSA):
The self-attention mechanism computes a weighted sum of the input sequences, focusing on different parts of the
sequence for each output element. Given an input sequence z ∈RN×D
Attention(Q, K, V ) = softmax
QKT
√
D

V
(3)
where Q = zWQ, K = zWK, and V = zWV are query, key, and value matrices, respectively. WQ, WK and WV
are learned projection matrices.
For multi-head attention, multiple attention heads are used:
MHSA(z) = [head1 ; head2 ; . . . ; headh]WO
(4)
where headi = Attention(Qi, Ki, Vi) and WO ∈RhD×D is a learned projection matrix.
2- Feed-Forward Neural Network (FFN)
Each encoder layer also includes a feed-forward neural network, applied to each position separately and identically:
FFN(x) = max(0, xW1 + b1)W2 + b2
(5)
where W1 ∈RD×Dff and W2 ∈RDff ×D are learned weight matrices, b1 ∈RDff and b2 ∈RD are learned bias
vectors, and Dff is the dimensionality of the feed-forward layer.
3- Layer Normalization and Residual Connections
Each sub-layer (MHSA and FFN) is followed by layer normalization and a residual connection:
z′
l = LayerNorm (MHSA(zl−1) + zl−1)
(6)
zl = LayerNorm (FFN(z′
l) + z′
l)
(7)
where zl−1 is the output from the previous encoder layer, MHSA(zl−1) is the result of applying multi-head
self-attention to that output, z
′
l is the input to the feed-forward network (FFN) sub-layer.zl denotes the output of lthlayer.
2.2.4
Classification Head
The output of the Transformer encoder is a sequence of embeddings, one for each input token. For classification tasks, a
special learnable classification token z0
0 is prepended to the input sequence before encoding. After passing through all
L encoder layers, the corresponding output embedding z0
L is used as a compact representation of the entire image. This
embedding is then passed through a multi-layer perceptron (MLP) to produce the final class scores:
logits = MLP
 z0
L

(8)
where the output logits ∈RK (for K classes) is used for prediction.
7


===== PAGE 8 =====
S.MEHDIPOUR ET AL.
3
Inductive biases in ML and DL Models
3.1
Definition of Inductive Biases
Inductive biases refer to a set of built-in assumptions that a ML model creates, which can facilitate the generalization
process, especially when working with limited data. We can consider these biases as prior knowledge that the model
uses to make predictions about data that it hasn’t seen before. In general terms, inductive biases guide the learning
algorithm to solutions that possess particular characteristics [7, 30].
In supervised learning, when the data is frequently incomplete or noisy, inductive biases are essential for the model’s
effectiveness. InCV, for example in CNNs, some of the inherent inductive biases include locality and translation
equivariance [20, 73, 25]. Locality assumes that important features, such as edges and textures, are restricted to small
spatial regions within an image, while translation equivariance guarantees that these features are identified regardless of
their position in the image. These biases enable CNNs to perform exceptionally well in tasks like image classification
and object detection, where spatial hierarchies play a key role [49, 74]. Similarly, in NLP, RNNs and Transformers
utilize sequential and contextual relationships as biases to process and understand text. The type of inductive biases has
a direct impact on a model’s performance on particular tasks.
3.2
Inductive Biases In CNNs and ViTs
CNNs have demonstrated remarkable effectiveness in modeling spatial hierarchies and capturing local dependencies
within data. Their architecture, which consists of convolutional layers followed by pooling layers, makes them well-
suited for image recognition, object detection, and segmentation, where learning spatial relationships is essential [84, 32].
The efficiency of CNNs is largely attributed to the strong inductive biases natively built into their design, including
locality, two-dimensional neighborhood structure, and translation equivariance [21]. These are uniformly applied across
all layers in the network, enabling CNNs to learn invariant and spatially-conscious feature representations.
In particular, two dominant inductive biases, locality and weight sharing, serve as architectural constraints that
significantly reduce the number of learnable parameters and enhance the model’s generalization capabilities [22].
Locality is a design principle whereby each neuron in a convolutional layer connects only to a small, spatially localized
subset of the input, known as the receptive field. This localized computation enables CNNs to effectively extract low-
level features such as edges, corners, and textures. As data passes through deeper layers, local features are hierarchically
combined to detect more abstract and global representations [28]. Weight sharing, meanwhile, ensures the same filters
(kernels) are employed at different spatial locations, which makes the model equivariant to translation. As a result,
inductively learned patterns in one region of an image are recognizable wherever they are, thereby making the model
stronger and more efficient, significantly reducing the number of trainable parameters, enabling it to generalize well,
and improving computational efficiency [17]. Together, these inductive biases not only reduce the model’s complexity
but also add prior knowledge about the structure of visual data, which accelerates training and improves performance,
especially in scenarios with limited labeled data. Although CNNs’ intrinsic inductive biases can be very effective when
data is insufficient, they can be limiting when there is sufficient data. In this context, it is necessary to select the most
effective inductive biases rather than employing all available ones [22]. The benefit of inductive biases tends to diminish
with very large datasets, which implies that transfer learning scenarios, where only a limited number of examples are
available for the new distribution, provide a valuable context for assessing the effectiveness and implementation of these
biases [30]. It is due of these characteristics that a randomly initialized CNN can perform well in object localization
tasks without any learning [13]. Despite all these advantages, the locality characteristic of CNNs causes them to degrade
in their ability to capture long-range dependencies [22].
ViTs have demonstrated remarkable effectiveness in modeling spatial hierarchies and capturing global dependencies
within data. Unlike CNNs, which are specifically designed to operate with local data by spatially bounded filters,
ViTs depend on self-attention mechanisms where every token (or image patch) can attend to any other token in the
image regardless of their spatial positions [21]. In the ViT architecture, while Multi-Layer Perceptron (MLP) layers
function locally and maintain a degree of translational equivariance, the self-attention layers are inherently global,
enabling ViTs to integrate information across the entire spatial domain at every layer. Rather than relying on the
strong inductive biases embedded within CNNs, such as locality, translation equivariance, and weight sharing, ViTs
replace these constraints with global processing achieved through multi-head self-attention. This design allows ViTs to
learn flexible and context-dependent representations by dynamically attending to different parts of the image. As a
result, ViTs are capable of incorporating broader contextual cues at earlier stages in the network, which can lead to
superior performance in tasks that benefit from understanding long-range relationships between image regions. On the
other hand, CNNs must build up such spatial relationships incrementally through repeated layers of local convolutions,
making the learning process more architecture-biased than data-driven.
However, this lack of built-in inductive biases also presents some challenges. The absence of explicit mechanisms for
capturing local details can make ViTs less effective at modeling fine-grained patterns, especially in the early layers
[71]. Additionally, ViTs exhibit high sensitivity to the choice of hyperparameters, such as the optimizer, learning rate
schedule, and network depth, which makes their training more delicate and often requires large-scale datasets and
8


===== PAGE 9 =====
S.MEHDIPOUR ET AL.
computational resources [109]. Surprisingly, despite these architectural differences, recent results show that ViTs can
implicitly learn some of the inductive biases that are commonly found in CNNs. For instance, early self-attention heads
in ViTs prefer to attend to local regions, mimicking the effect of convolutional kernels [109]. This observation shows
that with sufficient training data and good optimization, ViTs are capable of learning some of the beneficial priors that
CNNs have by nature. While ViTs offer a more flexible and globally conscious architecture for visual representation
learning, they lack the strong, human-crafted inductive biases that cause CNNs to excel so thoroughly in data-scarce
regimes. This trade-off between flexibility and architectural bias underscores the importance of understanding when
and how to use ViTs effectively, particularly with respect to the scale and nature of the data involved.
3.3
Strategies to Mitigate the Lack of Inductive Biases in ViTs
ViTs lack the explicit inductive biases that are inherent in CNNs, such as locality and weight sharing, which can
make it challenging for them to learn effectively, especially with limited data. One approach to mitigate this lack of
inductive biases involves injecting convolutional inductive biases into ViTs. This can be done through convolutional
patch embedding, which modifies how image data is turned into input tokens for the Transformer. Specifically, it
replaces or enhances the original flat patch embedding with convolutional layers to capture local spatial patterns more
effectively. Alternatively, a convolutional stem can be added at the beginning of the network to create hybrid models,
helping the model learn local features more effectively.
Another strategy involves using knowledge distillation from a teacher model that possesses strong inductive biases.
Distilling the knowledge in a neural network involves training a smaller neural network, known as the student or distilled
model, to replicate the behavior of a larger, more complex model or ensemble of models, often referred to as the teacher
or cumbersome model. The main goal is to transfer the broader understanding and generalization capability learned by
the larger model into the smaller one [36]. In this approach, one or more pre-trained CNN models are often used as the
teacher to transfer their inductive biases to the ViT student model [77]. This method has been shown to improve the
performance of ViTs, particularly on smaller datasets.
Using hierarchical structures is another way to mitigate the lack of inductive biases by introducing a multi-stage
architecture that processes visual information similarly to CNNs. Instead of attending globally from the start, these
models apply self-attention within local windows and gradually merge or downsample tokens across layers, creating
a spatial hierarchy [57]. This setup embeds locality and spatial coherence, which are key inductive biases of CNNs,
allowing the model to initially capture low-level features locally and subsequently build more abstract, global represen-
tations [107]. Therefore, the most common approaches used to mitigate the lack of inductive biases in ViTs can be
summarized as follows:
• Convolutional Patch Embedding: It involves replacing simple patch embedding with convolutional layers to
preserve local structures in the image, or employing recursive token aggregation, which functions similarly to
convolutions in capturing local spatial relationships. This strategy is utilized in models such as Tokens-to-Token
ViT (T2T-ViT) [114].
• Hybrid Models: One way to mitigate the lack of inductive biases in ViTs is to use a CNN or convolutional
backbone to extract low-level features and then feed this output into a ViT. This approach combines the spatial
inductive biases of CNNs with the global modeling capabilities of ViTs. Introducing a few convolutional
inductive biases in the early stages of ViT processing can strike a balance between inductive bias and the
learning capacity of Transformer blocks, while also influencing the optimization behavior of ViTs [109].
• Knowledge Distillation: Distilling knowledge from CNNs into ViTs during training is another possible
solution, as exemplified by DEiT [101], which uses a CNN as the teacher and a ViT as the student to improve
performance through teacher–student learning during the training process.
• Hierarchical Structure: Designing ViTs with hierarchical architectures to process multi-scale features is
what Swin Transformer (SwinT) does, which results in improved efficiency and local feature awareness [57].
4
Applications of ViTs in Precision Agriculture
ViTs have been increasingly applied to plant disease detection, leveraging their powerful feature extraction and
attention mechanisms. Various studies have explored different adaptations of ViTs to improve performance in this
domain. These adaptations can be broadly categorized into two types: those that use ViTs and its variants in their pure
form or with minor modifications, which we categorize as Pure Models, and those that implement significant changes
to the architecture and combine it with convolutional characteristics, which we categorize as Hybrid Models.
4.1
Pure Models
In this section, we review studies that employ pure ViT models or their variants with minor modifications. These
modifications include fine-tuning or other adjustments that maintain the overall ViT structure but aim to improve the
model’s performance. In Table 1, we provide a summary of such models. The main goals of these studies are to
9


===== PAGE 10 =====
S.MEHDIPOUR ET AL.
implement task-specific models through fine-tuning on specific datasets, to reduce the number of model parameters to
decrease resource consumption, and to create lightweight models for real-world deployment.
A simple use case of ViT was shown in a study that proposed a ViT-based approach for the automatic classification
of strawberry diseases using transfer learning and fine-tuning techniques [62]. The authors enhanced the ViT model
by adding new layers such as ReLU activation, batch normalization, dropout, and a softmax classifier. Comparative
evaluations against conventional CNN models like VGG16, VGG19 [88], ResNet50V2 [34], and MobileNet [39]
showed that the proposed ViT model, especially when combined with data augmentation, significantly outperformed
other methods. In their best setting, the model achieved an accuracy and F1-score of 92.7% on the test set. The research
highlighted the strengths of Transformer-based architectures in precision agriculture, especially in complex visual
tasks such as disease recognition from strawberry leaves, fruits, and flowers. The study also evaluated the proposed
method on the PlantVillage dataset to assess generalizability, where it attained 98.9% accuracy, outperforming both the
original ViT and previous state-of-the-art methods like Mask R-CNN [35] and MCLCNN. The authors concluded that
the ViT model’s ability to capture global image features made it a powerful alternative to CNNs for agricultural disease
detection tasks.
In another study, GreenViT [67], a novel ViT-based model, was proposed to improve plant disease detection by
addressing the limitations of traditional CNNs. Unlike CNNs, which may lose spatial information due to pooling
layers, GreenViT segmented input images into smaller patches to effectively capture essential features. The model was
evaluated using three datasets: PlantVillage (PV), Data Repository of Leaf Images (DRLI), and a new Plant Composite
(PC) dataset created by merging the first two datasets. PlantVillage consisted of 54,305 images spanning 38 classes (26
infected, 12 healthy). DRLI included 4,502 images of 12 plant species. GreenViT achieved impressive accuracy rates
of 100% on PV, 98% on DRLI, and 99% on PC, outperforming state-of-the-art models such as VGG16, MobileNetV1,
and ViT-Base. The model’s efficiency was highlighted by its reduced parameter count (21.65 million compared to 86
million in ViT-Base), making it computationally lightweight and suitable for edge devices.
As discussed in Section 3, using a hierarchical architecture was one of the possible solutions to mitigate the lack
of inductive biases in ViTs. SwinT is a type of ViT designed to address challenges associated with traditional ViTs,
such as the high computational cost of processing high-resolution images and the need for a hierarchical structure to
effectively capture features at multiple scales [57]. Several studies employed the SwinT model in precision agriculture
applications. An application of SwinT was introduced in a study [118] that examined Transformer models for accurate
fruit ripeness classification. The authors addressed the problem of performing fruit selection and pickup effectively in
the absence of professional labor. They created their own fruit datasets, including apples and pears, to train, test, and
compare various Transformer models, like ViT, SwinT, and MLP-based models. The MLP model performed well but
was prone to overfitting when it had higher capacity. The moving window-based self-attention in SwinT, as shown
in Figure 5, facilitated better feature extraction of visual objects. The experiments demonstrated that SwinT yielded
the best results, achieving a precision of 87.43% for fruit object detection. This work also combined the Transformer
module and YOLO [75] module to effectively classify the ripeness stages of apples and pears.
Figure 5: The Shifted window mechanism in Swin Transformer based on MSA [118]. The red box indicates a window
where self-attention is applied, while black boxes represent individual image patches. The shaded region B illustrates
the area that is masked out during shifted window self-attention to preserve the locality of each window.
An improved framework of the SwinT architecture [106] was proposed to identify cucumber leaf diseases, addressing
challenges such as complex backgrounds and limited sample sizes. The authors developed a SwinT-based and attention-
guided GAN (STA-GAN) model capable of generating diseased images without altering background integrity. The leaf
extraction module (Figure 6), which included the proposed backbone network and Grad-CAM [82], was incorporated
into the GAN to create STA-GAN (Figure 7). They employed a modified SwinT model, which used an improved feature
extraction technique of step-wise small patch embeddings and Coordinate Attention (CA), without adding any extra
parameters. Their results showed that STA-GAN and the improved SwinT had the potential to minimize dependency on
large manually annotated datasets, while at the same time improving disease detection.
10


===== PAGE 11 =====
S.MEHDIPOUR ET AL.
Figure 6: Overview of the proposed leaf extraction module [106]. It includes an improved SwinT-based backbone for
leaf region determination, Grad-CAM for generating a saliency map, and threshold segmentation for extracting the leaf
region from complex backgrounds.
Figure 7: The framework of the proposed STA-GAN [106]. The generator transforms healthy images (A) into diseased
images (B) using a series of convolutional, residual, and transposed convolutional layers. Leaf region masks guide both
the generator and discriminator to focus on leaf areas, enhancing disease translation accuracy.
There are some studies whose main contribution is the use of new self-built or custom datasets. For instance, a
customized ViT model [8] was developed to identify and categorize Java Plum leaf diseases. The dataset used in that
experiment was self-collected and included six classes: Bacterial Spot, Brown Blight, Powdery Mildew, Sooty Mold,
Dry, and Healthy leaves. The study showcased the capabilities of ViT for improving plant disease detection technologies
with respect to the example of Java Plum crops. The model developed in that work surpassed the performance of
traditional CNN-based models and achieved higher detection accuracy. The use of ViTs for plant disease detection
using multispectral images captured in natural environments was explored in a recent study [19]. That work aimed to
assess the performance of ViTs for plant disease recognition applied to multispectral imagery acquired under natural
conditions. The authors created a novel dataset named the “New Multispectral Dataset”, which contained pictures
of five agricultural plants: avocados, tomatoes, lime, passionfruit, and gooseberries. This database included pictures
taken under both visible and near-infrared (NIR) capture modes, thus offering more information for disease analysis
than RGB-only capture. Four ViTs models, namely ViTs-S16, ViTs-B16, ViTs-L16, and ViTs-B32, were trained from
scratch and subsequently fine-tuned using ImageNet-21k weights. The research examined the roles of model size,
patch size, and transfer learning on the models’ ability to classify diseases. The findings indicated that the ViT model
based on the B16 architecture and augmented with transfer learning provided the best results, with training and testing
accuracies of 93.71% and 90.02%, respectively.
11


===== PAGE 12 =====
S.MEHDIPOUR ET AL.
In addition to utilizing a self-built dataset, a novel residual-distilled Transformer architecture [120] was proposed
for the identification of rice leaf diseases. The authors argued that existing works on rice leaf disease identification
were unsatisfactory in terms of either accuracy or model interpretability. To address these issues, they introduced a
method that combined a ViT and a distilled Transformer in a residual block for feature extraction, followed by a MLP
for prediction. The researchers collected their own dataset on rice leaf disease between 2019 and 2020. This dataset,
which they referred to as the rice leaf disease dataset, consisted of pictures of four rice leaf diseases: bacterial blight,
brown spots, blast, and tungro. The proposed method achieved an 89% F1-score and 92% top-1 accuracy on the rice
leaf disease dataset, outperforming other state-of-the-art models.
In another work, LEViT [69] was proposed as a model based on ViT for identifying and classifying leaf diseases in
agriculture, with an emphasis on enhancing interpretability through Explainable AI techniques. Integrating Grad-CAM
provided a visual explanation of the model’s predictions in the form of important regions in the images of the leaves.
Their model achieved high performance, with a training accuracy of 95.22%, validation accuracy of 96.19%, and test
accuracy of 92.33%. Notably, Grad-CAM visualizations enhanced the trustworthiness and usability of the model by
elucidating its decision-making processes. Their findings showed that LEViT’s advanced structure helped to improve
leaf disease recognition and diagnosis, which was essential in precision farming. The integration of interpretability
enabled LEViT to address real-world issues of early disease detection and management.
As we mentioned, one of the approaches that can be taken is to create efficient models by reducing parameters or
employing transfer learning strategies. An innovative transfer learning strategy designed for plant disease recognition in
scenarios with limited data was proposed in a recent study [111]. Pre-training was performed using the PlantCLEF2022
dataset, which contained 2,885,052 images across 80,000 classes [44]. The authors employed a ViT model that was
initially self-trained on ImageNet and subsequently trained on the PlantCLEF2022 dataset. Several benchmark models,
such as CNNs and other ViT-based systems, were used to validate the proposed method. The results indicated that
the model outperformed the previous models in all dataset configurations, including few-shot learning scenarios. For
instance, it achieved remarkable results even when trained with only 20 images per class, and it also performed well in
plant growth stage prediction and weed recognition. Moreover, the dual transfer learning approach improved accuracy
while also demonstrating faster convergence, showcasing the strength of the method across various dataset types. This
study underscored the effectiveness of large-scale, plant-related datasets combined with advanced models like ViT
for transfer learning. It emphasized the necessity of appropriate source datasets, self-supervised pre-training, and
fine-tuning processes for plant disease recognition.
In another work [108], the SwinT network was used to improve the identification of weeds in agriculture. To address
challenges posed by limited training data at scale, a two-stage transfer learning strategy was employed. First, the authors
pre-trained the SwinT model on the ImageNet dataset and then fine-tuned it on the Plant Seedlings dataset as well as on
a private dataset named MWFI (Maize/Weed Field Image). This strategy leveraged pre-trained network parameters to
specialize in feature extraction for weed recognition. The proposed method achieved a recognition accuracy of 99.18%,
outperforming other CNN-based architectures such as EfficientNetV2 [95].
LeafViT, a ViT-based model tailored for detecting leaf diseases in plants, specifically targeted fine-grained features
critical for classification [47]. Experiments underscored the robustness of LeafViT across various configurations,
validating its computational efficiency relative to models such as VGG16. The authors highlighted its potential for
deployment on edge devices and its adaptability for object detection and segmentation tasks. Another work [6]
introduced a novel application of ViT models for detecting diseases in tomato plants, leveraging smartphone-based
technology. A smartphone application developed as part of that study integrated the ViT model for real-time, field-level
disease detection. By deploying the model on a smartphone application via TensorFlow Lite, that work provided a
practical tool for field use, enabling farmers with real-time insights to enhance crop health and yield.
In another study, IEM-ViT was proposed as an innovative approach to detect tea leaf diseases [116]. It incorporated
an information entropy weighting (IEW) method to prioritize regions within images that held significant information,
enhancing the feature extraction process. This model used a masked autoencoder (MAE) with an asymmetric encoder-
decoder architecture, effectively masking 75% of the input image to filter out redundant and irrelevant background
data. This method enabled proper model development and facilitated accurate disease recognition even when low-
quality images were used. The findings indicated that the IEM-ViT model surpassed previously developed algorithms.
Therefore, IEM-ViT was considered suitable for fast and large-scale tea disease recognition and was proposed as
deployable for recognizing diseases in other crops, contributing to enhanced agricultural management and sustainability.
FormerLeaf [96], a Transformer-based cassava leaf disease classification model, was introduced in a recent study.
The model consisted of 12 Transformer encoder layers with optimized self-attention mechanisms, which helped reduce
resource usage while maintaining high classification accuracy. To enhance FormerLeaf, two optimization techniques
were proposed: the Least Important Attention Pruning (LeIAP) algorithm and Sparse Matrix-Matrix Multiplication
(SPMM). LeIAP identified and removed less critical attention heads, resulting in a 28% reduction in model size and a
15% increase in inference speed without significant accuracy loss. SPMM reduced computational complexity from
O(n2)O(n2) to O(n2/p)O(n2/p), improving training efficiency by 10% and lowering resource consumption. These
improvements made FormerLeaf suitable for real-world applications in precision agriculture, where computational
12


===== PAGE 13 =====
S.MEHDIPOUR ET AL.
efficiency was critical. The authors also mentioned challenges, including dataset imbalance and variable image quality
due to diverse lighting conditions. To address these, they suggested using advanced augmentation techniques, such as
GANs [29], to improve model performance.
Table 1: Summary of pure models. These models employ the pure form of ViT, aiming to enhance its performance or
fine-tune it. (Reported metrics are directly extracted from the original publications. In certain studies, per-class metrics
are provided; metrics that are not reported are indicated with a "-".)
Paper
Dataset(s)
Accuracy
Precision
Recall
F1
Main Contribution
[62]
PlantVillage and Strawberry
Disease Detection dataset
92.7%
-
-
0.927
Fine-tuning a ViT model with additional dense
and normalization layers for precise strawberry
disease classification
[118]
Self-built
-
0.8743
-
-
Adopt SwinT for accurate ripeness classification
of various fruit types
[106]
Self-built
98.97%
-
0.9873
-
Improving patch partition of SwinT and using
SwinT-based Attention-guided GAN
[8]
Self-built
97.51%
Per-class
Per-class
Per-class
Introducing customised ViT model for effective
classification of Java Plum leaf diseases
[120]
Self-built
92%
0.88
0.91
0.89
Residual-distilled Transformer architecture for
rice leaf disease identification
[69]
New Plant Diseases Dataset
92.33%
-
-
-
ViT-based model incorporating Explainable AI
(XAI) features
[111]
PlantCLEF2022
Varies
-
-
-
Novel transfer learning strategy for versatile
plant disease recognition
[108]
Plant Seedling and a self-built
dataset
99.18%
0.9933
0.9911
0.9922
Fine-grained recognition using SwinT and two-
stage transfer learning
[47]
Plant Pathology 2020
97.15%
Per-class
Per-class
Per-class
ViT-based approach to identify and amplify sub-
tle discriminative regions
[6]
PlantVillage
90.99%
Per-class
Per-class
Per-class
Smartphone-based plant disease detection
[67]
PlantVillage
DRLI
PlantComposite
100%
98%
99%
-
-
-
-
-
-
-
-
-
Reducing number of learning parameters
[116]
Tea leaves disease classifica-
tion (Kaggle)
93.78%
0.9367
0.9380
0.9364
Asymmetric encoder-decoder architecture for
masked autoencoder with selective patch encod-
ing and pixel reconstruction
[96]
Cassava Leaf Disease Dataset
(Kaggle)
98.52%
-
-
0.9682
Proposing LeIAP algorithm for selecting key
attention heads in Transformer
4.2
Hybrid Models
In this section, we review models that use ViT with specific adjustments, particularly those that combine ViT with
certain CNN characteristics. A summary of the papers is shown in Table 2.
In a comparative study [9], the authors evaluated the performance of standalone and hybrid models and proposed a
lightweight DL approach utilizing ViT for automated plant disease classification. They compared ViT-based models,
CNNs, and hybrid architectures that combined both techniques. These models were evaluated on three different datasets:
the Wheat Rust Classification Dataset (WRCD), the Rice Leaf Disease Dataset (RLDD), and the PlantVillage dataset. A
ViT-based model achieved a 100% F1-score on WRCD with a 200×200 resolution, outperforming EfficientNetB0 [94]
in computational efficiency. For RLDD, hybrid models balanced accuracy and computational cost, with the ViT-based
model achieving the highest average precision. On the larger PlantVillage dataset, the ViT-only model reached an
F1-score of 98.77%, surpassing CNN-based architectures. Models that combined attention blocks and CNNs as hybrid
models offered a balance between accuracy and speed. They were comparatively faster than pure ViT models and
more accurate than CNN-only models. For instance, a hybrid model achieved high accuracy in the PlantVillage
experiments, indicating the potential of integrating attention mechanisms into CNNs for real-world applications. It
is worth mentioning that this study highlighted the need for a practical, lightweight, real-time implementation for
agricultural purposes. Multi-label classification with object localization for detailed disease identification was identified
as the focus of future research.
EfficientRMT-Net [83] is a novel hybrid model that integrated ResNet-50 and ViT in order to classify potato plant
leaf diseases effectively. EfficientRMT-Net employs depth-wise convolution for computational efficiency and utilizes
a stage-block architecture to enhance scalability and feature sensitivity. This paper emphasizes that the ability of
EfficientRMT-Net to process high-dimensional data efficiently while maintaining high accuracy positions it as a
valuable tool for smart farming. Another paper [115] proposed SEViT model combining Squeeze-and-Excitation [41]
13


===== PAGE 14 =====
S.MEHDIPOUR ET AL.
ResNet101 and ViT to tackle large-scale and fine-grained plant disease classification challenges. The preprocessing
network employs Squeeze-and-Excitation modules in ResNet101 to enhance channel-specific feature representation,
while the ViT component leverages its global attention mechanism for feature extraction and classification. SEViT
improves upon the limitations of CNN-based methods, which often struggle to distinguish diseases in visually similar
crops. Although comparisons with popular models such as MobileNetV3 [38], EfficientNet, and VGG16 highlight the
efficiency of SEViT in handling visually similar diseases across different crops, SEViT faces challenges including high
computational requirements due to the depth of the preprocessing network and its reliance on pretrained weights to
optimize the performance of ViT.
An enhanced ViT model [63] based on the MaxViT [103] architecture was introduced for identifying diseases
in maize leaves. By adapting the MaxViT structure with the addition of Squeeze-and-Excitation (SE) blocks and
implementing Global Response Normalization (GRN) in the MLP layers, the proposed model achieved significant
improvements in both accuracy and inference speed. The researchers created a comprehensive maize dataset by merging
the PlantVillage, PlantDoc, and CD&S datasets, forming the largest publicly available maize disease dataset. This
combined dataset, featuring four disease classes, was split into training, validation, and testing sets to rigorously evaluate
the generalization capabilities of DL models. The model was benchmarked against 28 CNN and 36 ViT architectures. It
achieved a record-breaking accuracy of 99.24% on the test set, outperforming existing methods in both accuracy and
inference speed, making it particularly well-suited for real-world agricultural applications.
A Spatial Convolutional Self-Attention-based Transformer (SCSA-Transformer) [51] for strawberry disease identifi-
cation was proposed to address challenges such as complex image backgrounds and dataset imbalances. The architecture
incorporated hierarchical feature mapping through convolutional modules and multi-head self-attention to capture
dependencies across diverse image regions. Experimental results demonstrated that the proposed method achieved
an accuracy of 99.10% and underscored the SCSA-Transformer’s capability for precise disease recognition, even in
complex scenarios, with faster convergence and reduced computational requirements compared to alternative methods.
In another study, a novel Convolutional SwinT (CST) model was proposed to address challenges in plant disease
detection [31]. By integrating convolutional layers into the SwinT architecture, the CST model enhanced feature
extraction from noisy and complex agricultural images. The study evaluated the model on multiple datasets: the
Cucumber Plant Diseases Dataset, Banana Leaf Disease Images, Potato Disease Leaf Dataset, and a tomato subset of
PlantVillage. Notably, CST maintained an accuracy of 0.795 even with 30% salt noise, demonstrating its robustness.
In addition to its strong classification performance, CST showed notable resilience to noise, outperforming baseline
models such as ResNet50 and LeViT-192 under salt noise conditions. While CST outperformed existing models in
noisy data accuracy, its computational complexity, which exceeded 48 million parameters, was noted as a limitation.
In a recent work [85], an automated system was proposed for identifying two prevalent mango leaf diseases,
anthracnose and powdery mildew, commonly found in the Andhra Pradesh region of India. The authors employed
Compact Convolutional Transformer (CCT) models, specifically CCT-7/8×2 and CCT-7/4×2, to classify diseased leaves
using DL-based ViT architectures. These models were selected for their efficiency in handling small datasets with
reduced computational requirements. The experimental evaluation was conducted using a self-built mango leaf image
dataset collected from over 50 trees. The dataset comprised 574 images labeled across four classes: healthy, dead,
anthracnose, and powdery mildew. The study confirmed that compact Transformers like CCT were highly effective for
disease detection in resource-limited agricultural contexts. The authors planned to extend their work by incorporating
additional disease categories and expanding the dataset to improve the generalizability of the proposed model.
A novel approach using ViTs was introduced to address challenges in paddy leaf disease identification [79]. The model
incorporated a multi-scale contextual feature extraction mechanism to capture both local and global representations
of lesions on paddy leaves. This was complemented by a weakly supervised Paddy Lesion Localization (PLL) unit
that prioritized significant lesion areas to enhance classification accuracy. The model’s performance was evaluated
on the Paddy Doctor dataset [68]. The proposed model outperformed several state-of-the-art models, including
DenseNet169 [42], ResNet50, and SwinT, achieving an accuracy of 98.74%, an F1-score of 98.18%, and an AUC
of 99.49%. These results demonstrated significant improvements over baseline models, highlighting the efficacy of
lesion-focused feature extraction and context-aware learning in accurately classifying complex diseases. The authors
emphasized that integrating lesion localization with multi-scale ViT structures enabled robust performance across
diverse visual challenges. The model’s enhanced ability to identify and prioritize discriminative lesion areas provided
more reliable disease detection, supporting its potential for real-world applications in precision agriculture.
A Hybrid Pooled Multihead Attention (HPMA) model [81] was proposed for agricultural pest classification, enhancing
both local and global feature extraction in images. HPMA integrated hybrid pooling and ViT techniques to address
the limitations of conventional attention mechanisms and CNN models. It utilized a novel attention mechanism that
combined max and min pooling to improve feature weighting and discriminative power. The model was tested on a
newly collected dataset of 10 pest classes, achieving a testing accuracy of 98%. Its effectiveness was further validated
on two benchmark datasets: a medium dataset (MD) with 40 classes and a large dataset (LD) with 102 classes. For the
MD, HPMA achieved a testing accuracy of 98.02%, while for the LD, it reached 95.98%. These results emphasized the
14


===== PAGE 15 =====
S.MEHDIPOUR ET AL.
model’s adaptability to varying dataset sizes and complexities. Comparisons with state-of-the-art CNN and Transformer
models highlighted HPMA’s efficiency, offering competitive accuracy with relatively lower computational requirements.
An innovative approach for accurate tomato disease identification [56] was presented through the development
of the NanoSegmenter model, which integrated Transformer structures with lightweight techniques such as the
inverse bottleneck, quantization, and sparse attention mechanisms. This method aimed to balance high precision
with computational efficiency, addressing challenges in traditional models such as detailed instance segmentation and
efficient deployment on mobile devices. The use of lightweight processing enabled smartphone integration, which was
vital for practical field use by farmers, streamlining disease detection and supporting timely agricultural interventions.
The CRFormer [117] model was introduced to address challenges in segmenting grape leaf diseases from complex
natural backgrounds. The model employed a unique cross-resolution mechanism to retain high-resolution (HR) and low-
resolution (LR) feature maps in parallel, enhancing contextual understanding. It featured a Large-Kernel Mining (LKM)
attention mechanism for adaptive spatial and channel encoding, and a Multi-Path Feed-Forward Network (MPFFN) for
multi-scale representation. The model used a lightweight Hamburger decoder for effective multi-resolution data fusion.
The results highlighted CRFormer’s capability to handle the complexities of real-world agricultural segmentation tasks.
EfficientNet Convolutional Group-Wise Transformer (EGWT) [23] is a novel architecture for crop disease detection.
It combined the convolutional capabilities of EfficientNet with the hierarchical and lightweight group-wise Transformer
mechanism. This architecture addressed challenges in computational efficiency and parameter constraints while
maintaining high accuracy. EGWT extracted local features via EfficientNet convolution and processed them through
grouped Transformer modules, reducing redundancy and emphasizing relevant spatial relationships. The EGWT
architecture was validated using three benchmark datasets: PlantVillage, Cassava, and Tomato Leaves. Visualization
of intermediate layers confirmed its ability to focus accurately on disease-affected regions. Despite challenges in
recognizing complex Cassava leaf patterns and imbalanced datasets, EGWT demonstrated superior performance across
metrics. Its lightweight design made it an excellent choice for deployment in resource-constrained environments,
potentially transforming automated crop disease diagnosis in agriculture.
Another work proposed an innovative approach to enhance plant disease diagnosis through synthetic data augmenta-
tion. The research introduced LeafyGAN [89], a GAN framework comprising a pix2pix GAN for segmentation and a
CycleGAN for disease pattern generation. By generating synthetic images to address dataset imbalances, LeafyGAN en-
sured robust augmentation, producing high-quality representations of diseased leaves. A MobileViT classification model
[59], selected for its lightweight architecture and computational efficiency, was trained on augmented datasets, achieving
accuracies of 99.92% on the PlantVillage dataset and 75.72% on the PlantDoc dataset. These results highlighted
the efficacy of LeafyGAN in supporting models tailored for low-resource environments. LeafyGAN demonstrated
notable advancements over existing augmentation methods, such as the LFLSeg module and traditional GANs, by
preserving the background during disease pattern translation and producing visually coherent images. The integration
of MobileViT, with its blend of ViT and convolutional operations, underscored the balance between performance
and efficiency. Compared to resource-heavy architectures, MobileViT effectively handled the augmented datasets,
demonstrating scalability for real-world applications. This framework marked a significant step in addressing data
scarcity and computational limitations, paving the way for practical deployment in agriculture for disease diagnosis and
prevention.
One idea for creating a hybrid model is using ensemble models. In a recent study [76], researchers introduced the
MDSCIRNet architecture, a novel DL model combining Depthwise Separable Convolution (DSC) and ViT-based multi-
head attention mechanisms for detecting potato leaf diseases. In addition to the standalone model, hybrid approaches
integrating classical machine learning methods (e.g., SVM, Random Forest, Logistic Regression, and AdaBoost) with
the MDSCIRNet model were proposed. These combinations yielded competitive results, particularly in cases leveraging
ensemble learning techniques. In another study [52], the authors introduced an ensemble learning approach with
hard and soft voting strategies that integrated three CNN models (MobileNetV3, DenseNet201, ResNext50 [110])
and two Transformer models (ViT and SwinT) to classify leaf diseases . Experimental results showed that ensemble
learning improved classification accuracy, with ViT achieving superior performance as a standalone model. The authors
also employed Grad-CAM visualization to highlight the regions of input images most influential for classification
decisions, demonstrating that the models effectively localized disease areas. Their results indicated that ensemble
learning combining CNN and Transformer models provided a powerful approach for effective classification of plant
diseases, which was useful in practical agriculture. A novel ensemble model [45], Residual Swin Transformer Networks
(RST-Nets), was proposed to address the challenges of plant disease recognition, including noise, varying image
intensities, and the subtle differences between healthy and diseased plants. The model integrated residual convolutional
blocks with SwinT, leveraging the latter’s hierarchical architecture for scalable complexity and global context awareness.
Residual connections enhanced the feature extraction process by retaining crucial information from earlier layers. The
model achieved outstanding results, surpassing state-of-the-art models such as ResNet and GoogleNet [92], but its
computational overhead limited its suitability for resource-constrained IoT and edge devices.
Another way to create a hybrid model is to build a multi-stage model that combines CNN and ViT by dividing the
network into sequential stages. This hierarchical approach reduces computational costs while preserving essential
15


===== PAGE 16 =====
S.MEHDIPOUR ET AL.
Figure 8: Overview of the ConvViT architecture [55]. The top row illustrates the overall hierarchical structure of
ConvViT, while the bottom row presents a detailed view of Stage 2. Each stage is composed of N convolutional blocks
followed by M Transformer blocks.
information, enabling robust performance in complex environments. As shown in Figure 8, ConvViT [55], which
combined ViT and CNNs, was developed to identify kiwifruit diseases in complex natural environments. The authors
created a custom dataset of 25,168 images capturing six types of kiwifruit leaf diseases and validated the model’s
performance on this dataset, as well as on the publicly available PlantVillage and AIChallenger2018 datasets. To
enhance feature extraction, the model employed overlapping patch embeddings and alternating convolutional and
Transformer layers, ensuring both global and local feature representation. On the kiwifruit dataset, ConvViT achieved
a top identification accuracy of 98.78%, surpassing benchmarks such as ResNet, ViT, and ResMLP [102], while
maintaining a lightweight design with reduced parameters and FLOPs. Key improvements included overlapping patch
embedding to preserve local image continuity and reduce computational complexity, as well as a multi-stage design
to balance feature extraction efficiency and computational cost. The Transformer’s global attention complemented
CNN’s local feature extraction, making ConvViT highly effective in complex environments with variable lighting,
backgrounds, and noise. The model’s innovative design and lightweight adaptability made it a valuable backbone for
broader identification tasks in agriculture, potentially aiding real-world crop disease management efforts.
A separate study [121] presented a hybrid model combining CNN with Transformer encoders to improve the
accuracy and generalization of crop disease diagnosis. The authors emphasized handling complex backgrounds and
disease similarities, challenges that traditional CNNs struggled to address. Using the PlantVillage dataset, the model
achieved a high recognition accuracy of 99.62%. For real-world applications, datasets with complex backgrounds were
also used: Dataset1 (apple, cassava, and cotton diseases) and Dataset2 (apple scab, cassava brown streak, and cotton
boll blight), where the proposed model attained balanced accuracies of 96.58% and 95.97%, respectively, demonstrating
robust generalization capabilities. The key to the model’s success was the integration of a Transformer encoder to
capture global features, which compensated for the limitation of CNN in extracting global context. Furthermore, a
novel hybrid loss function, combining cross-entropy with Centerloss, was introduced to optimize feature separability
and reduce intra-class variance. In another paper [14] researchers integrated lightweight CNNs and Transformer-based
frameworks, specifically the BEiT model [4]. This fusion leveraged the feature extraction strengths of CNNs and
the contextual learning capabilities of Transformers to detect rice leaf diseases effectively. The model processed
images by dividing them into visual tokens and applying a two-stage mechanism: reconstruction of visual elements and
masked image modeling. The reconstruction stage extracted meaningful patterns, while the masked image modeling
stage predicted hidden sections of the image, enhancing its understanding. To improve interpretability, the Local
Interpretable Model-Agnostic Explanations (LIME) technique was employed, coupled with segmentation using Simple
Linear Iterative Clustering (SLIC), which highlighted the critical regions contributing to predictions. The explainable
nature of the model, supported by attention mapping and segmentation, provided transparency in decision-making,
16


===== PAGE 17 =====
S.MEHDIPOUR ET AL.
fostering trust among end-users in agricultural diagnostics. This architecture was particularly suited for deployment on
resource-constrained devices, enabling real-time application in smart agriculture.
There are also studies that use transfer learning in addition to hybrid models. In a recent research, the authors proposed
a hybrid model, TLMViT [93], for plant disease classification. They incorporated data augmentation techniques, which
addressed class imbalance and mitigated overfitting, along with a two-phase feature extraction process: initial extraction
using pre-trained CNNs models (e.g., VGG16, VGG19, ResNet50, AlexNet, and Inception V3 [48]), and deep feature
extraction using ViT. Final classification was conducted using an MLP classifier. The model achieved a validation
accuracy of 98.81% on the PlantVillage dataset and 99.86% on the Wheat Rust dataset. These results represented a
1.11% and 1.099% improvement in accuracy compared to transfer learning models without ViT. The findings indicated
the effectiveness of combining transfer learning and ViT for deep feature extraction. The authors also highlighted that
TLMViT effectively leveraged pre-trained models to reduce dimensionality and computational complexity. Another
paper [40] proposed the FOTCA model, which combined the strengths of Transformers and CNNs for plant leaf disease
detection. By integrating an Adaptive Fourier Neural Operator (AFNO) with traditional convolutional down-sampling,
FOTCA effectively captured both global and local features. Key innovations in FOTCA included the use of learnable
Fourier features for positional encoding, which enhanced feature representation by mapping images to the frequency
domain. This approach improved robustness to variations such as rotation and scaling, addressing limitations of ViT on
small and medium-sized datasets. Transfer learning on a pre-trained ImageNet model also optimized training efficiency,
enabling quick convergence.
Some studies utilize multi-track models, particularly dual-track (or dual-path) architectures, that integrate two parallel
networks, such as a ViT and a CNN, to leverage the strengths of both. Figure 9 shows COFFORMER [78], a novel
dual-path visual Transformer designed for efficient and interpretable diagnosis of coffee leaf diseases, which integrates
lesion segmentation and disease classification into a unified framework . The segmentation path employs a U-shaped
architecture built on COFFORMER blocks, which use multiscale convolutional pooling for token mixing, while the
classification path incorporates a Coffee Lesion Attention (CLA) module to focus on critical lesion regions. The
framework evaluates both the type and severity of diseases, enabling real-time and explainable disease diagnosis. The
dataset used for evaluation consists of 1,685 images of Arabica coffee leaves captured under controlled conditions
and annotated for five disease types and severity levels. Experimental results demonstrate COFFORMER’s superior
performance compared to state-of-the-art models. For segmentation, it achieved a Dice Similarity Coefficient (DSC) of
98.14% and a mean Intersection over Union (mIoU) of 97.98%, outperforming baselines such as Swin-Unet [12] and
PSPNet [119]. In classification tasks, COFFORMER achieved an accuracy of 99.1% and an F1-score of 99.32% for
disease type detection, alongside 99.12% accuracy for severity estimation. These results demonstrate its robustness in
identifying diverse lesions and disease severity across real-world coffee leaf images.
For classification of citrus diseases, a novel dual-track deep fusion network [46] was proposed that integrates a
CNN-based Group Shuffle Depthwise Feature Pyramid (GSDFP) and a SwinT. In this architecture, the GSDFP branch
extracts local features using convolutional layers and a Feature Pyramid Network (FPN) for multiscale feature fusion,
while the SwinT branch captures global features through hierarchical feature maps and self-attention mechanisms.
The outputs from these branches are fused and passed through a Shuffle Attention (SA) module to enhance contextual
relationships between features. This dual-track design effectively combines local and global feature extraction, enabling
precise classification. The proposed network achieved state-of-the-art results, outperforming existing models. It obtained
a classification accuracy of 98.19%, surpassing benchmarks such as DenseNet-121 (92%) and AlexNet (94.3%). For
classification of apple leaf diseases, a dual-branch model, named DBCoST [87], integrates CNN and SwinT. This
model includes a Feature Fusion Module (FFM) that combines information from both branches to improve accuracy.
To enhance feature integration, the paper also introduces a channel attention mechanism that adjusts the importance
of different feature channels. The model aims to overcome challenges such as noise from the natural environment,
including overlapping branches and fruits in the images. In comparative experiments with state-of-the-art methods,
including EfficientNetV2S and MobileNetV3L, DBCoST demonstrated superior performance in recognizing apple leaf
diseases, particularly in challenging natural environments. In a recent paper, a hybrid DL model [65] was introduced to
detect crop diseases in complex field environments. Recognizing the limitations of conventional CNN-based models in
handling diverse real-world scenarios, the authors proposed a dual-branch network that fused CNNs for local feature
extraction and Transformers for global context modeling. The architecture incorporated a four-layer pyramid structure
and a novel attention mechanism, using multi-head self-attention enhanced by depthwise separable convolutions and
downsampling to reduce computational overhead. This design ensured that critical disease-related areas on leaves
were emphasized while mitigating the influence of noisy backgrounds. To validate their model, the authors curated a
custom dataset of 45,547 images depicting twelve categories of healthy and diseased crops, captured under real-world
field conditions and sourced from both competitive and open datasets. Additionally, DCTN was evaluated on the
publicly available CD&S dataset, which included maize leaf diseases photographed at Purdue University’s agricultural
research facility. DCTN achieved state-of-the-art accuracy rates of 93.01% on their own dataset and 99.69% on
the CD&S dataset, outperforming popular CNN- and Transformer-based models such as ResNet50, EfficientNetB5,
and DeiT-small. These results highlighted DCTN’s robustness and generalization capability in realistic agricultural
17


===== PAGE 18 =====
S.MEHDIPOUR ET AL.
Figure 9: The architecture of COFFORMER. It consists of three main components: a segmentation subnetwork for
detecting biotic stress lesions, a dual-head classification subnetwork for identifying disease type and severity, and a
CLA block that enhances feature representation using guidance from the segmentation decoder [78].
settings, offering a promising direction for practical plant disease diagnostics. The Triple-Branch SwinT Classification
(TSTC) [112] network has been proposed for the simultaneous and separate classification of plant diseases and their
severity. The proposed model uses a multitask feature extraction module with a triple-branch structure, integrating
SwinT for feature extraction and compact bilinear pooling (CBP) for feature fusion. It also employs a deep supervision
module to enhance feature discrimination across both hidden and output layers, improving the model’s accuracy
and generalization. Unlike single-task models, TSTC’s multitask approach avoids species dependency and improves
flexibility, addressing challenges like imbalanced datasets through robust feature fusion and supervision strategies. A
critical contribution of this study lies in its end-to-end multitask framework, which leverages CBP to enhance feature
fusion and deep supervision to improve training efficiency. The findings suggest that TSTC is highly effective in
agricultural applications, providing accurate predictions that can inform targeted disease management strategies.
There are still other hybrid models that introduce additional novelties. A novel multi-label model, LDI-NET, shown
in Figure 10, was proposed for the simultaneous identification of plant type, leaf disease, and severity [113]. The model
stood out due to its single-branch architecture, which avoided complex network designs and excessive categorization.
LDI-NET was built around three main modules: the feature tokenizer, token encoder, and multi-label decoder. The
feature tokenizer integrated the strengths of both CNN and Transformers, combining CNN’s ability to capture local
details with the Transformer’s proficiency in extracting long-range global features. This module tokenized image data
into compact spatial features, enhancing both local and global context awareness. The token encoder module played a
crucial role in enriching the extracted tokens by facilitating information exchange among them through multi-head
self-attention and MLP structures. This design allowed LDI-NET to capture complex relationships among plant type,
disease, and severity features. The multi-label decoder module, which incorporated a residual structure, processed
these context-rich tokens. It integrated shallow- and deep-level features through adaptive feature embeddings and
cross-attention mechanisms to efficiently output multi-label identification results. The results underscored the model’s
ability to handle multi-label identification tasks effectively, demonstrating its potential for enhancing plant disease
detection in practical agricultural applications.
The Spatial Convolutional Self-Attention-based Transformer (SCSA-Transformer) [50] has been proposed to enhance
strawberry disease identification under complex backgrounds. The research addresses challenges like class imbalance,
limited large-scale datasets, and background complexity in agricultural disease detection. The SCSA-Transformer
18


===== PAGE 19 =====
S.MEHDIPOUR ET AL.
Figure 10: The architecture of LDI-NET [113]. It includes a feature tokenizer module that leverages both CNN and
Transformer strengths, a token encoder module for modeling relationships among plant type, disease, and severity, and
a multi-label decoder for selectively extracting features to enable accurate multi-label identification.
leverages convolutional layers to encode spatial features alongside a Transformer module for global feature extraction,
improving efficiency and precision compared to existing models. The proposed model reduced the number of parameters
by nearly half compared to the SwinT, facilitating lighter deployment and faster training. These enhancements
underscore the method’s applicability to real-world agricultural scenarios, particularly for identifying diseases with
diverse and complex visual backgrounds.
A real-time plant disease identification framework called CondConViT [98] has been proposed, which integrates a ViT
with a Conditional Convolutional Neural Network (CondConv) and is further enhanced by a novel attention mechanism
known as Conditional Attention with Statistical Squeeze-and-Excitation (CASSE). To improve generalization and
robustness, the authors introduced a data augmentation technique based on a modified CycleGAN, termed C3GAN,
which synthesized realistic in-field images from healthy plant samples. The model was designed to be lightweight, with
only 0.95 million parameters, and was deployable on edge devices for drone-based surveillance. CondConViT was
evaluated against seven state-of-the-art models and demonstrated superior interpretability and classification performance
across diverse environmental conditions using both Grad-CAM and LIME techniques. They used six datasets, including
five public datasets: PlantVillage [61], Embrapa [5], Plantpathology [100], Maize and Rice [15], and a newly created
in-field drone image dataset named IIITDMJ-Maize [97], which comprised 416 labeled images of maize plants captured
under various weather conditions. C3GAN was employed to augment the IIITDMJ-Maize dataset by generating 300
synthetic images for three maize diseases: common rust, northern leaf blight, and gray leaf spot. The CondConViT
model achieved the highest or near-highest performance across all datasets in terms of accuracy, F1-score, and AUC.
Furthermore, it exhibited strong generalization on unseen raw drone images and outperformed heavier models, making
it suitable for real-time and resource-constrained agricultural applications. In another study [99], a parallel fusion of
CNN and ViT modules was proposed to capture both local and global features. Unlike sequential CNN-ViT hybrids,
this approach enables independent feature extraction followed by fusion using MobileNetV2 [80] and Squeeze-and-
Excitation (SE) blocks, thereby improving efficiency and interpretability. ConViTX is ultra-lightweight, comprising
only 704,882 trainable parameters and requiring 0.647 GFLOPs, making it ideal for deployment on resource-constrained
IoT/edge devices. Explainability was achieved using Grad-CAM and LIME, and the model’s feature discrimination
capability was further validated through t-SNE visualizations. The model was rigorously evaluated on five datasets:
PlantVillage, Embrapa, PlantDoc, a custom in-field maize dataset (IIITDM-Maize), and a combined multi-species
dataset called PlantCOMB. On PlantVillage, ConViTX achieved an accuracy of 99.63%, an F1-score of 99.64%, and an
AUC of 99.96%, outperforming nine state-of-the-art (SOTA) models. On Embrapa, it achieved an F1-score of 95.12%
and an AUC of 99.74%, and on PlantDoc, it retained the top spot in most metrics. On the custom maize dataset, it
achieved 93.02% accuracy, matching the best competitors while showing a 2% higher AUC. On raw drone images,
it reached 61.42% accuracy, surpassing most SOTA models in generalizability. Beyond static evaluation, ConViTX
was deployed on a Jetson Nano-equipped drone and tested on real-time video frames captured from maize farms. It
processed frames at approximately 29 FPS and demonstrated competitive results, notably identifying northern leaf blight
with 58% accuracy and healthy plants with 73% accuracy. Though challenges remain with subtle diseases like common
rust, the model achieved a better balance between speed, accuracy, and interpretability compared to other lightweight
and heavyweight models. Overall, ConViTX proves to be an efficient, interpretable, and deployable solution, opening
a promising avenue for hybrid CNN-ViT architectures in real-world smart agriculture systems. RepAgrViT [18], a
novel lightweight hybrid architecture, was proposed to integrate CNN and ViT principles for plant disease recognition
in Internet of Things (IoT) edge environments. Unlike traditional CNN-ViT hybrids, RepAgrViT employs a unique
dual-stream design consisting of a token mixer and a channel mixer arranged in series to simultaneously process
local and global features. A core innovation is the Bilinear Attention Transformation (BATransform) module, which
19


===== PAGE 20 =====
S.MEHDIPOUR ET AL.
captures long-range dependencies between diseased and healthy leaf regions. Additionally, the model integrates
structural reparameterization techniques to reduce inference complexity while preserving accuracy. At just 9.6M
parameters, the model demonstrates strong computational efficiency while retaining competitive performance. To
optimize hyperparameters efficiently, the authors introduce Lite-AVPSO, a novel Particle Swarm Optimization variant
that incorporates adaptive weighted delayed velocity and neighborhood-based local search mechanisms. This approach
significantly enhances the hyperparameter search process over alternatives like TPE, GA, and Hyperband. Through an
extensive optimization process spanning over 200 trials, the model achieved 98.54% average accuracy across three
plant disease datasets: the Coffee Leaf Dataset , the Plant Pathology Dataset , and the Rice Leaf Disease Dataset. On
these datasets, RepAgrViT achieved 98.03%, 96.99%, and 98.78% accuracy respectively. Overall, this work contributes
a highly efficient and interpretable AI system tailored for precision agriculture in resource-constrained settings.
Table 2: Summary of hybrid models. These models are designed to leverage the complementary strengths of ViT and
CNN architectures. (Reported metrics are directly extracted from the original publications. In certain studies, per-class
or scenario-specific metrics are provided; metrics that are not reported are indicated with a "-".)
Paper
Dataset(s)
Accuracy
Precision
Recall
F1
Main Contribution
[9]
Wheat
Rust
Classification
Dataset (WRCD), the Rice
Leaf Disease Dataset (RLDD),
and PlantVillage
Scenario-
specific
Scenario-
specific
Scenario-
specific
Scenario-
specific
Proposing lightweight DL approach based on
ViT, CNN, and hybrid models for real-time au-
tomated plant disease classification
[83]
PlantVillage
99.24%
Scenario-
specific
Scenario-
specific
Scenario-
specific
Proposing hybrid model of ResNet-50 and ViT
for classifying potato plant leaf diseases
[115]
Gathered from search engines
88.34%
0.8833
0.8761
0.8750
Combining Squeeze-and-Excitation ResNet101
and ViT for large-scale and fine-grained disease
classification
[63]
Combination of PlantVillage,
PlantDoc, and CD&S
99.24%
0.9915
0.9937
0.9926
Adapting MaxViT structure with SE blocks and
implementing Global Response Normalization
in MLP layers for 4-class maize data
[51]
Self-built
99.10%
0.9777
0.9847
0.9775
Proposing SCSA-Transformer to solve straw-
berry disease recognition under complex back-
grounds
[31]
Cucumber, Banana, Potato,
Tomato datasets
Different
in
model
variants
-
-
-
Proposing Convolutional Swin Transformer
(CST) model for robust and accurate plant dis-
ease detection under natural, controlled, and
noisy conditions
[85]
Self-built
94.17%
0.9334
-
0.9459
Proposing Compact Convolutional Transformer
(CCT)-based model for automated detection of
anthracnose and powdery mildew diseases in
mango leaves
[79]
Paddy Doctor
98.74%
0.9853
0.9786
0.9818
Introducing lesion-aware visual transformer
with multi-scale contextual feature extraction
and weakly supervised lesion localization
[81]
Self-built
98%
0.97
0.98
0.97
Introducing Hybrid Pooled Multihead Attention
(HPMA) model that improves pest classifica-
tion by effectively capturing local and global
features
[56]
Self-built
-
0.98
0.97
-
Proposing lightweight NanoSegmenter model
based on Transformer architecture for high-
precision and efficient tomato disease detection,
incorporating inverted bottleneck, quantization,
and sparse attention techniques
[117]
Field-PV, PlantVillage, Syn-
PV
-
Per-class
Per-class
-
Proposing Cross-Resolution Transformer (CR-
Former) with large-kernel attention and multi-
path feed-forward network for accurate grape
leaf disease segmentation in complex back-
grounds
[23]
PlantVillage
Cassava leaves
Tomato leaves
99.8%
84.29%
99.9%
0.9998
0.8107
0.9980
0.9997
0.8205
0.9993
0.9997
0.7929
0.9990
Combining EfficientNet with group-wise Trans-
former for leaf disease detection
[89]
PlantVillage
PlantDoc
99.92%
75.72%
0.996
0.75
0.996
0.74
0.995
0.72
Proposing LeafyGAN, a GAN-based augmen-
tation framework for synthetic leaf disease data
generation, enabling lightweight MobileViT
models to achieve high diagnosis accuracy even
with limited real data
Continued on next page
20


===== PAGE 21 =====
S.MEHDIPOUR ET AL.
Paper
Dataset(s)
Accuracy
Precision
Recall
F1
Main Contribution
[76]
Potato Leaf Dataset (Kaggle)
99.33%
Per-class
-
-
Using ensemble models for potato disease clas-
sification
[52]
PlantVillage & Kaggle
Varies by vot-
ing ways
Varies by vot-
ing ways
Varies by vot-
ing ways
Varies by vot-
ing ways
Using ensemble models for leaf disease classifi-
cation
[45]
PlantVillage
Per-class
Per-class
Per-class
Per-class
Using ensemble of SwinT and residual CNN
models
[55]
PlantVillage
AIChallenger2018
Self-built
99.84%
86.83%
98.78%
0.9850
0.8534
-
0.9898
0.8342
-
0.9865
0.8539
-
Proposing a hybrid model (ConvViT) for ki-
wifruit disease with improved patch embedding
and reduced complexity
[121]
PlantVillage
Custom1
Custom2
99.62%
96.58%
95.97%
-
-
-
Proposing a hybrid model of CNN and Trans-
former to enhance diagnosis in complex scenes
[14]
PlantVillage
Dhan-Shomadhan
97.43%
96.22%
0.97
0.96
0.96
0.95
0.96
0.95
Introducing an interpretable hybrid model of
lightweight CNN and Transformer for rice leaf
disease
[93]
PlantVillage
Wheat Rust Dataset
98.81%
99.86%
0.9872
0.9978
0.9876
0.9965
0.9873
0.9971
Proposing hybrid model combining transfer
learning and Vision Transformer for deep fea-
ture extraction and classification of plant dis-
eases
[40]
PlantVillage
99.8%
-
-
0.9931
Proposing hybrid model combining adaptive
Fourier Neural Operators and CNNs to enhance
global and local feature extraction for plant leaf
disease recognition
[78]
Coffee leaf stresses dataset
99.1%
-
-
0.9932
Proposing a Dual-path ViT for efficient and in-
terpretable diagnosis
[46]
Citrus datasets
98.19%
0.9839
0.9819
-
Proposing a dual-branch network combining
GSDFP (Group Shuffle Depthwise Feature Pyra-
mid) for local multi-scale feature extraction and
SwinT for global context learning for citrus dis-
ease classification
[87]
PlantPathology
97.32%
0.9733
0.9740
0.9736
Introducing DBCoST, a dual-branch architec-
ture that integrates CNN for extracting local
features and SwinT for capturing global infor-
mation, with a Feature Fusion Module (FFM)
to enhance disease identification in apple leaves
[65]
Custom
CD&S
93.01%
99.69%
0.9299
0.9969
0.9301
0.9969
0.9299
0.9969
Proposing the DenseCNNs and Transformer
Network (DCTN), featuring a novel multi-head
self-attention mechanism for accurate detection
of field crop diseases
[112]
AIChallenger2018
99.00%
-
-
Per-class
Proposing the TSTC network, a triple-branch
Swin Transformer model for simultaneous dis-
ease and severity classification, utilizing multi-
task feature extraction, compact bilinear pool-
ing, and deep supervision to enhance perfor-
mance and achieve high accuracy on both tasks
[113]
AIChallenger2018
Varies
by
plant
-
-
-
Proposing LDI-NET, a multi-label network
for simultaneous identification of plant type,
leaf disease, and severity using a single-
branch model, combining CNN and Trans-
former strengths for feature extraction and em-
ploying a multi-label decoder for improved fea-
ture utilization
[50]
Self-built
99.10%
0.9777
0.9847
0.9775
Using Multi-Head Self-Attention (MSA) and
a Spatial Convolutional Self-Attention-based
Transformer (SCSA-Transformer) for accurate
and efficient recognition of multiple strawberry
disease classes
[98]
5 Public, 1 Self-built
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Proposing a real-time plant disease identifi-
cation system using drone-based surveillance,
featuring a lightweight ViT and CNN fusion
model with conditional attention and statistical
squeeze-and-excitation
Continued on next page
21


===== PAGE 22 =====
S.MEHDIPOUR ET AL.
Paper
Dataset(s)
Accuracy
Precision
Recall
F1
Main Contribution
[99]
5 Public
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Proposing an ultra-lightweight and interpretable
hybrid model that combines CNN and ViT in
a parallel architecture for accurate and efficient
plant disease classification, suitable for deploy-
ment on resource-constrained IoT devices
[18]
5 Public , 1 Custom
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Varies
by
dataset
Proposing a novel lightweight hybrid architec-
ture that combines CNN and ViT components
through a dual-stream token and channel mixer,
along with a Bilinear Attention Transformation
module for efficient local-global feature extrac-
tion
5
Findings and Open Challenges
5.1
Key Findings
Several important findings have emerged from the review of ViTs in precision agriculture. ViTs have shown
substantial improvements over traditional CNNs in various agricultural tasks, notably in plant disease detection and crop
monitoring. Their ability to capture long-range relationships within images enables more accurate identification of plant
diseases and pests, even in complex environments. Combining ViTs with CNNs in hybrid models has further boosted
classification accuracy and robustness, making them a promising approach for agricultural challenges. Furthermore,
transfer learning, in which ViTs pre-trained on large datasets are fine-tuned for agricultural applications, has proven
effective in addressing data limitations and improving model performance without requiring vast amounts of labeled
data. We can summarize the key findings as follows:
• ViTs demonstrate strong performance in plant disease detection: Many studies have shown that ViTs
can achieve high accuracy in classifying plant diseases, often outperforming traditional CNN models. Their
ability to learn long-range dependencies in images is particularly beneficial for plant disease detection, where
symptoms can be distributed across the leaf.
• Self-attention mechanism is a key advantage: The self-attention mechanism allows ViTs to focus on the
most important parts of an image without manual feature extraction. This is helpful in detecting small, subtle
spots on leaves. ViTs can dynamically weigh different regions of the input image, ensuring that significant
features receive adequate attention during classification. This helps capture both local and global contextual
information. This approach is different from CNNs, which apply convolution operations uniformly across
an image and can sometimes miss small but significant features. The ability of ViTs to focus on key features
helps improve accuracy and reduce the need for extensive pre-processing of images.
• ViTs can be optimized for improved performance: Transfer learning can be effectively used to enhance
the performance of ViTs for plant disease identification, even with limited data. This involves pre-training
the model on a large dataset and then fine-tuning it on a smaller, disease-specific dataset. Attention head
pruning can reduce model size and improve inference speed by removing less important attention heads
without significant loss of accuracy. Sparse matrix multiplication in self-attention blocks can improve training
efficiency and reduce GPU consumption without sacrificing performance. Knowledge distillation can be used
to train a smaller, faster model that retains the performance of a larger model, which is especially useful in
situations where computational resources are limited.
• Hybrid models are highly effective in improving performance: Combining ViTs and CNNs can leverage
the strengths of both architectures. For example, CNNs, by extracting local features, can help ViTs see better,
allowing the model to capture fine-grained details of the image while maintaining the ability to understand
global relationships and context. This ultimately improves performance in tasks such as plant disease detection
and image classification.
These findings highlight the potential of ViTs in precision agriculture and demonstrate their effectiveness in automated
plant health monitoring systems, contributing to precision farming. These developments underscore the increasing
importance of ViTs in revolutionizing precision agriculture, particularly for tasks that demand high accuracy and
scalability in large-scale agricultural operations.
5.2
Open Challenges
Despite the significant progress made with ViTs in precision agriculture, several challenges still hinder their wider
adoption. A key issue is the absence of inductive biases in ViTs. Unlike CNNs, which take advantage of spatial
hierarchies and local patterns in images through convolutional filters, ViTs do not possess these built-in inductive
biases. Consequently, ViTs require large, high-quality datasets to perform well, making them data-hungry. This reliance
on vast amounts of data can be a major obstacle in agriculture, where acquiring labeled datasets is both costly and
22


===== PAGE 23 =====
S.MEHDIPOUR ET AL.
time-consuming. As a result, ViTs may struggle to reach optimal performance without substantial data augmentation or
pre-training on large-scale datasets like ImageNet.
Another challenge is the high computational cost and resource demands of ViTs. The self-attention mechanism
that enables ViTs to capture long-range dependencies within images is computationally expensive, particularly when
dealing with the high-resolution images common in agricultural applications. This leads to high memory and processing
power requirements, making it difficult to deploy ViTs on resource-limited devices such as drones or edge sensors
used in agricultural fields. Additionally, training ViTs can take considerably longer than training CNN-based models,
which may hinder their use in real-time agricultural applications where fast responses are essential. Lastly, generalizing
across varied agricultural environments presents another significant challenge. While ViTs have demonstrated strong
performance on specific datasets, they may not be as effective in different climates, crop types, or farming practices.
The agricultural landscape is highly diverse, and models trained on data from one region or crop type may fail to adapt
to new environments with different lighting conditions, backgrounds, or plant variations. This lack of environmental
robustness could limit the practical application of ViTs in precision agriculture on a global scale. Therefore, enhancing
the ability of ViTs to generalize across diverse conditions is a critical area for future research. We can summarize the
open challenges as:
• Need for more comprehensive and diverse datasets: Many studies use publicly available datasets such as
PlantVillage, which may not fully represent the variability of real-world scenarios. Real-world conditions
involve a multitude of factors, including diverse plant species, various disease types and stages, and different
environmental and lighting conditions, which are often not captured in standard datasets. Datasets also need
to account for image variations, including different angles, distances, and image quality. The lack of diverse
data can lead to models that are not robust and may not generalize well to new, unseen data. Although some
studies have created their own datasets to address this issue, collecting and annotating large-scale, high-quality
datasets remains expensive and time-consuming. Table 3 shows the number of times each dataset has been
used.
• Improving the interpretability of models: While ViTs have shown promising results, their internal workings
can be less interpretable than those of traditional CNNs. The self-attention mechanisms in ViTs enable the
model to learn complex relationships between different parts of an image, but it is not always clear why the
model makes a particular prediction or which features are most influential in its decision-making process.
This lack of interpretability can make it difficult for farmers and agronomists to trust the model or understand
how to use it effectively. Some studies address this by using techniques such as Grad-CAM to visualize the
areas of an image that are most important for classification. Further research is needed to develop methods
that provide deeper insight into the inner workings of ViTs and the rationale behind their predictions, such as
feature visualization and saliency maps.
• Reducing computational costs: Transformer-based models can be computationally expensive, requiring
significant computing resources. This cost stems from the self-attention mechanism, whose complexity scales
with the input size, making it more difficult to train on larger images and datasets. As a result, deploying
these models on resource-constrained devices, such as smartphones or edge devices, can be challenging,
particularly in areas with limited access to high-performance computing infrastructure. Some studies propose
solutions such as pruning less important attention heads or using sparse matrix multiplication to reduce
computational overhead and enhance practicality in real-world applications. Other approaches, including
knowledge distillation and hybrid models that combine ViTs with CNNs, also aim to improve the efficiency of
ViTs for deployment. Further research is needed to develop more efficient architectures and training techniques
to make ViTs more accessible and suitable for practical use in plant disease detection.
• Addressing the severity of disease: Most existing studies focus only on detecting the presence or absence
of a disease, without considering its severity. However, disease severity can significantly impact crop yield,
and determining it is important for developing appropriate treatment plans. More research is needed to
develop methods that not only detect the presence of a disease but also assess its severity. This may involve
classifying the disease into different stages or using metrics to quantify its extent. Such approaches would
require modifying models to output not just a class label but also a measure of disease severity. Additionally,
datasets may need to be labeled with more detailed information, including both disease classes and severity
levels. Future studies should explore severity assessment methods to support more practical and informed
plant disease management strategies.
23


===== PAGE 24 =====
S.MEHDIPOUR ET AL.
Table 3: Frequency of dataset usage across the analyzed studies. Datasets created by gathering images from the internet
or by combining other datasets are categorized under the ’Custom’ category. Datasets created by the authors themselves
are categorized under the ’Self-built’ category. Datasets that do not have a unique name or DOI link and are simply
accessible from Kaggle are categorized under the ’From Kaggle’ category.
Dataset
Description
Number of use
PlantVillage [61]
Contains over 50,000 images of plant leaves across 14
species and 26 diseases
20
From Kaggle
Other publicly available datasets
14
Self-built
Dataset built by the authors
12
Custom
Datasets gathered from search engines, created by combin-
ing other datasets, or not easily accessible
10
PlantPathology2020
[100]
Contains over 3,000 high-quality real-life images of apple
leaf diseases under varying conditions
4
PlantDoc [90]
Contains over 2,000 images across 13 plant species and 17
disease classes
4
CD&S [2]
Contains over 4,000 corn images comprising field images
and augmented images
2
Embrapa [5]
Contains over 56,000 images of 171 diseases and other
disorders affecting 21 plant species
2
PaddyDoctor [68]
Contains over 16,000 annotated paddy leaf images across
13 classes
1
Plant Seedling [27]
Contains over 900 unique plants belonging to 12 species
1
PlantCLEF2022 [44]
Contains over 2,000,000 images and 80,000 classes
1
DRLI [16]
Contains over 4,000 images of healthy and diseased leaves
1
PlantComposite [66]
Contains over 58,000 images of healthy and diseased leaves
1
Citrus
Fruits
and
Leaves dataset [72]
Contains over 600 images of citrus leaves and 150 images
of citrus fruits, with five classes for each
1
Dhan-Shomadhan [37]
Contains over 1,100 images of five rice leaf diseases
1
Strawberry
Disease
dataset [1]
Contains 2,500 images of seven types of strawberry diseases
1
Maize and Rice dataset
[15]
Contains 500 rice images and 466 maize images
1
These challenges highlight that, while ViTs hold great potential for plant disease detection, much work remains to
ensure that they are robust, accurate, and practical for real-world applications. Addressing these challenges will be
crucial for realizing the full potential of ViTs in precision agriculture.
6
Conclusion
The adoption of ViTs in agriculture represents a significant departure from conventional DL approaches, offering
considerable potential for capturing complex visual patterns without relying on the strong inductive biases inherent
in convolutional architectures. While CNNs have historically demonstrated exceptional performance in agricultural
applications due to their locality and translation equivariance, ViTs present a more adaptable and scalable alternative.
Their effectiveness is further enhanced when combined with specialized training strategies and hybrid architectures
that strategically reintroduce inductive biases, enabling a balanced trade-off between flexibility and domain-specific
24


===== PAGE 25 =====
S.MEHDIPOUR ET AL.
robustness. Collectively, these advancements indicate that ViTs, whether as standalone models or within hybrid
frameworks, are poised to play a pivotal role in the future of agricultural applications.
In this survey, we have traced the evolution of ViT-based approaches in agriculture, from vanilla Transformer models
to hybrid architectures that leverage the strengths of both CNNs and Transformers. The reviewed literature demonstrates
the potential of ViTs across a range of agricultural tasks, including disease identification, yield prediction, and precision
agriculture, with most models outperforming or complementing traditional methods. Despite these advances, several
challenges remain, including data scarcity, high computational requirements, and the need for task-specific model
customization. Addressing these challenges is crucial for fully realizing the potential of ViTs in practical agricultural
applications. As the field progresses, future research should prioritize the enhancement of model efficiency, the
adaptation of models to diverse agricultural domains, and the creation of robust, meticulously annotated agricultural
datasets.
References
[1] Usman Afzaal, Bhuwan Bhattarai, Yagya Raj Pandeya, and Joonwhoan Lee. An instance segmentation model
for strawberry diseases based on mask r-cnn. Sensors, 21(19):6565, 2021.
[2] Aanis Ahmad, Dharmendra Saraswat, Aly El Gamal, and Gurmukh Johal. Cd&s dataset: Handheld imagery
dataset acquired under field conditions for corn disease identification and severity estimation. arXiv preprint
arXiv:2110.12084, 2021.
[3] Aanis Ahmad, Dharmendra Saraswat, and Aly El Gamal. A survey on using deep learning techniques for plant
disease diagnosis and recommendations for development of appropriate tools. Smart Agricultural Technology, 3:
100083, 2023.
[4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv
preprint arXiv:2106.08254, 2021.
[5] Jayme Garcia Arnal Barbedo, Luciano Vieira Koenigkan, Bernardo Almeida Halfeld-Vieira, Rodrigo Veras
Costa, Katia Lima Nechet, Claudia Vieira Godoy, Murillo Lobo Junior, Flavia Rodrigues Alves Patricio, Viviane
Talamini, Luiz Gonzaga Chitarra, et al. Annotated plant pathology databases for image-based detection and
recognition of diseases. IEEE Latin America Transactions, 16(6):1749–1757, 2018.
[6] Utpal Barman, Parismita Sarma, Mirzanur Rahman, Vaskar Deka, Swati Lahkar, Vaishali Sharma, and Manob Jy-
oti Saikia. Vit-smartagri: vision transformer and smartphone-based plant disease detection for smart agriculture.
Agronomy, 14(2):327, 2024.
[7] Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:149–198,
2000.
[8] Auvick Chandra Bhowmik, Md Taimur Ahad, Yousuf Rayhan Emon, Faruk Ahmed, Bo Song, and Yan Li.
A customised vision transformer for accurate detection and classification of java plum leaf disease. Smart
Agricultural Technology, 8:100500, 2024.
[9] Yasamin Borhani, Javad Khoramdel, and Esmaeil Najafi. A deep learning based approach for automated plant
disease classification using vision transformer. Scientific Reports, 12(1):11554, 2022.
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
[11] Ilaria Buja, Erika Sabella, Anna Grazia Monteduro, Maria Serena Chiriacò, Luigi De Bellis, Andrea Luvisi, and
Giuseppe Maruccio. Advances in plant disease detection and monitoring: From traditional assays to in-field
diagnostics. Sensors, 21(6):2129, 2021.
[12] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet:
Unet-like pure transformer for medical image segmentation. In European conference on computer vision, pages
205–218. Springer, 2022.
[13] Yun-Hao Cao and Jianxin Wu. A random cnn sees objects: One inductive bias of cnn and its applications. In
Proceedings Of The AAAI Conference On Artificial Intelligence, volume 36, pages 194–202, 2022.
[14] Amitabha Chakrabarty, Sarder Tanvir Ahmed, Md Fahim Ul Islam, Syed Mahfuzul Aziz, and Siti Sarah Maidin.
An interpretable fusion model integrating lightweight cnn and transformer architectures for rice leaf disease
identification. Ecological Informatics, 82:102718, 2024.
[15] Junde Chen, Jinxiu Chen, Defu Zhang, Yuandong Sun, and Yaser Ahangari Nanehkaran. Using deep transfer
learning for image-based plant disease identification. Computers and electronics in agriculture, 173:105393,
2020.
25


===== PAGE 26 =====
S.MEHDIPOUR ET AL.
[16] Siddharth Singh Chouhan, Uday Pratap Singh, Ajay Kaul, and Sanjeev Jain. A data repository of leaf images:
Practice towards plant conservation with plant pathology. In 2019 4th International Conference on Information
Systems and Computer Networks (ISCON), pages 700–707. IEEE, 2019.
[17] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on
machine learning, pages 2990–2999. PMLR, 2016.
[18] Guowei Dai, Zhimin Tian, Chaoyu Wang, Qingfeng Tang, Hu Chen, and Yi Zhang. Lightweight vision
transformer with lite-avpso hyperparameter optimization for agricultural disease recognition. IEEE Internet of
Things Journal, 2025.
[19] Malithi De Silva and Dane Brown. Plant disease detection using vision transformers on multispectral natural
environment images. In 2023 International Conference on Artificial Intelligence, Big Data, Computing and Data
Communication Systems (icABCD), pages 1–6. IEEE, 2023.
[20] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with
fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.
[21] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020.
[22] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit:
Improving vision transformers with soft convolutional inductive biases. In International conference on machine
learning, pages 2286–2296. PMLR, 2021.
[23] Jing Feng, Wen Eng Ong, Wen Chean Teh, and Rui Zhang. Enhanced crop disease detection with efficientnet
convolutional group-wise transformer. IEEE Access, 12:44147–44162, 2024.
[24] Konstantinos P Ferentinos. Deep learning models for plant disease detection and diagnosis. Computers and
electronics in agriculture, 145:311–318, 2018.
[25] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural
networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine
Learning, pages 3165–3176. PMLR, 2020.
[26] Tanya Garg, Padmanabh Dwivedi, Manoj K Mishra, Naveen Chandra Joshi, Neeraj Shrivastava, and Vaib-
hav Mishra. Artificial intelligence in plant disease identification: Empowering agriculture. In Methods in
Microbiology, volume 55, pages 179–193. Elsevier, 2024.
[27] Thomas Mosgaard Giselsson, Rasmus Nyholm Jørgensen, Peter Kryger Jensen, Mads Dyrmann, and Henrik Skov
Midtiby. A public image database for benchmark of plant seedling classification algorithms. arXiv preprint
arXiv:1711.05458, 2017.
[28] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press Cambridge, 2016.
[29] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.
[30] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. Proceedings of
the Royal Society A, 478(2266):20210068, 2022.
[31] Yifan Guo, Yanting Lan, and Xiaodong Chen. Cst: Convolutional swin transformer for detecting the degree and
types of plant diseases. Computers and Electronics in Agriculture, 202:107407, 2022.
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional
networks for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 37(9):1904–
1916, 2015.
[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
European conference on computer vision, pages 630–645. Springer, 2016.
[35] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE
international conference on computer vision, pages 2961–2969, 2017.
[36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
[37] Md Fahad Hossain. Dhan-shomadhan: A dataset of rice leaf disease classification for bangladeshi local rice.
arXiv preprint arXiv:2309.07515, 2023.
26


===== PAGE 27 =====
S.MEHDIPOUR ET AL.
[38] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun
Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 1314–1324, 2019.
[39] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.
arXiv preprint arXiv:1704.04861, 2017.
[40] Bo Hu, Wenqian Jiang, Juan Zeng, Chen Cheng, and Laichang He. Fotca: hybrid transformer-cnn architecture
using afno for accurate plant leaf disease image recognition. Frontiers in Plant Science, 14:1231903, 2023.
[41] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 7132–7141, 2018.
[42] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708,
2017.
[43] C Jackulin and SJMS Murugavalli. A comprehensive review on detection of plant disease using machine learning
and deep learning approaches. Measurement: Sensors, 24:100441, 2022.
[44] Alexis Joly, Hervé Goëau, Stefan Kahl, Lukáš Picek, Titouan Lorieul, Elijah Cole, Benjamin Deneu, Maximilien
Servajean, Andrew Durso, Hervé Glotin, et al. Overview of lifeclef 2022: an evaluation of machine-learning based
species identification and species distribution prediction. In International Conference of the Cross-Language
Evaluation Forum for European Languages, pages 257–285. Springer, 2022.
[45] Ponugoti Kalpana, R Anandan, Abdelazim G Hussien, Hazem Migdady, and Laith Abualigah. Plant disease
recognition using residual convolutional enlightened swin transformer networks. Scientific Reports, 14(1):8660,
2024.
[46] R Karthik, Sameeha Hussain, Timothy Thomas George, and Rashmi Mishra. A dual track deep fusion network
for citrus disease classification using group shuffle depthwise feature pyramid and swin transformer. Ecological
Informatics, 78:102302, 2023.
[47] H Keerthan Bhat, Aashish Mukund, S Nagaraj, and R Prakash. Leafvit: Vision transformers-based leaf disease
detection. In International Conference on Innovations in Computational Intelligence and Computer Vision, pages
85–102. Springer, 2022.
[48] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. Advances in neural information processing systems, 25, 2012.
[49] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. Advances in neural information processing systems, 25, 2012.
[50] Gaoqiang Li, Lin Jiao, Peng Chen, Kang Liu, Rujing Wang, Shifeng Dong, and Chenrui Kang. Spatial
convolutional self-attention-based transformer module for strawberry disease identification under complex
background. Computers and Electronics in Agriculture, 212:108121, 2023.
[51] Gaoqiang Li, Lin Jiao, Peng Chen, Kang Liu, Rujing Wang, Shifeng Dong, and Chenrui Kang. Spatial
convolutional self-attention-based transformer module for strawberry disease identification under complex
background. Computers and Electronics in Agriculture, 212:108121, 2023.
[52] Li-Hua Li and Radius Tanone. Ensemble learning based on cnn and transformer models for leaf diseases classifi-
cation. In 2024 18th International Conference on Ubiquitous Information Management and Communication
(IMCOM), pages 1–6. IEEE, 2024.
[53] Lili Li, Shujuan Zhang, and Bin Wang. Plant disease detection and classification by deep learning—a review.
IEEE Access, 9:56683–56698, 2021.
[54] Lili Li, Shujuan Zhang, and Bin Wang. Plant disease detection and classification by deep learning—a review.
IEEE Access, 9:56683–56698, 2021.
[55] Xiaopeng Li, Xiaoyu Chen, Jialin Yang, and Shuqin Li. Transformer helps identify kiwifruit diseases in complex
natural environments. Computers and Electronics in Agriculture, 200:107258, 2022.
[56] Yufei Liu, Yihong Song, Ran Ye, Siqi Zhu, Yiwen Huang, Tailai Chen, Junyu Zhou, Jiapeng Li, Manzhou
Li, and Chunli Lv. High-precision tomato disease detection using nanosegmenter based on transformer and
lightweighting. Plants, 12(13):2559, 2023.
[57] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-
former: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 10012–10022, 2021.
27


===== PAGE 28 =====
S.MEHDIPOUR ET AL.
[58] Anne-Katrin Mahlein. Plant disease detection by imaging sensors–parallels and specific demands for precision
agriculture and plant phenotyping. Plant disease, 100(2):241–251, 2016.
[59] Sachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision
transformer. arXiv preprint arXiv:2110.02178, 2021.
[60] Sharada P Mohanty, David P Hughes, and Marcel Salathé. Using deep learning for image-based plant disease
detection. Frontiers in plant science, 7:1419, 2016.
[61] Sharada P Mohanty, David P Hughes, and Marcel Salathé. Using deep learning for image-based plant disease
detection. Frontiers in plant science, 7:1419, 2016.
[62] Hai Thanh Nguyen, Tri Dac Tran, Thanh Tuong Nguyen, Nhi Minh Pham, Phuc Hoang Nguyen Ly, and
Huong Hoang Luong. Strawberry disease identification with vision transformer-based models. Multimedia Tools
and Applications, 83(29):73101–73126, 2024.
[63] Ishak Pacal. Enhancing crop productivity and sustainability through disease identification in maize leaves:
Exploiting a large dataset with an advanced vision transformer model. Expert Systems with Applications, 238:
122099, 2024.
[64] Ishak Pacal, Ismail Kunduracioglu, Mehmet Hakki Alma, Muhammet Deveci, Seifedine Kadry, Jan Nedoma,
Vlastimil Slany, and Radek Martinek. A systematic review of deep learning techniques for plant diseases.
Artificial Intelligence Review, 57(11):304, 2024.
[65] Denghao Pang, Hong Wang, Jian Ma, and Dong Liang. Dctn: a dense parallel network combining cnn and
transformer for identifying plant disease in field. Soft Computing, 27(21):15549–15561, 2023.
[66] Sana Parez, Naqqash Dilshad, Turki M Alanazi, and Jong-Weon Lee. Towards sustainable agricultural systems:
A lightweight deep learning model for plant disease detection. Comput. Syst. Sci. Eng., 47(1):515–536, 2023.
[67] Sana Parez, Naqqash Dilshad, Norah Saleh Alghamdi, Turki M Alanazi, and Jong Weon Lee. Visual intelligence
in precision agriculture: Exploring plant disease detection via efficient vision transformers. Sensors, 23(15):
6949, 2023.
[68] Petchiammal, Briskline Kiruba, Murugan, and Pandarasamy Arjunan. Paddy doctor: A visual image dataset
for automated paddy disease classification and benchmarking. In Proceedings of the 6th Joint International
Conference on Data Science & Management of Data (10th ACM IKDD CODS and 28th COMAD), pages
203–207, 2023.
[69] Bh Prashanthi, AV Praveen Krishna, and Ch Mallikarjuna Rao. Levit-leaf disease identification and classification
using an enhanced vision transformers (vit) model. Multimedia Tools and Applications, pages 1–32, 2024.
[70] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[71] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision
transformers see like convolutional neural networks? Advances in neural information processing systems, 34:
12116–12128, 2021.
[72] Hafiz Tayyab Rauf, Basharat Ali Saleem, M Ikram Ullah Lali, Muhammad Attique Khan, Muhammad Sharif,
and Syed Ahmad Chan Bukhari. A citrus fruits and leaves dataset for detection and classification of citrus
diseases through machine learning. Data in brief, 26:104340, 2019.
[73] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing. In
International conference on machine learning, pages 2892–2901. PMLR, 2017.
[74] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object
detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,
2016.
[75] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object
detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,
2016.
[76] Hatice Catal Reis and Veysel Turk. Potato leaf disease detection with a novel deep learning model based on
depthwise separable convolution and transformer networks. Engineering Applications of Artificial Intelligence,
133:108307, 2024.
[77] Sucheng Ren, Zhengqi Gao, Tianyu Hua, Zihui Xue, Yonglong Tian, Shengfeng He, and Hang Zhao. Co-advise:
Cross inductive bias distillation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern
recognition, pages 16773–16782, 2022.
28


===== PAGE 29 =====
S.MEHDIPOUR ET AL.
[78] Abdullah Ali Salamai. Towards automated, efficient, and interpretable diagnosis coffee leaf disease: A dual-path
visual transformer network. Expert Systems with Applications, page 124490, 2024.
[79] Abdullah Ali Salamai, Nouran Ajabnoor, Waleed E Khalid, Mohammed Maqsood Ali, and Abdulaziz Ali Murayr.
Lesion-aware visual transformer network for paddy diseases detection in precision agriculture. European Journal
of Agronomy, 148:126884, 2023.
[80] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4510–4520, 2018.
[81] T Saranya, C Deisy, and S Sridevi. Efficient agricultural pest classification using vision transformer with hybrid
pooled multihead attention. Computers in Biology and Medicine, 177:108584, 2024.
[82] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv
Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the
IEEE international conference on computer vision, pages 618–626, 2017.
[83] Kashif Shaheed, Imran Qureshi, Fakhar Abbas, Sohail Jabbar, Qaisar Abbas, Hafsa Ahmad, and Muhammad Za-
heer Sajid. Efficientrmt-net—an efficient resnet-50 and vision transformers approach for classifying potato plant
leaf diseases. Sensors, 23(23):9516, 2023.
[84] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation.
IEEE transactions on pattern analysis and machine intelligence, 39(4):640–651, 2016.
[85] M Shereesha, C Hemavathy, Hasthi Teja, G Madhusudhan Reddy, Bura Vijay Kumar, and Gurram Sunitha.
Precision mango farming: Using compact convolutional transformer for disease detection. In International
Conference on Innovations in Bio-Inspired Computing and Applications, pages 458–465. Springer, 2022.
[86] Muhammad Shoaib, Babar Shah, Shaker Ei-Sappagh, Akhtar Ali, Asad Ullah, Fayadh Alenezi, Tsanko Gechev,
Tariq Hussain, and Farman Ali. An advanced deep learning models-based plant disease detection: A review of
recent research. Frontiers in Plant Science, 14:1158933, 2023.
[87] Haiping Si, Mingchun Li, Weixia Li, Guipei Zhang, Ming Wang, Feitao Li, and Yanling Li. A dual-branch model
integrating cnn and swin transformer for efficient apple leaf disease classification. Agriculture, 14(1):142, 2024.
[88] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[89] Aadarsh Kumar Singh, Akhil Rao, Pratik Chattopadhyay, Rahul Maurya, and Lokesh Singh. Effective plant
disease diagnosis using vision transformer trained with leafy-generative adversarial network-generated images.
Expert Systems with Applications, page 124387, 2024.
[90] Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar Kumawat, and Nipun Batra. Plantdoc: A
dataset for visual plant disease detection. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, pages
249–253. 2020.
[91] Srdjan Sladojevic, Marko Arsenovic, Andras Anderla, Dubravko Culibrk, and Darko Stefanovic. Deep neural
networks based recognition of plant diseases by leaf image classification. Computational intelligence and
neuroscience, 2016(1):3289801, 2016.
[92] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 1–9, 2015.
[93] Amer Tabbakh and Soubhagya Sankar Barpanda. A deep features extraction model based on the transfer learning
model and vision transformer “tlmvit” for plant disease classification. IEEE Access, 11:45377–45392, 2023.
[94] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning, pages 6105–6114. PMLR, 2019.
[95] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International conference on
machine learning, pages 10096–10106. PMLR, 2021.
[96] Huy-Tan Thai, Kim-Hung Le, and Ngan Luu-Thuy Nguyen. Formerleaf: An efficient vision transformer for
cassava leaf disease detection. Computers and Electronics in Agriculture, 204:107518, 2023.
[97] Poornima Singh Thakur, Shubhangi Chaturvedi, Pritee Khanna, Tanuja Sheorey, and Aparajita Ojha. Real-time
plant disease identification: Fusion of vision transformer and conditional convolutional network with c3gan-based
data augmentation. IEEE Transactions on AgriFood Electronics, 2024.
29


===== PAGE 30 =====
S.MEHDIPOUR ET AL.
[98] Poornima Singh Thakur, Shubhangi Chaturvedi, Pritee Khanna, Tanuja Sheorey, and Aparajita Ojha. Real-time
plant disease identification: Fusion of vision transformer and conditional convolutional network with c3gan-based
data augmentation. IEEE Transactions on AgriFood Electronics, 2024.
[99] Poornima Singh Thakur, Shubhangi Chaturvedi, Ayan Seal, Pritee Khanna, Tanuja Sheorey, and Aparajita Ojha.
An ultra lightweight interpretable convolution-vision transformer fusion model for plant disease identification:
Convitx. IEEE Transactions on Computational Biology and Bioinformatics, 2025.
[100] Ranjita Thapa, Kai Zhang, Noah Snavely, Serge Belongie, and Awais Khan. The plant pathology challenge 2020
data set to classify foliar disease of apples. Applications in plant sciences, 8(9):e11390, 2020.
[101] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
Training data-efficient image transformers & distillation through attention. In International conference on
machine learning, pages 10347–10357. PMLR, 2021.
[102] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier
Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image
classification with data-efficient training. IEEE transactions on pattern analysis and machine intelligence, 45(4):
5314–5321, 2022.
[103] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit:
Multi-axis vision transformer. In European conference on computer vision, pages 459–479. Springer, 2022.
[104] Abhishek Upadhyay, Narendra Singh Chandel, Krishna Pratap Singh, Subir Kumar Chakraborty, Balaji M
Nandede, Mohit Kumar, A Subeesh, Konga Upendar, Ali Salem, and Ahmed Elbeltagi. Deep learning and
computer vision in plant disease detection: a comprehensive review of techniques, models, and trends in precision
agriculture. Artificial Intelligence Review, 58(3):1–64, 2025.
[105] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.
[106] Fengyi Wang, Yuan Rao, Qing Luo, Xiu Jin, Zhaohui Jiang, Wu Zhang, and Shaowen Li. Practical cucumber
leaf disease recognition using improved swin transformer and small sample size. Computers and Electronics in
Agriculture, 199:107163, 2022.
[107] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of
the IEEE/CVF international conference on computer vision, pages 568–578, 2021.
[108] Yecheng Wang, Shuangqing Zhang, Baisheng Dai, Sensen Yang, and Haochen Song. Fine-grained weed
recognition using swin transformer and two-stage transfer learning. Frontiers in Plant Science, 14:1134932,
2023.
[109] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early convolutions help
transformers see better. Advances in neural information processing systems, 34:30392–30400, 2021.
[110] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations
for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 1492–1500, 2017.
[111] Mingle Xu, Sook Yoon, Yongchae Jeong, and Dong Sun Park. Transfer learning for versatile plant disease
recognition with limited data. Frontiers in Plant Science, 13:1010981, 2022.
[112] Bin Yang, Zhulian Wang, Jinyuan Guo, Lili Guo, Qiaokang Liang, Qiu Zeng, Ruiyuan Zhao, Jianwu Wang, and
Caihong Li. Identifying plant disease and severity from leaves: A deep multitask learning framework using
triple-branch swin transformer and deep supervision. Computers and Electronics in Agriculture, 209:107809,
2023.
[113] Bin Yang, Mingwei Li, Fei Li, Yongbo Wang, Qiaokang Liang, Ruiyuan Zhao, Caihong Li, and Jianwu Wang. A
novel plant type, leaf disease and severity identification framework using cnn and transformer with multi-label
method. Scientific Reports, 14(1):11664, 2024.
[114] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and
Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of
the IEEE/CVF international conference on computer vision, pages 558–567, 2021.
[115] Qingtian Zeng, Liangwei Niu, Shansong Wang, and Weijian Ni. Sevit: a large-scale and fine-grained plant disease
classification model based on transformer and attention convolution. Multimedia Systems, 29(3):1001–1010,
2023.
[116] Jiahong Zhang, Honglie Guo, Jin Guo, and Jing Zhang. An information entropy masked vision transformer
(iem-vit) model for recognition of tea diseases. Agronomy, 13(4):1156, 2023.
30


===== PAGE 31 =====
S.MEHDIPOUR ET AL.
[117] Xinxin Zhang, Chaojun Cen, Fei Li, Meng Liu, and Weisong Mu. Crformer: cross-resolution transformer for
segmentation of grape leaf diseases with context mining. Expert Systems with Applications, 229:120324, 2023.
[118] Zhenghua Zhang, Zhangjie Gong, Qingqing Hong, and Ling Jiang. Swin-transformer based classification for
rice diseases recognition. In 2021 International Conference on Computer Information Science and Artificial
Intelligence (CISAI), pages 153–156. IEEE, 2021.
[119] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881–2890, 2017.
[120] Changjian Zhou, Yujie Zhong, Sihan Zhou, Jia Song, and Wensheng Xiang. Rice leaf disease identification by
residual-distilled transformer. Engineering Applications of Artificial Intelligence, 121:106020, 2023.
[121] Weidong Zhu, Jun Sun, Simin Wang, Jifeng Shen, Kaifeng Yang, and Xin Zhou. Identifying field crop diseases
using transformer-embedded convolutional neural network. Agriculture, 12(8):1083, 2022.
31
