

===== PAGE 1 =====
arXiv:2101.00001v1  [cs.LG]  4 Jan 2021
Etat de l’art sur l’application des bandits
multi-bras
1st Djallel Bouneffouf
IBM Research
Djallel.bouneffouf@ibm.com
New York, USA
Abstract—Le domaine des bandits multi-bras connaˆıt
actuellement une renaissance, alors que de nouveaux
param`etres de probl`emes et des algorithmes motiv´es par di-
verses applications pratiques sont introduits, en s’ajoutant
au probl`eme classique des bandits. Cet article vise `a
fournir un examen complet des principaux d´eveloppements
r´ecents dans de multiples applications r´eelles des bandits.
Plus pr´ecis´ement, nous introduisons une taxonomie des
applications communes bas´ees sur le MAB et r´esumons
l’´etat de l’art pour chacun de ces domaines. De plus,
nous identiﬁons les tendances actuelles importantes et
fournissons de nouvelles perspectives concernant l’avenir
de ce domaine en plein essor.
Index Terms—bandit
I. INTRODUCTION
De nombreuses applications pratiques n´ecessitent des
probl`emes de prise de d´ecision s´equentiels, o`u un agent
doit choisir la meilleure action parmi plusieurs alterna-
tives. Des exemples de telles applications incluent les
essais cliniques [1], syst`emes de recommandation [2]–
[11], [11]–[19], [19]–[45], [45]–[92] et la d´etection
d’anomalies [93]. Dans certains cas, des informations
secondaires ou un contexte sont associ´es `a chaque
action (par exemple, le proﬁl d’un utilisateur), et le
retour d’information, ou r´ecompense, est limit´e `a l’option
choisie. Par exemple, dans les essais cliniques, le con-
texte est le dossier m´edical du patient (par exemple,
´etat de sant´e, ant´ec´edents familiaux, etc.), les actions
correspondent aux options de traitement compar´ees et la
r´ecompense repr´esente le r´esultat du traitement propos´e
(par exemple, succ`es ou ´echec). Un aspect important
affectant le succ`es `a long terme dans de tels contextes est
de trouver un bon compromis entre l’exploration (par ex-
emple, essayer une nouveau traitement) et l’exploitation
(choisir le traitement le plus connue `a ce jour).
Ce
compromis
inh´erent
entre
l’exploration
et
l’exploitation existe dans de nombreux probl`emes de
prise de d´ecision s´equentiels, et est traditionnellement
Identify applicable funding agency here. If none, delete this.
formul´e comme le probl`eme des bandit, qui se pr´esente
comme suit:
´Etant donn´e K actions possibles, ou
”bras”, chacun associ´e `a une distribution de probabilit´e
de r´ecompense ﬁxe mais inconnue [94], [95], `a chaque
it´eration, un agent s´electionne un bras `a jouer et rec¸oit
une r´ecompense, ´echantillonn´ee `a partir de la distribution
de probabilit´e du bras respectif ind´ependamment des
actions pr´ec´edentes. La tˆache d’un agent est d’apprendre
`a choisir ses actions aﬁn que les r´ecompenses cumul´ees
au ﬁl du temps soient maximis´ees.
Notez que l’agent doit essayer diff´erentes bras pour
apprendre leurs r´ecompenses (c’est-`a-dire explorer le
gain), et ´egalement utiliser ces informations apprises aﬁn
de recevoir le meilleur gain (exploiter les gains appris).
Il existe un compromis naturel entre l’exploration et
l’exploitation. Par exemple, essayer chaque bras exacte-
ment une fois, puis jouer le meilleur d’entre. Cette
approche est souvent susceptible de conduire `a des
solutions tr`es sous-optimales lorsque les r´ecompenses
des bras sont incertaines. Diff´erentes solutions ont ´et´e
propos´ees pour ce probl`eme, bas´ees sur une formulation
stochastique
[94]–[96] et une formulation bay´esienne
[97]; cependant, ces approches ne tenaient pas compte du
contexte ou des informations secondaires dont disposait
l’agent.
Il est `a not´e que le probl`eme des bandits peut ˆetre
vu comme la forme la plus simple d’apprentissage
par renforcement, dans laquelle l’agent est sans ´etat.
Lorsque le syst`eme a des ´etats, les actions provoquent
des changements d’´etats et les r´ecompenses d´ependent
´egalement des ´etats. Par cons´equent, dans l’apprentissage
par renforcement, les r´ecompenses `a diff´erentes ´etapes
ne sont pas ind´ependantes les unes des autres. En
fait, les algorithmes classiques pour l’apprentissage par
renforcement (avec ´etats) utilisent souvent des solu-
tions au probl`eme des bandits multi-bras comme sous-
programmes pour d´eﬁnir des politiques exploration ex-
ploitation dans l’apprentissage par renforcement. Par ex-


===== PAGE 2 =====
emple, il est bien connu ǫ-greedy L’algorithme de bandits
multi-bras est souvent combin´e avec l’algorithme de pro-
grammation dynamique de Bellman pour l’apprentissage
par renforcement aﬁn de d´eﬁnir les choix d’actions.
En outre, de nombreux algorithmes d’apprentissage par
renforcement, lorsqu’ils sont appliqu´es `a des syst`emes
sans ´etat, se r´eduisent `a des algorithmes de bandit multi-
bras.
Une version particuli`erement utile du MAB est le
contextual multi-arm bandit (CMAB), ou simplement le
contextual bandit, o`u `a chaque it´eration, avant de choisir
un bras, l’agent observe un N-dimensions du contexte,
ou vecteur de features. L’agent utilise ce contexte, ainsi
que les r´ecompenses des bras jou´ees dans le pass´e,
pour choisir quel bras jouer dans l’it´eration actuelle.
Au ﬁl du temps, le but de l’agent est de collecter
sufﬁsamment d’informations sur la relation entre les
vecteurs de contexte et les r´ecompenses, aﬁn qu’il puisse
pr´edire le prochain meilleur bras `a jouer en regardant
le contexte actuel [98], [99]. Diff´erents algorithmes ont
´et´e propos´es pour le cas g´en´eral, dont LINUCB [100],
Neural Bandit [101] et Contextual Thompson Sampling
(CTS) [99], o`u une d´ependance lin´eaire est g´en´eralement
suppos´ee entre la r´ecompense attendue d’une action et
son contexte.
Nous allons maintenant fournir un aperc¸u des di-
verses applications du bandit au probl`emes de la vie
r´eelle (sant´e, r´eseau informatique, ﬁnance, et au-del`a),
ainsi qu’en apprentissage automatique. En particulier,
lorsque les approches bandit peuvent aider `a am´eliorer
le r´eglage des hyperparam`etres et d’autres choix al-
gorithmiques importants dans l’apprentissage supervis´e,
l’apprentissage actif et l’apprentissage par renforcement.
II. APPLICATIONS DES BANDITS
Le bandit stochastique aborde les d´eﬁs associ´es `a
la pr´esence d’incertitude dans la prise de d´ecision
s´equentielle. Ce type d’incertitude a une interaction
complexe avec le dilemme de l’exploration exploitation
et fournit donc un formalisme naturel pour la plupart des
probl`emes de prise de d´ecision.
A. Sant´e
Essais cliniques.
La collecte de donn´ees pour
´evaluer l’efﬁcacit´e du traitement sur des animaux pen-
dant tous les stades de la maladie peut ˆetre difﬁ-
cile lors de l’utilisation de proc´edures conventionnelles
d’allocation de traitement al´eatoire, car de mauvais
choix de traitements peuvent entraˆıner une d´et´erioration
de la sant´e du sujet. Les auteurs de [1] visent `a
concevoir une strat´egie d’allocation adaptative pour
am´eliorer l’efﬁcacit´e de la collecte de donn´ees en al-
louant plus d’´echantillons pour explorer des traitements
prometteurs. Ils pr´esentent cette application comme un
probl`eme de bandit contextuel et introduisent un algo-
rithme pratique d’exploration exploitation dans ce cadre.
Le travail repose sur le sous-´echantillonnage pour com-
parer les options de traitement en utilisant une quantit´e
´equivalente d’informations. Ils ´etendent la strat´egie de
sous-´echantillonnage au contexte de bandit contextuel
en appliquant un sous-´echantillonnage dans la r´egression
avec processus gaussien.
Warfarine est l’anticoagulant oral le plus utilis´e dans
le monde; cependant, l’administration d’un dosage pr´ecis
reste un d´eﬁimportant, car le dosage appropri´e peut
ˆetre tr`es variable entre les individus en raison de divers
facteurs cliniques, d´emographiques et g´en´etiques. Les
m´edecins suivent actuellement une strat´egie `a dose ﬁxe:
les patients commencent avec une dose de 5 mg / jour
(ce qui est la posologie appropri´ee pour la majorit´e
des patients) et ajustent lentement la dose au cours de
quelques semaines en suivant les taux d’anticoagulant
du patient. Cependant, une posologie initiale incorrecte
peut entraˆıner des cons´equences tr`es n´efastes telles qu’un
accident vasculaire c´er´ebral (si la dose initiale est trop
faible) ou une h´emorragie interne (si la dose initiale
est trop ´elev´ee). Ainsi, les auteurs de [102] abordent
le probl`eme de l’apprentissage et de l’attribution d’un
dosage initial appropri´e aux patients en mod´elisant le
probl`eme comme un bandit avec des covariables de haute
dimension, et proposent un nouvel algorithme de bandit
efﬁcace bas´e sur l’estimateur LASSO.
Mod´elisation du cerveau et du comportement.
S’inspirant des ´etudes comportementales de la prise
de d´ecision humaine chez les patients souffrant de
diff´erents troubles mentaux, les auteurs de [43] proposent
un cadre param´etrique g´en´eral pour le probl`eme des
bandits qui ´etend l’approche standard d’´echantillonnage
de Thompson pour incorporer les biais de traitement
des r´ecompenses associ´es `a plusieurs conditions neu-
rologiques et psychiatriques, y compris les maladies
de Parkinson et d’Alzheimer, le trouble de d´eﬁcit de
l’attention/ hyperactivit´e (TDAH), la d´ependance et la
douleur chronique. Ils d´emontrent empiriquement, du
point de vue de la mod´elisation comportementale, que
leur model peut ˆetre consid´er´e comme une premi`ere
´etape vers un mod`ele de calcul uniﬁcateur capturant les
anomalies du traitement des r´ecompenses dans plusieurs
conditions mentales.
2


===== PAGE 3 =====
B. La ﬁnance
Ces derni`eres ann´ees, la s´election s´equentielle de
portefeuilles a suscit´e un int´erˆet croissant `a l’intersection
de l’apprentissage automatique et de la ﬁnance quantita-
tive. Le compromis entre l’exploration et l’exploitation,
dans le but de maximiser la r´ecompense cumulative,
est une formulation naturelle des probl`emes de choix
de portefeuille. Dans [103], les auteurs ont propos´e un
algorithme de bandit pour faire des choix de portefeuille
en ligne en exploitant les corr´elations entre plusieurs
bras. En construisant des portefeuilles orthogonaux `a
partir de plusieurs actifs et en int´egrant leur approche
au cadre des bandits, les auteurs d´erivent la strat´egie
de portefeuille optimale repr´esentant une combinaison
d’investissements passifs et actifs selon une fonction de
r´ecompense ajust´ee au risque. Dans [104], les auteurs
int`egrent la conscience du risque dans le cadre classique
du bandit et introduisent un nouvel algorithme pour
la construction de portefeuille. En ﬁltrant les actifs en
fonction de la structure topologique du march´e ﬁnancier
et en combinant la politique optimale de bandit avec la
minimisation d’une mesure de risque, ils parviennent `a
un ´equilibre entre le risque et le rendement.
C. Tariﬁcation dynamique
Les entreprises de vente en ligne sont souvent
confront´ees au probl`eme de tariﬁcation dynamique:
l’entreprise doit d´ecider des prix en temps r´eel pour
chacun de ses multiples produits. L’entreprise peut mener
des exp´eriences de prix (faire des changements de prix
fr´equents) pour se renseigner sur la demande et max-
imiser les proﬁts `a long terme. Les auteurs de [105]
proposent une politique d’exp´erimentation dynamique
des prix, o`u l’entreprise ne dispose que d’informations
incompl`etes sur la demande. Pour ce param`etre g´en´eral,
les auteurs d´erivent un algorithme de tariﬁcation qui
´equilibre le fait de gagner un proﬁt imm´ediat par rapport
`a l’apprentissage pour les b´en´eﬁces futurs. L’approche
combine un bandit avec une identiﬁcation partielle de
la demande des consommateurs `a partir de la th´eorie
´economique. Semblable `a [105], les auteurs de [106]
consid`erent la tariﬁcation multi-produits dynamique de
haute dimension avec un mod`ele de demande lin´eaire de
faible dimension ´evolutif. Ils montrent que le probl`eme
de maximisation des revenus se r´eduit `a une optimi-
sation convexe de bandit en ligne avec des informa-
tions secondaires donn´ees par les demandes observ´ees.
L’approche applique un algorithme d’optimisation con-
vexe de bandit dans un espace projet´e de faible di-
mension couvert par les caract´eristiques du produit la-
tent, tout en apprenant simultan´ement cette dur´ee via
la d´ecomposition en valeur singuli`ere en ligne d’une
matrice contenant les demandes observ´ees.
D. Syst`emes de recommandation
Les syst`emes de recommandation sont fr´equemment
utilis´es dans diverses applications pour pr´edire les
pr´ef´erences
de
l’utilisateur.
Cependant,
ils
sont
´egalement
confront´es
au
dilemme
exploration-
exploitation lorsqu’ils font une recommandation, car ils
doivent exploiter leurs connaissances sur les ´el´ements
pr´ec´edemment choisis qui int´eressent l’utilisateur, tout
en explorant de nouveaux ´el´ements susceptibles de
plaire `a l’utilisateur. Les auteurs de [107] abordent ce
d´eﬁen utilisant le param`etre bandit, en particulier pour
les syst`emes de recommandation `a grande ´echelle qui
ont un nombre vraiment grand ou inﬁni d’´el´ements. Ils
proposent deux approches de bandit `a grande ´echelle
dans des situations o`u aucune information pr´ealable
n’est disponible. Une exploration continue de leurs
approches peut r´esoudre le probl`eme du d´emarrage
`a froid dans les syst`emes de recommandation. Dans
les
syst`emes
de
recommandation
contextuels,
la
plupart des approches existantes se concentrent sur la
recommandation d’´el´ements pertinents aux utilisateurs,
en tenant compte des informations contextuelles, telles
que l’heure, le lieu ou les aspects sociaux. Cependant,
aucune de ces approches n’a pris en compte le probl`eme
de l’´evolution du contenu des utilisateurs. Dans [87],
les auteurs introduisent un algorithme qui prend en
compte cette dynamique. Il est bas´e sur une exploration
/ exploitation dynamique et peut ´equilibrer de mani`ere
adaptative les deux aspects, en d´ecidant quelle situation
est la plus pertinente pour l’exploration ou l’exploitation.
En ce sens, [27] propose d’´etudier la ”fraˆıcheur” du
contenu de l’utilisateur
`a travers le probl`eme du
bandit. Ils introduisent l’algorithme Freshness-Aware
Thompson
Sampling
pour
la
recommandation
de
nouveaux documents.
E. Maximisation de l’inﬂuence
Les auteurs de [108] consid`erent la maximisation
de l’inﬂuence (IM) dans les r´eseaux sociaux, qui est
le probl`eme de maximiser le nombre d’utilisateurs qui
prennent conscience d’un produit en s´electionnant un
ensemble d’utilisateurs auxquels exposer le produit. Ils
proposent une nouvelle param´etrisation qui rend non
seulement le cadre ind´ependant du mod`ele de diffusion
sous-jacent, mais aussi statistiquement efﬁcace pour ap-
prendre des donn´ees.
Ils donnent une fonction de substitution monotone et
submodulaire correspondante, et montrent qu’il s’agit
3


===== PAGE 4 =====
d’une bonne approximation de l’objectif original de
la MI. Ils consid`erent ´egalement le cas d’un nouveau
marketeur cherchant `a exploiter un r´eseau social ex-
istant, tout en apprenant simultan´ement les facteurs
r´egissant la propagation de l’information. Pour cela, ils
d´eveloppent un algorithme de bandit bas´e sur LinUCB.
Les auteurs de [109] ´etudient ´egalement le probl`eme de
maximisation de l’inﬂuence en ligne dans les r´eseaux
sociaux mais sous le mod`ele de cascade ind´ependant.
Plus pr´ecis´ement, ils essaient d’apprendre l’ensemble
des ”meilleures graines ou inﬂuenceurs” dans un r´eseau
social en ligne tout en interagissant `a plusieurs reprises
avec lui. Ils abordent les d´eﬁs de l’espace d’action
combinatoire, car le nombre d’ensembles d’inﬂuenceurs
r´ealisables augmente de mani`ere exponentielle avec le
nombre maximum d’inﬂuenceurs et un retour limit´e, car
seule la partie inﬂuenc´ee du r´eseau est observ´ee.
F. R´ecup´eration de l’information
Les auteurs de [110] soutiennent que le proces-
sus de s´election it´erative de recherche d’informations
peut ˆetre naturellement mod´elis´e comme un probl`eme
de bandit contextuel. Le mod`ele de bandit conduit `a
des m´ethodes tr`es efﬁcaces pour l’arbitrage des doc-
uments. Dans ce cadre d’attribution des bandits, ils
proposent sept nouvelles m´ethodes de jugement de doc-
uments, dont cinq sont des m´ethodes stationnaires et
deux sont des m´ethodes non stationnaires. Cette ´etude
comparative comprend les m´ethodes existantes conc¸ues
pour l’´evaluation bas´ee sur la mise en commun et les
m´ethodes existantes conc¸ues pour la m´eta-recherche.
Dans la recherche d’informations mobiles, les auteurs de
[16] introduisent un algorithme qui aborde ce dilemme
dans le domaine de la recherche d’informations bas´ees
sur le contexte (CBIR). Il est bas´e sur une exploration /
exploitation dynamique et il peut ´equilibrer de mani`ere
adaptative les deux aspects en d´ecidant quelle situation
d’utilisateur est la plus pertinente pour l’exploration
ou l’exploitation. Dans un cadre en ligne d´elib´er´ement
conc¸u, ils effectuent des ´evaluations aupr`es des utilisa-
teurs mobiles.
G. Syst`emes de Dialogue
S´election de r´eponse de dialogue. La s´election
de r´eponse de dialogue est une
´etape importante
vers
la
g´en´eration
de
r´eponse
naturelle
dans
les
agents conversationnels. Les travaux existants sur les
mod`eles conversationnels se concentrent principalement
sur l’apprentissage supervis´e hors ligne `a l’aide d’un
large ensemble de paires contexte-r´eponse. Dans [111],
les auteurs se concentrent sur l’apprentissage en ligne de
la s´election des r´eponses dans les syst`emes de dialogue.
Ils proposent un mod`ele de bandit contextuel avec une
fonction de r´ecompense non lin´eaire qui utilise une
repr´esentation distribu´ee du texte pour la s´election de
r´eponse en ligne. Un LSTM bidirectionnel est utilis´e
pour produire les repr´esentations distribu´ees du con-
texte de dialogue et des r´eponses, qui servent d’entr´ee
`a un bandit contextuel. Ils proposent une m´ethode
d’´echantillonnage personnalis´ee de Thompson qui est
appliqu´ee `a un espace de caract´eristiques polynomiales
pour approximer la r´ecompense.
Syst`emes de dialogue. L’objectif de la pro-activit´e
dans les syst`emes de dialogue est d’am´eliorer la con-
vivialit´e des agents conversationnels en leur permet-
tant d’initier des conversations. Alors que les syst`emes
de dialogue sont devenus de plus en plus populaires,
les syst`emes de dialogue actuels ax´es sur les tˆaches
sont principalement r´eactifs, car les utilisateurs humains
ont tendance `a lancer des conversations. Les auteurs
de [112] proposent d’introduire le paradigme des ban-
dits contextuels comme cadre pour des syst`emes de
dialogue proactifs. Les bandits contextuels ont ´et´e le
mod`ele de choix pour le probl`eme de la maximisa-
tion des r´ecompenses avec r´etroaction partielle car ils
correspondent bien `a la description de la tˆache, ils
explorent ´egalement la notion de m´emoire dans ce
paradigme, o`u ils proposent deux mod`eles de m´emoire
diff´erentiables qui agissent comme des parties du fonc-
tion d’estimation de r´ecompense param´etrique. Le pre-
mier, les r´eseaux de m´emoire s´elective par convolution,
utilise une s´election d’interactions pass´ees dans le cadre
de l’aide `a la d´ecision. Le deuxi`eme mod`ele, appel´e
r´eseau de m´emoire attentive contextuelle, met en oeuvre
un m´ecanisme d’attention diff´erentiable sur les interac-
tions pass´ees de l’agent. Le but est de g´en´eraliser le
mod`ele classique des bandits contextuels aux contextes
o`u les informations temporelles doivent ˆetre incorpor´ees
et exploit´ees de mani`ere apprenable.
Syst`emes de dialogue multi-domaines. Construire
des agents de dialogue multi-domaines est une tˆache
difﬁcile et un probl`eme ouvert dans l’IA moderne.
Dans le domaine du dialogue, la capacit´e d’orchestrer
plusieurs agents de dialogue form´es ind´ependamment,
ou comp´etences, pour cr´eer un syst`eme uniﬁ´e est d’une
importance particuli`ere. Dans [113], les auteurs ´etudient
la tˆache d’orchestration du dialogue en ligne, o`u ils
d´eﬁnissent l’orchestration post´erieure comme la tˆache
de s´electionner un sous-ensemble de comp´etences qui
r´epond le mieux `a une entr´ee utilisateur en utilisant des
fonctionnalit´es extraites `a la fois de l’entr´ee utilisateur et
4


===== PAGE 5 =====
de l’individu comp´etences. Pour tenir compte des coˆuts
vari´es associ´es `a l’extraction des caract´eristiques des
comp´etences, ils consid`erent l’orchestration post´erieure
en ligne avec un budget d’ex´ecution des comp´etences.
Ce param`etre est formalis´e en tant que bandit attentif
au contexte avec observations, une variante des bandits
attentifs au contexte, puis l’´evalue sur des ensembles de
donn´ees conversationnelles simul´ees.
H. D´etection d’une anomalie
Les auteurs de [93] ´etudient le probl`eme de la
d´etection d’anomalies dans un cadre interactif. Leur
objectif est de maximiser les v´eritables anomalies
pr´esent´ees `a l’expert humain apr`es ´epuisement d’un
budget donn´e. Parall`element `a cette ligne, ils formulent
le probl`eme `a travers le cadre de bandit et d´eveloppent un
nouvel algorithme de bandit contextuel collaboratif, qui
mod´elise explicitement les attributs et les d´ependances
de noeuds de mani`ere transparente dans un cadre com-
mun, et g`ere le dilemme exploration-exploitation lors de
l’interrogation.
Les transactions par carte de cr´edit susceptibles d’ˆetre
frauduleuses par les syst`emes de d´etection automatis´es
sont g´en´eralement transmises `a des experts humains
pour v´eriﬁcation. Pour limiter les coˆuts, il est courant
de ne s´electionner que les transactions les plus sus-
pectes pour enquˆete. Les auteurs de [114] afﬁrment
qu’un compromis entre l’exploration et l’exploitation
est imp´eratif pour permettre l’adaptation aux change-
ments de comportement. L’exploration consiste en la
s´election et l’investigation des transactions dans le but
d’am´eliorer les mod`eles pr´edictifs, et l’exploitation con-
siste `a enquˆeter sur les transactions d´etect´ees comme
suspectes. Mod´elisant la d´etection des transactions fraud-
uleuses comme une r´ecompense, ils utilisent un ap-
prenant d’arbre de r´egression incr´ementiel pour cr´eer des
grappes de transactions avec des r´ecompenses attendues
similaires. Cela permet l’utilisation d’un algorithme de
bandit contextual(CMAB) pour fournir le compromis
exploration / exploitation.
I.
T´el´ecommunication
Dans [115], un mod`ele de bandit a ´et´e utilis´e pour
d´ecrire le probl`eme de la meilleure s´election de r´eseau
sans ﬁl par un dispositif multi-Radio Access Technology
(multi-RAT), dans le but de maximiser la qualit´e perc¸ue
par l’utilisateur ﬁnal. Le mod`ele propos´e ´etendre le
mod`ele MAB classique de deux mani`eres. Premi`erement,
il pr´evoit deux actions diff´erentes: mesurer et utiliser;
deuxi`emement, il permet aux actions de s’´etaler sur
plusieurs ´etapes de temps. Deux nouveaux algorithmes
conc¸us pour tirer parti de la plus grande ﬂexibilit´e offerte
par le mod`ele muMAB ont ´egalement ´et´e introduits. Le
premier, appel´e mesure-utilisation-UCB1 est d´eriv´e de
l’algorithme UCB1, tandis que le second, appel´e Mesure
avec intervalle logarithmique, est conc¸u de mani`ere
appropri´ee pour le nouveau mod`ele aﬁn de tirer parti
de la nouvelle action de mesure, tout en en utilisant
agressivement le meilleur bras. Les auteurs de [116]
d´emontrent la possibilit´e d’optimiser les performances
de la technologie Long Range Wide Area Network.
Les auteurs sugg`erent que les nœuds utilisent des al-
gorithmes de bandit multi-bras, pour s´electionner les
param`etres de communication (facteur d’´etalement et
puissance d’´emission). Les ´evaluations montrent que de
telles m´ethodes d’apprentissage permettent de g´erer bien
mieux le compromis entre la consommation d’´energie
et la perte de paquets qu’un algorithme Adaptive Data
Rate adaptant les facteurs d’´etalement et les puissances
de transmission sur la base des valeurs du rapport signal
sur interf´erence et du rapport de bruit.
J. Bandit dans les applications r´eelles: r´esum´e et orien-
tations futures
TABLE I
APPLICATION DES BANDITS DANS LA VIE R´EELLE
Non-
Non-
MAB
stat
CMAB
stat
MAB
CMAB
Sant´e
√
√
La ﬁnance
√
Tariﬁcation dynamique
√
Syst`eme de recommandation
√
√
√
√
Maximisation
√
Syst`eme de dialogue
√
T´el´ecomunication
√
D´etection d’anomalie
√
Le tableau I fournit un r´esum´e des formulations de
probl`emes de bandit utilis´ees dans diverses applications
sp´eciﬁques `a un domaine. Le choix du mod`ele de bandit
est souvent sp´eciﬁque au domaine. Par exemple, il est
´evident que le bandit non stationnaire n’a pas ´et´e utilis´e
dans les applications de soins de sant´e, car des change-
ments signiﬁcatifs ne sont pas attendus dans le processus
de prise des d´ecisions de traitement, c’est-`a-dire pas de
transition dans l’´etat du patient; de telles transitions,
si elles se produisaient, seraient mieux mod´elis´ees en
utilisant l’apprentissage par renforcement plutˆot que le
bandit non stationnaire. Il existe clairement d’autres
domaines o`u le bandit non stationnaire est un cadre plus
appropri´e, mais il semble que ce param`etre n’ait pas
encore ´et´e ´etudi´e de mani`ere signiﬁcative dans les do-
maines de la sant´e. Par exemple, la d´etection d’anomalie
5


===== PAGE 6 =====
est un domaine dans lequel un bandit contextuel non
stationnaire pourrait ˆetre utilis´e, car dans ce contexte,
l’anomalie pourrait ˆetre contradictoire, ce qui signiﬁe
que tout bandit appliqu´e `a ce param`etre devrait avoir
une sorte de condition de d´erive, aﬁn de s’adapter `a de
nouveaux types d’attaques. Un autre constat est qu’aucun
des travaux existants n’a tent´e de d´evelopper un algo-
rithme capable de r´esoudre ces diff´erentes tˆaches en
mˆeme temps, ou d’appliquer les connaissances obtenues
dans un domaine `a un autre domaine, ouvrant ainsi une
direction de recherche sur le multitask et le transfer
learning dans le cadre de bandit. De plus, ´etant donn´e
la nature en ligne du probl`eme de bandit, le lifelong
learning serait une prochaine ´etape naturelle.
III. BANDIT POUR UN MEILLEUR APPRENTISSAGE
AUTOMATIQUE
Dans cette section, nous d´ecrivons comment les
algorithmes de bandit pourraient ˆetre utilis´es pour
am´eliorer d’autres algorithmes, par ex. diverses tech-
niques d’apprentissage automatique.
A. Selection d’algorithm
La s´election de l’algorithme est g´en´eralement bas´ee
sur des mod`eles de performances d’algorithme, appris
au cours d’une s´equence d’apprentissage hors ligne
distincte, qui peut ˆetre d’un coˆut prohibitif. Dans des
travaux r´ecents, ils ont adopt´e une approche en ligne,
dans laquelle un mod`ele de performance est mis `a jour
de mani`ere it´erative et utilis´e pour guider la s´election.
Le compromis exploration-exploitation qui en r´esultait a
´et´e repr´esent´e comme un probl`eme de bandit avec des
conseils d’experts, en utilisant un solveur existant pour
ce jeu, cela n´ecessitait l’utilisation d’une limite arbitraire
sur les temps d’ex´ecution de l’algorithme, annulant ainsi
le regret optimal du solveur. Dans [117], un cadre
plus simple a ´et´e propos´e pour repr´esenter la s´election
d’algorithmes comme un probl`eme de bandit, en utilisant
des informations partielles et une limite inconnue sur les
pertes.
B. Optimisation des hyperparam`etres
[118] a formul´e l’optimisation des hyperparam`etres
comme
un
probl`eme
de
bandit
non
stochastique
d’exploration pure o`u des ressources pr´ed´eﬁnies, telles
que des it´erations, des ´echantillons de donn´ees ou
des fonctionnalit´es sont allou´ees `a des conﬁgurations
´echantillonn´ees al´eatoirement. Ces travaux ont intro-
duit un nouvel algorithme, Hyperband, pour ce cadre
et analyse ses propri´et´es th´eoriques, offrant plusieurs
garanties. En outre, Hyperband ´etait compar´e aux
m´ethodes d’optimisation bay´esiennes populaires; il a ´et´e
observ´e qu’Hyperband peut fournir une acc´el´eration plus
grande par rapport `a ses concurrents sur une vari´et´e de
probl`emes d’apprentissage.
C. S´election des Features
Dans un apprentissage supervis´e en ligne classique, la
v´eritable ´etiquette d’un ´echantillon est toujours r´ev´el´ee
au classiﬁcateur, contrairement `a un bandit o`u une mau-
vaise classiﬁcation se traduit par une r´ecompense nulle,
et seule la classiﬁcation correcte donne la r´ecompense
1. Les auteurs de
[119] ´etudie le probl`eme de la
s´election des fonctionnalit´es en ligne, o`u le but est
de faire des pr´edictions pr´ecises en utilisant seulement
un petit nombre de fonctionnalit´es actives `a l’aide de
l’algorithme epsilon-greedy. Les auteurs de [120] abor-
dent le probl`eme de la s´election de fonctionnalit´es en
ligne en abordant le probl`eme d’optimisation combina-
toire dans le cadre de bandit stochastique avec retour
de bandit, en utilisant l’algorithme d’´echantillonnage de
Thompson.
D. Bandit pour l’apprentissage actif
L’´etiquetage de tous les exemples dans un cadre
de classiﬁcation supervis´ee peut ˆetre coˆuteux. Les
strat´egies d’apprentissage actif r´esolvent ce probl`eme
en s´electionnant les exemples non ´etiquet´es les plus
utiles pour obtenir l’´etiquette et pour former un mod`ele
pr´edictif. Le choix des exemples `a ´etiqueter peut ˆetre vu
comme un dilemme entre l’exploration et l’exploitation
sur l’espace d’entr´ee. Dans [13], une nouvelle strat´egie
d’apprentissage actif g`ere ce compromis en mod´elisant
le probl`eme d’apprentissage actif comme un probl`eme de
bandit contextuel. ils proposent un algorithme s´equentiel
appel´e Active Thompson Sampling (ATS), qui, `a chaque
tour, attribue une distribution d’´echantillonnage sur
le clusteur, ´echantillonne un point de cette distri-
bution et interroge l’oracle pour cette ´etiquette de
point d’´echantillonnage. Les auteurs de [121] proposent
´egalement un algorithme d’apprentissage actif bas´e
sur des groupes de bandits `a plusieurs bras pour le
probl`eme de la classiﬁcation binaire. Ils utilisent des
id´ees telles que des limites de conﬁance inf´erieures et
une r´egularisation auto-concordante tir´ee de la litt´erature
sur les bandits `a plusieurs bras pour concevoir leur
algorithme.
E. Clustering
[122] consid`ere le clustering collaboratif, qui est
un paradigme d’apprentissage automatique concern´e par
l’analyse non supervis´ee de donn´ees complexes `a vues
6


===== PAGE 7 =====
multiples `a l’aide de plusieurs algorithmes fonctionnant.
Les applications bien connues du clustering collaboratif
incluent le clustering `a vues multiples et le clustering de
donn´ees distribu´e, o`u plusieurs algorithmes ´echangent
des informations aﬁn de s’am´eliorer mutuellement. L’un
des principaux probl`emes du clustering collaboratif et `a
vues multiples est d’´evaluer quelles collaborations seront
b´en´eﬁques ou pr´ejudiciables. De nombreuses solutions
ont ´et´e propos´ees `a ce probl`eme, et toutes concluent
que, `a moins que deux mod`eles ne soient tr`es proches,
il est difﬁcile de pr´edire `a l’avance le r´esultat d’une
collaboration. Pour r´esoudre ce probl`eme, les auteurs de
[122] proposent un algorithme collaboratif de clustering
peer to peer bas´e sur le principe des bandits multi-
bras non stochastiques pour ´evaluer en temps r´eel quels
algorithmes ou vues peuvent apporter des informations
utiles.
F. Apprentissage par renforcement
Les syst`emes cyber-physiques autonomes jouent un
rˆole important dans nos vies. Pour s’assurer que les
agents se comportent de mani`ere align´ee sur les valeurs
des soci´et´es dans lesquelles ils op`erent, nous devons
d´evelopper des techniques qui permettent `a ces agents
non seulement de maximiser leur r´ecompense dans un
environnement, mais aussi d’apprendre et de suivre les
contraintes implicites assum´ees par la soci´et´e. Dans [60],
les auteurs ´etudient un cadre o`u un agent peut observer
des traces de comportement de membres de la soci´et´e
mais n’a pas acc`es `a l’ensemble explicite de contraintes
qui donnent lieu au comportement observ´e. Au lieu de
cela, l’apprentissage par renforcement inverse est utilis´e
pour apprendre de telles contraintes. C’est contraintes
sont ensuite combin´ees avec une fonction de valeur
orthogonale grˆace `a l’utilisation d’un orchestrateur con-
textuel qui choisit entre deux politiques. L’orchestrateur
de bandit contextuel permet `a l’agent de m´elanger les
politiques de mani`ere novatrice, en prenant les meilleures
actions `a partir d’une politique de maximisation de la
r´ecompense.
G. Bandit pour l’apprentissage automatique:
R´esum´e et orientations futures
Le tableau II r´esume les types de probl`emes de ban-
dit utilis´es pour r´esoudre les probl`emes d’apprentissage
automatique mentionn´es ci-dessus. Nous voyons, par
exemple, que le bandit contextuel n’a pas ´et´e utilis´e
dans la s´election des hyperparam`etres. Cette observation
pourrait indiquer une direction pour les travaux futurs,
o`u des informations secondaires pourraient ˆetre utilis´ees
dans la s´election des caract´eristiques. En outre, le bandit
TABLE II
BANDIT POUR L’APPRENTISSAGE AUTOMATIQUE
MAB
Non
CMAB
Non
Station-
Station-
MAB
CMAB
S´election d’algorithme
√
Optimisation des param`etres
√
S´election des fonctionnalit´es
√
√
Apprentissage actif
√
√
Clustering
√
RL
√
√
√
non stationnaire a rarement ´et´e pris en compte dans
ces situations probl´ematiques, ce qui sugg`ere ´egalement
des extensions possibles des travaux actuels. Par ex-
emple, le bandit contextuel non stationnaire pourrait
ˆetre utile dans le cadre de s´election de caract´eristiques
non stationnaires, o`u trouver les bonnes caract´eristiques
d´epend du temps et du contexte lorsque l’environnement
ne cesse de changer. Notre principale observation est
´egalement que chaque technique ne r´esout qu’un seul
probl`eme d’apprentissage automatique `a la fois; Ainsi,
la question est de savoir si un param`etre de bandit et
des algorithmes peuvent ˆetre d´evelopp´es pour r´esoudre
simultan´ement plusieurs probl`emes d’apprentissage au-
tomatique, et si le transfert et l’apprentissage continu
peuvent ˆetre r´ealis´es dans ce contexte. Une solution
pourrait ˆetre de mod´eliser tous ces probl`emes dans un
cadre de bandit combinatoire, o`u l’algorithme de bandit
trouverait la solution optimale pour chaque probl`eme `a
chaque it´eration; ainsi, le bandit combinatoire pourrait
en outre ˆetre utilis´e comme outil pour faire progresser
l’apprentissage automatique.
IV. CONCLUSIONS
Dans cet article, nous avons pass´e en revue certains
des travaux r´ecents les plus notables sur les applications
du bandit et du bandit contextuel, `a la fois dans des do-
maines r´eels et dans l’apprentissage automatique. Nous
avons r´esum´e, de mani`ere organis´ee (tableaux 1 et 2),
diverses applications existantes, par types de param`etres
de bandit utilis´es, et discut´e des avantages de l’utilisation
des techniques de bandit dans chaque domaine. Nous
d´ecrivons bri`evement plusieurs probl`emes importants et
des extensions futures prometteuses. En r´esum´e, le cadre
du bandit, comprenant `a la fois le bandit multi-bras
et le bandit contextuel, est actuellement des domaines
de recherche tr`es actifs et prometteurs, et de multiples
nouvelles techniques et applications ´emergent chaque
ann´ee. Nous esp´erons que notre enquˆete pourra aider le
lecteur `a mieux comprendre certains aspects cl´es de ce
7


===== PAGE 8 =====
domaine passionnant et `a avoir une meilleure perspective
sur ses avanc´ees notables et ses promesses futures.
REFERENCES
[1] A. Durand, C. Achilleos, D. Iacovides, K. Strati, G. D. Mitsis,
and J. Pineau, “Contextual bandits for adapting treatment in a
mouse model of de novo carcinogenesis,” in Machine Learning
for Healthcare Conference, pp. 67–82, 2018.
[2] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Hybrid-
ε-greedy for mobile context-aware recommender system,” in
Paciﬁc-Asia Conference on Knowledge Discovery and Data
Mining, pp. 468–479, Springer, Berlin, Heidelberg, 2012.
[3] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Follow-
ing the user’s interests in mobile context-aware recommender
systems: The hybrid-e-greedy algorithm,” in 2012 26th Inter-
national Conference on Advanced Information Networking and
Applications Workshops, pp. 657–662, IEEE, 2012.
[4] R. Allesiardo, R. F´eraud, and D. Bouneffouf, “A neural net-
works committee for the contextual bandit problem,” in Interna-
tional Conference on Neural Information Processing, pp. 374–
381, Springer, Cham, 2014.
[5] D. Bouneffouf, “Situation-aware approach to improve context-
based recommender system,” arXiv preprint arXiv:1303.0481,
2013.
[6] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Consid-
ering the high level critical situations in con-text-aware recom-
mender systems,” in 2nd International Workshop on Information
Management for Mobile Applications, p. 26, 2012.
[7] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Ex-
ploration/exploitation trade-off in mobile context-aware recom-
mender systems,” in Australasian Joint Conference on Artiﬁcial
Intelligence, pp. 591–601, Springer, Berlin, Heidelberg, 2012.
[8] D. Bouneffouf, “Applying machine learning techniques to
improve user acceptance on ubiquitous environement,” arXiv
preprint arXiv:1301.4351, 2013.
[9] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganarski, “Risk-
aware recommender systems,” in International Conference on
Neural Information Processing, pp. 57–65, Springer, Berlin,
Heidelberg, 2013.
[10] D. Bouneffouf, “Role of temporal inference in the recognition
of textual inference,” arXiv preprint arXiv:1302.5645, 2013.
[11] D. Bouneffouf, DRARS, a dynamic risk-aware recommender
system. PhD thesis, 2013.
[12] D. Bouneffouf, “Improving adaptation of ubiquitous recomman-
der systems by using reinforcement learning and collaborative
ﬁltering,” arXiv preprint arXiv:1303.2308, 2013.
[13] D. Bouneffouf, R. Laroche, T. Urvoy, R. F´eraud, and R. Alle-
siardo, “Contextual bandit for active learning: Active thompson
sampling,” in International Conference on Neural Information
Processing, pp. 405–412, Springer, Cham, 2014.
[14] D. Bouneffouf, “Towards user proﬁle modelling in recom-
mender system,” arXiv preprint arXiv:1305.1114, 2013.
[15] D. Bouneffouf, Role de l’inference temporelle dans la recon-
naissance de l’inference textuelle. PhD thesis, Universit´e des
Sciences et de la Technologie, 2008.
[16] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Contex-
tual bandits for context-based information retrieval,” in Interna-
tional Conference on Neural Information Processing, pp. 35–42,
Springer, Berlin, Heidelberg, 2013.
[17] D.
Bouneffouf,
“The
impact
of
situation
clustering
in
contextual-bandit algorithm for context-aware recommender
systems,” arXiv preprint arXiv:1304.3845, 2013.
[18] D. Bouneffouf, “Recommandation mobile, sensible au contexte
de contenus\’evolutifs: Contextuel-e-greedy,” arXiv preprint
arXiv:1402.1986, 2014.
[19] D. Bouneffouf and I. Birol, “Sampling with minimum sum
of squared similarities for nystrom-based large scale spectral
clustering,” in Twenty-Fourth International Joint Conference on
Artiﬁcial Intelligence, 2015.
[20] D. Bouneffouf, “Hybrid q-learning applied to ubiquitous rec-
ommender system,” arXiv preprint arXiv:1303.2651, 2013.
[21] D. Bouneffouf, “Mobile recommender systems methods: An
overview,” arXiv preprint arXiv:1305.1745, 2013.
[22] D. Bouneffouf, “Exponentiated gradient exploration for active
learning,” Computers, vol. 5, no. 1, p. 1, 2016.
[23] D. Bouneffouf, “Evolution of the user’s content: An overview
of the state of the art,” arXiv preprint arXiv:1305.1787, 2013.
[24] D. Bouneffouf, “” l’apprentissage automatique”, une ´etape
importante dans l’adaptation des syst`emes d’information `a
l’utilisateur,” 2013.
[25] D. Bouneffouf, “Context-based information retrieval in risky
environment,” arXiv preprint arXiv:1409.7729, 2014.
[26] D. Bouneffouf, “\’etude des dimensions sp\’eciﬁques du con-
texte dans un syst\eme de ﬁltrage d’informations,” arXiv
preprint arXiv:1405.6287, 2014.
[27] D. Bouneffouf, “Freshness-aware thompson sampling,” in In-
ternational Conference on Neural Information Processing,
pp. 373–380, Springer, Cham, 2014.
[28] R. Allesiardo, R. F´eraud, and D. Bouneffouf, “Prise de d´ecision
contextuelle en bande organis´ee: Quand les bandits font un
brainstorming,” 2014.
[29] D. Bouneffouf, “Temporal logic and its applications in natural
language processing,” 2011.
[30] D. Bouneffouf, “R-ucb: a contextual bandit algorithm for risk-
aware recommender systems,” arXiv preprint arXiv:1408.2195,
2014.
[31] D. Bouneffouf, “Proposition d’une technique de gestion de
projet dans les startups,” arXiv preprint arXiv:1303.2317, 2013.
[32] D. Bouneffouf, “Learning and inference engine applied to
ubiquitous recommender system,”
[33] D. Bouneffouf, “La logique temporelle et ses applications dans
le traitement du langage naturel,” 2013.
[34] D. Bouneffouf, “Ant clustering to improve situation-aware rec-
ommender systems,” 2014.
[35] D. Bouneffouf, “Contextual bandit algorithm for risk-aware
recommender systems,” in 2016 IEEE Congress on Evolutionary
Computation (CEC), pp. 4667–4674, IEEE, 2016.
[36] D. Bouneffouf and R. F´eraud, “Multi-armed bandit problem
with known trend,” Neurocomputing, vol. 205, pp. 16–21, 2016.
[37] D. Bouneffouf, “Exponentiated gradient linucb for contextual
multi-armed bandits,” arXiv preprint arXiv:1305.2415, 2013.
[38] D. Bouneffouf and I. Birol, “Theoretical analysis of the min-
imum sum of squared similarities sampling for nystr¨om-based
spectral clustering,” in 2016 International Joint Conference on
Neural Networks (IJCNN), pp. 3856–3862, IEEE, 2016.
[39] D. Bouneffouf, “Temporal logic in natural language processing,”
2013.
[40] D. Bouneffouf, “Optimizing an utility function for explo-
ration/exploitation trade-off in context-aware recommender sys-
tem,” arXiv preprint arXiv:1303.0485, 2013.
[41] D. Bouneffouf and I. Birol, “Ensemble minimum sum of
squared similarities sampling for nystr¨om-based spectral clus-
tering,” in 2016 International Joint Conference on Neural Net-
works (IJCNN), pp. 3851–3855, IEEE, 2016.
[42] D. Bouneffouf, I. Rish, G. A. Cecchi, and R. F´eraud, “Context
attentive bandits: contextual bandit with restricted context,” in
IJCAI 2017, 2017.
[43] D. Bouneffouf, I. Rish, and G. A. Cecchi, “Bandit models
of human behavior: Reward processing in mental disorders,”
in International Conference on Artiﬁcial General Intelligence,
pp. 237–248, Springer, Cham, 2017.
8


===== PAGE 9 =====
[44] D. Bouneffouf, “Drars: un syst`eme de recommandation dy-
namique sensible au risque,”
[45] B. Lin, G. Cecchi, D. Bouneffouf, and I. Rish, “Adaptive
representation selection in contextual bandit with unlabeled
history,” 2018.
[46] D. Bouneffouf, “Nystrom sampling depends on the eigenspec-
trum shape of the data,” 2018.
[47] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi, “Using
contextual bandits with behavioral constraints for constrained
online movie recommendation.,” in IJCAI, pp. 5802–5804,
2018.
[48] M. Riemer, T. Klinger, D. Bouneffouf, and M. Franceschini,
“Scalable recollections for continual lifelong learning,” in Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence,
vol. 33, pp. 1352–1359, 2019.
[49] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi,
“Incorporating behavioral constraints in online ai systems,” in
AAAI, 2019.
[50] D. Bouneffouf, “Eigenspectrum shape based nystr¨om sampling,”
in 2018 International Joint Conference on Neural Networks
(IJCNN), pp. 1–6, IEEE, 2018.
[51] D. Bouneffouf, A. Bouzeghoub, and A. L. Ganc¸arski, “Follow-
ing the user’s interests in mobile context-aware,”
[52] M. Riemer, M. Franceschini, D. Bouneffouf, and T. Klinger,
“Generative knowledge distillation for general purpose function
compression,” in NIPS 2017 Workshop on Teaching Machines,
Robots, and Humans, vol. 5, p. 30, 2017.
[53] B. Lin, D. Bouneffouf, G. A. Cecchi, and I. Rish, “Contex-
tual bandit with adaptive feature extraction,” in 2018 IEEE
International Conference on Data Mining Workshops (ICDMW),
pp. 937–944, IEEE, 2018.
[54] A. Choromanska, B. Cowen, S. Kumaravel, R. Luss, M. Rigotti,
I. Rish, P. Diachille, V. Gurev, B. Kingsbury, R. Tejwani,
et al., “Beyond backprop: Online alternating minimization with
auxiliary variables,” in International Conference on Machine
Learning, pp. 1193–1202, 2019.
[55] D. Bouneffouf and I. Rish, “A survey on practical applications
of multi-armed and contextual bandits,” in The IEEE World
Congress on Computational Intelligence (IEEE WCCI), 2020.
[56] B. Djallel, A. Bouzeghoub, and A. L. Ganarski, “Risk-aware
recommender systems,”
[57] S. Upadhyay, M. Agarwal, D. Bounneffouf, and Y. Khazaeni,
“A bandit approach to posterior dialog orchestration under a
budget,” NIPS 2018.
[58] S. Liu, P. Ram, D. Bouneffouf, G. Bramble, A. R. Conn,
H. Samulowitz, and A. G. Gray, “Automated machine learning
via admm,” CoRR, abs/1905.00424, 2019.
[59] D. Bouneffouf, S. Parthasarathy, H. Samulowitz, and M. Wistub,
“Optimal exploitation of clustering and history information in
multi-armed bandit,” arXiv preprint arXiv:1906.03979, 2019.
[60] R.
Noothigattu,
D.
Bouneffouf,
N.
Mattei,
R.
Chandra,
P. Madan, K. Varshney, M. Campbell, M. Singh, and F. Rossi,
“Interpretable multi-objective reinforcement learning through
policy orchestration,” arXiv preprint arXiv:1809.08343, 2018.
[61] M. Yurochkin, S. Upadhyay, D. Bouneffouf, M. Agarwal, and
Y. Khazaeni, “Online semi-supervised learning with bandit
feedback,” 2019.
[62] B. Lin, G. Cecchi, D. Bouneffouf, J. Reinen, and I. Rish, “Re-
inforcement learning models of human behavior: Reward pro-
cessing in mental disorders,” arXiv preprint arXiv:1906.11286,
2019.
[63] R.
Noothigattu,
D.
Bouneffouf,
N.
Mattei,
R.
Chandra,
P. Madan, K. R. Varshney, M. Campbell, M. Singh, and F. Rossi,
“Teaching ai agents ethical values using reinforcement learning
and policy orchestration,” IBM Journal of Research and Devel-
opment, vol. 63, no. 4/5, pp. 2–1, 2019.
[64] C. Aggarwal, D. Bouneffouf, H. Samulowitz, B. Buesser,
T. Hoang, U. Khurana, S. Liu, T. Pedapati, P. Ram, A. Rawat,
et al., “How can ai automate end-to-end data science?,” arXiv
preprint arXiv:1910.14436, 2019.
[65] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi, “Using
multi-armed bandits to learn ethical priorities for online ai
systems,” IBM Journal of Research and Development, vol. 63,
no. 4/5, pp. 1–1, 2019.
[66] S. Mehta, F. Rossi, K. Varshney, A. Balakrishnan, D. Boun-
effouf, N. Mattei, R. Noothigattu, R. Chandra, P. Madan,
M. Campbell, et al., “Ai ethics,” 2019.
[67] S. Liu, P. Ram, D. Vijaykeerthy, D. Bouneffouf, G. Bramble,
H. Samulowitz, D. Wang, A. Conn, and A. G. Gray, “An admm
based framework for automl pipeline conﬁguration.,” in AAAI,
pp. 4892–4899, 2020.
[68] S. Sharma, Y. Zhang, J. M. R´ıos Aliaga, D. Bouneffouf,
V. Muthusamy, and K. R. Varshney, “Data augmentation for
discrimination prevention and bias disambiguation,” in Proceed-
ings of the AAAI/ACM Conference on AI, Ethics, and Society,
pp. 358–364, 2020.
[69] A. Balakrishnan, D. Bouneffouf, N. Mattei, and F. Rossi,
“Constrained decision-making and explanation of a recommen-
dation,” Jan. 16 2020. US Patent App. 16/050,176.
[70] B. Lin, G. A. Cecchi, D. Bouneffouf, J. Reinen, and I. Rish,
“A story of two streams: Reinforcement learning models from
human behavior and neuropsychiatry.,” in AAMAS, pp. 744–752,
2020.
[71] K. Varshney, M. Campbell, M. Singh, and F. Rossi, “Teaching
ai agents ethical values using reinforcement learning and policy
orchestration,”
[72] D. Bouneffouf and E. Claeys, “Hyper-parameter tuning for the
contextual bandit,” arXiv preprint arXiv:2005.02209, 2020.
[73] B. Lin, G. Cecchi, D. Bouneffouf, J. Reinen, and I. Rish, “Uni-
ﬁed models of human behavioral agents in bandits, contextual
bandits and rl,” arXiv preprint arXiv:2005.04544, 2020.
[74] D. Bouneffouf and E. Claeys, “Learning exploration for con-
textual bandit,” in AutoML@ ICML 2019: 6th ICML Workshop
on Automated Machine Learning, 2016.
[75] B. Lin, D. Bouneffouf, and G. Cecchi, “Online learning in
iterated prisoner’s dilemma to mimic human behavior,” arXiv
preprint arXiv:2006.06580, 2020.
[76] P. Ram, S. Liu, D. Vijaykeerthi,
D. Wang, D. Bounef-
fouf, G. Bramble, H. Samulowitz, and A. G. Gray, “Solv-
ing constrained cash problems with admm,” arXiv preprint
arXiv:2006.09635, 2020.
[77] D. Bouneffouf, “Online learning with corrupted context: Cor-
rupted contextual bandits,” arXiv preprint arXiv:2006.15194,
2020.
[78] D. Bouneffouf, S. Upadhyay, and Y. Khazaeni, “Contextual
bandit with missing rewards,” arXiv preprint arXiv:2007.06368,
2020.
[79] D. Bouneffouf, “Location-aware approach to improve context-
based recommender system,” arXiv preprint arXiv:1303.0481,
2013.
[80] K. Toutanova and H. Wu, “Proceedings of the 52nd annual
meeting of the association for computational linguistics (volume
1: Long papers),” in Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long
Papers), 2014.
[81] C. S. Leung, Neural Information Processing: 19th International
Conference, ICONIP 2012, Doha, Qatar, November 12-15,
2012, Proceedings, Part III. Springer, 2012.
[82] A. T. B. Jin, Neural Information Processing: 21st International
Conference, ICONIP 2014, Kuching, Malaysia, November 3-6,
2014: Proceedings. Springer, 2014.
[83] D. Bouneffouf, “Computing the dirichlet-multinomial
log-
likelihood function,” arXiv preprint arXiv:2007.11967, 2020.
9


===== PAGE 10 =====
[84] D. Bouneffouf, “Spectral clustering using eigenspectrum shape
based nystrom sampling,” arXiv preprint arXiv:2007.11416,
2020.
[85] D. Bouneffouf, S. Upadhyay, and Y. Khazaeni, “Online learning
from less data: Contextual bandit with missing rewards,”
[86] A. Gupta, Y. Ong, B. Da, L. Feng, and S. Handoko, “2016 ieee
congress on evolutionary computation (cec),” 2016.
[87] D.
Bouneffouf,
A.
Bouzeghoub,
and
A.
L.
Ganc¸arski,
“contextual-bandit algorithm for context-aware recommender
system,” in International conference on neural information
processing, pp. 324–331, Springer, Berlin, Heidelberg, 2012.
[88] D. Bouneffouf, I. Rish, and G. A. Cecchi, “Bandit models of
human behavior,”
[89] D. Bouneffouf, C. Aggarwal, H. Samulowitz, B. Buesser,
T. Hoang, U. Khurana, S. Liu, T. Pedapati, P. Ram, A. Rawat,
et al., “Survey on automated end-to-end data science?,”
[90] D. Bouneffouf, “Spectral clustering using eigenspectrum shape
based sampling,”
[91] D. Bouneffouf and E. Claeys, “Online hyperparameter tuning
for contextual bandits,” 2018.
[92] D. Bouneffouf, “Toward computing the dirichlet-multinomial
log-likelihood function,”
[93] K. Ding, J. Li, and H. Liu, “Interactive anomaly detection
on attributed networks,” in Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining,
WSDM ’19, (New York, NY, USA), pp. 357–365, ACM, 2019.
[94] T. L. Lai and H. Robbins, “Asymptotically efﬁcient adaptive
allocation rules,” Advances in Applied Mathematics, vol. 6,
no. 1, pp. 4–22, 1985.
[95] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis
of the multiarmed bandit problem,” Machine Learning, vol. 47,
no. 2-3, pp. 235–256, 2002.
[96] D. Bouneffouf and R. F´eraud, “Multi-armed bandit problem
with known trend,” Neurocomputing, vol. 205, pp. 16–21, 2016.
[97] S. Agrawal and N. Goyal, “Analysis of thompson sampling
for the multi-armed bandit problem,” in COLT 2012 - The
25th Annual Conference on Learning Theory, June 25-27, 2012,
Edinburgh, Scotland, pp. 39.1–39.26, 2012.
[98] J. Langford and T. Zhang, “The epoch-greedy algorithm for
multi-armed bandits with side information,” in Advances in
neural information processing systems, pp. 817–824, 2008.
[99] S. Agrawal and N. Goyal, “Thompson sampling for contextual
bandits with linear payoffs,” in ICML (3), pp. 127–135, 2013.
[100] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-
bandit approach to personalized news article recommendation,”
CoRR, 2010.
[101] R. Allesiardo, R. F´eraud, and D. Bouneffouf, “A neural net-
works committee for the contextual bandit problem,” in Neu-
ral Information Processing - 21st International Conference,
ICONIP 2014, Kuching, Malaysia, November 3-6, 2014. Pro-
ceedings, Part I, pp. 374–381, 2014.
[102] H. Bastani and M. Bayati, “Online decision-making with high-
dimensional covariates,” Available at SSRN 2661896, 2015.
[103] W. Shen, J. Wang, Y.-G. Jiang, and H. Zha, “Portfolio choices
with orthogonal bandit learning,” in Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence, 2015.
[104] X. Huo and F. Fu, “Risk-aware multi-armed bandit problem with
application to portfolio selection,” Royal Society open science,
vol. 4, no. 11, p. 171377, 2017.
[105] K. Misra, E. M. Schwartz, and J. Abernethy, “Dynamic online
pricing with incomplete information using multi-armed bandit
experiments,” 2018.
[106] J. Mueller, V. Syrgkanis, and M. Taddy, “Low-rank bandit
methods for high-dimensional dynamic pricing,” arXiv preprint
arXiv:1801.10242, 2018.
[107] Q. Zhou, X. Zhang, J. Xu, and B. Liang, “Large-scale bandit
approaches for recommender systems,” in International Confer-
ence on Neural Information Processing, pp. 811–821, Springer,
2017.
[108] S. Vaswani, B. Kveton, Z. Wen, M. Ghavamzadeh, L. V. Lak-
shmanan, and M. Schmidt, “Model-independent online learning
for inﬂuence maximization,” in Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70, pp. 3530–
3539, JMLR. org, 2017.
[109] Z. Wen, B. Kveton, M. Valko, and S. Vaswani, “Online inﬂuence
maximization under independent cascade model with semi-
bandit feedback,” in Advances in neural information processing
systems, pp. 3022–3032, 2017.
[110] D. E. Losada, J. Parapar, and A. Barreiro, “Multi-armed ban-
dits for adjudicating documents in pooling-based evaluation
of information retrieval systems,” Information Processing &
Management, vol. 53, no. 5, pp. 1005–1025, 2017.
[111] B. Liu, T. Yu, I. Lane, and O. J. Mengshoel, “Customized
nonlinear bandits for online response selection in neural con-
versation models,” in AAAI, 2018, pp. 5245–5252, 2018.
[112] T. Silander et al., “Contextual memory bandit for pro-active
dialog engagement,” 2018.
[113] S. Upadhyay, M. Agarwal, D. Bounneffouf, and Y. Khazaeni,
“A bandit approach to posterior dialog orchestration under a
budget,” 2018.
[114] D. J. Soemers, T. Brys, K. Driessens, M. H. Winands, and
A. Now´e, “Adapting to concept drift in credit card transaction
data streams using contextual bandits and decision trees,” in
AAAI, 2018.
[115] S. Boldrini, L. De Nardis, G. Caso, M. Le, J. Fiorina, and M.-
G. Di Benedetto, “mumab: A multi-armed bandit model for
wireless network selection,” Algorithms, vol. 11, no. 2, p. 13,
2018.
[116] R. Kerkouche, R. Alami, R. F´eraud, N. Varsier, and P. Maill´e,
“Node-based optimization of lora transmissions with multi-
armed bandit algorithms,” in ICT 2018, Saint Malo, France,
June 26-28, 2018, pp. 521–526, 2018.
[117] M. Gagliolo and J. Schmidhuber, “Algorithm selection as a
bandit problem with unbounded losses,” in Learning and In-
telligent Optimization, 4th International Conference, LION 4,
Venice, Italy, January 18-22, 2010. Selected Papers, pp. 82–96,
2010.
[118] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Tal-
walkar, “Hyperband: A novel bandit-based approach to hyperpa-
rameter optimization,” arXiv preprint arXiv:1603.06560, 2016.
[119] J. Wang, P. Zhao, S. C. Hoi, and R. Jin, “Online feature selection
and its applications,” IEEE Transactions on Knowledge and
Data Engineering, vol. 26, no. 3, pp. 698–710, 2014.
[120] D. Bouneffouf, I. Rish, G. A. Cecchi, and R. F´eraud, “Context
attentive bandits: Contextual bandit with restricted context,”
in IJCAI 2017, Melbourne, Australia, August 19-25, 2017,
pp. 1468–1475, 2017.
[121] R. Ganti and A. G. Gray, “Building bridges: Viewing active
learning from the multi-armed bandit lens,” arXiv preprint
arXiv:1309.6830, 2013.
[122] J. Sublime and S. Lefebvre, “Collaborative clustering through
constrained networks using bandit optimization,” in 2018 Inter-
national Joint Conference on Neural Networks, IJCNN 2018,
Rio de Janeiro, Brazil, July 8-13, 2018, pp. 1–8, 2018.
10


===== PAGE 11 =====
This figure "fig1.png" is available in "png"
 format from:
http://arxiv.org/ps/2101.00001v1
