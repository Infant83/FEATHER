<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/openalex/text/W4410193211.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/openalex/text/W4410193211.txt</h1></header>
  <main><p><em>Truncated view for readability.</em></p><div class="meta-block"><p><strong>Title:</strong> Using GNN property predictors as molecule generators</p><p><strong>Authors:</strong> Félix Therrien, Edward H. Sargent, Oleksandr Voznyy</p><p><strong>Journal:</strong> Nature Communications</p><p><strong>Published:</strong> 2025-05-08</p><p><strong>Source:</strong> <a href="https://doi.org/10.1038/s41467-025-59439-1">https://doi.org/10.1038/s41467-025-59439-1</a></p><p><strong>PDF:</strong> <a href="../archive/openalex/pdf/W4410193211.pdf">./archive/openalex/pdf/W4410193211.pdf</a></p></div><pre>

===== PAGE 1 =====
Article
<a href="https://doi.org/10.1038/s41467-025-59439-1">https://doi.org/10.1038/s41467-025-59439-1</a>
Using GNN property predictors as molecule
generators
Félix Therrien
, Edward H. Sargent
&amp; Oleksandr Voznyy
Graph neural networks (GNNs) have emerged as powerful tools to accurately
predict materials and molecular properties in computational and automated
discovery pipelines. In this article, we exploit the invertible nature of these
neural networks to directly generate molecular structures with desired elec-
tronic properties. Starting from a random graph or an existing molecule, we
perform a gradient ascent while holding the GNN weights ﬁxed in order to
optimize its input, the molecular graph, towards the target property. Valence
rules are enforced strictly through a judicious graph construction. The method
relies entirely on the property predictor; no additional training is required on
molecular structures. We demonstrate the application of this method by
generating molecules with speciﬁc energy gaps veriﬁed with density functional
theory (DFT) and with speciﬁc octanol-water partition coefﬁcients (logP). Our
approach hits target properties with rates comparable to or better than state-
of-the-art generative models while consistently generating more diverse
molecules. Moreover, while validating our framework we created a dataset of
1617 new molecules and their corresponding DFT-calculated properties that
could serve as an out-of-distribution test set for QM9-trained models.
One of the ultimate goals of computational materials science is to
rapidly identify promising material structures and compositions with
speciﬁc properties to guide experimental scientists and automated
laboratories1. This is particularly true in the time-critical ﬁelds of
pharmacy and materials for energy and sustainability where a number
of large scale autonomous experimentation initiatives are underway2,3.
In the past decades, computational materials discovery has been
achieved by going through large databases of existing materials and
computing properties from ﬁrst principles using methods such as
density functional theory (DFT) and molecular dynamics4,5. These
methods have proven to be successful in many cases6–9, but suffer
from two main limitations: (1) they are computationally expensive and
(2) they cover a small subspace of all possible materials.
In an effort to alleviate the ﬁrst problem, machine learning (ML)
based property prediction methods have become an integral part of
materials science10,11. Various models exploit different materials repre-
sentations (e.g., graphs, ﬁngerprints)12,13, model architectures (e.g.,
neural networks, random forests)14 and datasets (experimental or
computed) and their accuracy has been steadily increasing, often
competing with that of DFT10. The success and adoption of these models
is due largely to the powerful tools developed by the ML and data
science communities (such as Pytorch15, Tensorﬂow16, Pandas17, etc.).
However, despite their promising performance on benchmark datasets,
ML property predictors still suffer from poor generalizability18, exhibit-
ing much lower performance on out-of-distribution data, i.e., materials
that are different from what they have been trained on.
Materials and molecule generation can alleviate the second lim-
itation: it can theoretically explore the full space of all possible mate-
rials. Traditionally this has been done using minima hopping19,
metadynamics20 and evolutionary approaches21–23, but machines can
also learn to generate realistic materials4. The goal is no longer to
predict properties but to predict realistic structures24. Materials and
molecular generation using ML is a rapidly evolving ﬁeld with a large
body of recent methods including variational autoencoders25–27, ﬂow-
based models28,29, diffusion models30,31, models based on reinforce-
ment learning (RL)32,33 and many others34,35.
In this paper, we exploit one of the most important and funda-
mental features of neural networks, their differentiability, to directly
Received: 2 July 2024
Accepted: 16 April 2025
Check for updates
University of Toronto, Ontario, Canada.
e-mail: o.voznyy@utoronto.ca
Nature Communications|   (2025) 16:4301 
1
1234567890():,;
1234567890():,;


===== PAGE 2 =====
optimize a target property with respect to the graph representation
itself starting from a pre-trained predictive model. This concept
sometimes termed gradient ascent, or input optimization, has been
used extensively in other ﬁelds36,37 and a similar idea has been applied
to molecular generation with SELFIES38. Here we apply input optimi-
zation to molecular GNNs. We describe how carefully constraining the
molecular representation makes this “naive” approach possible and
show that it can generate molecules with requested properties as
veriﬁed with density functional theory and empirical models. It does so
with comparable or better performance than existing methods while
consistently generating the most diverse set of molecules.
Results
Rationale and workﬂow overview
Our method can technically be applied to any GNN architecture that
uses molecular graphs. To train our GNN we use (1) an explicit repre-
sentation of the adjacency matrix, A, where non-zero elements are the
bond orders and (2) a feature matrix, F, often referred to as a feature
vector, that contains a one-hot representation of the atoms. These two
matrices fully describe the graph and contain exactly the same infor-
mation as a SMILES string. Since all functions in the GNN have well-
deﬁned gradients (allowing it to be trained in the ﬁrst place), the
adjacency matrix and the feature vector can be optimized through a
gradient descent with respect to a target property as illustrated in
Fig. 1. This is termed gradient ascent–although it does not change the
direction of the gradients but rather, the variable with respect to which
they are taken. This approach could be seen as a “naive” way to tackle
the problem of conditional molecular and materials generation;
unconstrained, it would lead to meaningless results that do not follow
the basic structures of the adjacency matrix (e.g., its symmetry) and
the feature matrix (e.g., one non-zero element per line). The major
contribution of this paper is to enforce structural and chemical rules
such that optimized inputs can only be valid molecules allowing for
direct optimization into graph space.
The adjacency matrix is constructed from a weight vector wadj
containing N2N
2
elements. These elements are squared and populated
in an upper triangular matrix with zeros on the main diagonal. The
resulting matrix is then added to its transpose to obtain a positive
symmetric matrix with zero trace. Elements of the matrix are then
rounded to the nearest integer to obtain the adjacency matrix. The key
element here is that the adjacency matrix needs to have non-zero
gradients with respect to wadj, which is not the case when using a
conventional rounding half-up function. To alleviate this problem we
used a sloped rounding function,
½xsloped = ½x + aðx  ½xÞ
ð1Þ
where [x] is the conventional rounding half-up function and a is an
adjustable hyper-parameter. These steps guarantee that only valid
near-integer ﬁlled adjacency matrices are constructed. However, it
does not take into account any chemistry: atoms are allowed to form as
many bonds as there are rows in the adjacency matrix. To avoid that,
we use two strategies: (1) we penalize valence (sum of bond orders) of
more than 4 through the loss function and (2) we do not allow
gradients in the direction of higher number of bonds when the valence
is already 4.
The feature vector, on the other hand, is constructed directly
from the adjacency matrix. The idea is to deﬁne the atoms from their
valence, i.e., the sum of their bond orders. For example, a node with
four edges of value one, or, in other words, an atom forming four
single bonds, would be deﬁned as a carbon atom, a node forming one
double bond would be deﬁned as an oxygen atom, a node forming a
double bond and a single bond would be deﬁned as a nitrogen atom,
etc. In terms of matrices, this means that the sum of a row (or column)
of the adjacency matrix deﬁnes the element associated with that row
(or column).
To deal with an arbitrary number of elements that may have the
same valence, an additional weight matrix wfea is used to differentiate
between these elements. If the valence of an atom is 1, the weights of
the corresponding row in wfea specify if this atom is H, F or Cl for
example.
More details on how constraints on the adjacency matrix and
feature vectors are implemented can be found in the supplementary
information.
Energy gap targeting
The ﬁrst task is to generate molecules with a speciﬁc energy gap μ
between their highest occupied molecular orbital (HOMO) and their
lowest unoccupied molecular orbital (LUMO). The HOMO-LUMO gap
is a particularly interesting property because it is relevant to a number
of applications, it is relatively expensive to compute and it is available
in many databases39–42. Furthermore, the ability to generate molecules
with a speciﬁc emission wavelength is of special interest to our own
research on the discovery of efﬁcient blue organic light emitting
diodes (OLED) materials.
We trained a simple GNN proxy model on the QM9 dataset39 and
used our direct inverse design procedure to generate 100 molecules
with HOMO-LUMO gaps within 10 meV of 3 different target values: the
ﬁrst percentile of QM9 energy gaps, 4.1 eV, the median, 6.8 eV, and the
99th percentile, 9.3 eV. As an additional soft target, we aimed for atomic
fractions (the relative number of each elements in the molecule) close
to the average atomic fractions in QM9. This helps guide the genera-
tion towards molecules that the proxy can predict better.
The results are illustrated in Fig. 2. Note that all generated mole-
cules are within the requested range according to the proxy model by
Fig. 1 | Key concepts of the generative framework. a Molecular representation for this work using an HCN molecule as an example. b A visual representation of a typical
training process for a neural network in comparison to an input optimization scheme.
Article
<a href="https://doi.org/10.1038/s41467-025-59439-1">https://doi.org/10.1038/s41467-025-59439-1</a>
Nature Communications|   (2025) 16:4301 
2


===== PAGE 3 =====
construction: generation stops when that criterion is met. The DFT-
calculated energy gap, on the other hand, is distributed around the
requested property with relatively small overlap between different
targets.
The generated molecules predictions are overlaid on the predic-
tions for the QM9 dataset. It is apparent that the proxy model’s per-
formance is signiﬁcantly worse on generated molecules than on the
test set. If the model was generalizing perfectly, we should expect the
performance to be similar to that of the test set, MAE=0.12eV, rather
than the observed performance of about 0.8 eV. This highlights the
importance of benchmarking generating schemes on DFT-conﬁrmed
properties, not solely on ML predicted properties.
We compared our method to JANUS35 an ML enhanced state-of-
the-art genetic algorithm that was recently tested against several
materials generations schemes on various benchmarks43. We chose to
compare our method to a genetic algorithm, because of their pre-
valence, their performance44 and because, like our method they do not
require any training other than that of the proxy model. We ran the
algorithm directly with DFT as an evaluation function and with our
proxy model; the results are presented in Table 1 and in Figure S6.
Details of the calculations and JANUS model parameters can be found
in the SI.
As a measure of performance, for each target, we counted the
number of molecules within 0.5 eV of the target, the mean absolute
distance from the target value and the average Tanimoto distance
between Morgan ﬁngerprints of each pair molecules within 0.5 eV of
the target. In Table 1 we refer to our method as DIDgen, direct inverse
design generator. DIDgen and JANUS are both able to signiﬁcantly
increase the proportion of molecules within the target range com-
pared to a random draw of QM9 molecules. Our approach nearly
matches or outperforms the genetic algorithm for all 9 metrics in
Table 1.
DIDgen takes on average 12.0 s to generate an in-target molecule
for 4.1 eV, 2.1 s for 6.8 eV and 10.4 s for 9.3 eV whereas JANUS takes
about 100 s to generate 100 in-target molecules (1 s per molecule) for
all targets using the same computer (4-CPU, 3.40 GHz). Timing varies
signiﬁcantly depending on the task and parameters used for both
methods. DIDgen generates all molecules completely independently;
all 100 molecules could be generated simultaneously. We did not
implement batch generation with the use of a GPU, but it might sig-
niﬁcantly enhance performance in future versions.
logP targeting
The second task is to target a speciﬁc range of octanol-water partition
coefﬁcient (logP) values. It is relevant for drug discovery where logP
can be used as a measure of cell permeability45,46. Most commercial
drugs have a value between 0 and 5. In more recent studies in the ﬁeld
of ML, logP and penalized logP have been used extensively as a
benchmark for generative models due to the existence of a cheap
empirical model for logP developed by Wildman and Crippen45
sometimes called “Crippen logP” that is readily available in the RDkit47.
Here we will use the same target range as33 which were used in several
recent papers on molecular generation.
We trained “CrippenNet” a GNN developed speciﬁcally for this
task on a subset of the ZINC dataset26,48 and QM939. More details about
CrippenNet can be found in the Methods and in the supplementary
information. For each of the two target ranges ([-2.5, -2], [5, 5.5]) we
generated 1000 molecules and evaluated their diversity using the
average pairwise Tanimoto distance–we evaluated the diversity of all
generated molecules, not only the ones in the target range to be
consistent with ref. 33. We limited the molecule size to 85 atoms
(including hydrogens), again to be consistent with ref. 33. For this task
generated molecules can contain the same element types as in the
ZINC250 dataset: C, O, N, F, H, S, Cl, Br, I, and P. We initialized the
generation with random molecules from QM9 because CrippenNet
performed signiﬁcantly better on these molecules.
The results are presented in Table 2 in comparison with
refs. 25,27,32–34. DIDgen generates the most diverse molecules of all
methods for both target ranges. When compared to other methods that
use a trained proxy model as a predictor for logP, as opposed to the
ground truth empirical model which we termed “oracle” in Table 2,
DIDgen shows the highest performance by a factor of x4 and x2
respectively. However, it does not have a higher success rate then GCPN
and SGDS which both use the oracle directly. This is not surprising since
the success rate relies heavily on the proxy model performance and
generalizability. All methods except JT-VAE could technically use Crip-
penNet which would offer a way to compare the generation schemes
themselves separately of the proxy model’s performance (like we did for
the energy gap task).
DIDgen takes on average 5.6 s to generate a molecule in the −2.5 to
−2 range and it takes on average 3.4 s to generate a molecule in the 5 to
5.5 range on a local machine (4-CPU, 3.40GHz). The authors of LIMO
report generating 33 molecules per second withinthe target range on 2
GTX 1080 Ti GPUs. Other authors did not report their compute times.
Again, the performance of DIDgen could be improved with trivial
parallelization and batch generation on GPUs.
Discussion
Generative methods like ours that use a learned proxy rely heavily on
its performance, especially its ability to generalize in order to hit target
properties. Models that can directly use an oracle do not always suffer
from this limitation. For example, in the logP targeting task, the oracle
was not computationally limiting and thus the </pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
