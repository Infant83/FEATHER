{
  "results": [
    {
      "url": "https://arxiv.org/abs/2511.16652",
      "title": "Evolution Strategies at the Hyperscale",
      "raw_content": "[2511.16652] Evolution Strategies at the Hyperscale\n===============\n\n[Skip to main content](https://arxiv.org/abs/2511.16652#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n\n[![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)>[cs](https://arxiv.org/list/cs/recent)> arXiv:2511.16652 \n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nComputer Science > Machine Learning\n===================================\n\n**arXiv:2511.16652** (cs) \n\n [Submitted on 20 Nov 2025]\n\nTitle:Evolution Strategies at the Hyperscale\n============================================\n\nAuthors:[Bidipta Sarkar](https://arxiv.org/search/cs?searchtype=author&query=Sarkar,+B), [Mattie Fellows](https://arxiv.org/search/cs?searchtype=author&query=Fellows,+M), [Juan Agustin Duque](https://arxiv.org/search/cs?searchtype=author&query=Duque,+J+A), [Alistair Letcher](https://arxiv.org/search/cs?searchtype=author&query=Letcher,+A), [Antonio León Villares](https://arxiv.org/search/cs?searchtype=author&query=Villares,+A+L), [Anya Sims](https://arxiv.org/search/cs?searchtype=author&query=Sims,+A), [Dylan Cope](https://arxiv.org/search/cs?searchtype=author&query=Cope,+D), [Jarek Liesen](https://arxiv.org/search/cs?searchtype=author&query=Liesen,+J), [Lukas Seier](https://arxiv.org/search/cs?searchtype=author&query=Seier,+L), [Theo Wolf](https://arxiv.org/search/cs?searchtype=author&query=Wolf,+T), [Uljad Berdica](https://arxiv.org/search/cs?searchtype=author&query=Berdica,+U), [Alexander David Goldie](https://arxiv.org/search/cs?searchtype=author&query=Goldie,+A+D), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville,+A), [Karin Sevegnani](https://arxiv.org/search/cs?searchtype=author&query=Sevegnani,+K), [Shimon Whiteson](https://arxiv.org/search/cs?searchtype=author&query=Whiteson,+S), [Jakob Nicolaus Foerster](https://arxiv.org/search/cs?searchtype=author&query=Foerster,+J+N)\n\nView a PDF of the paper titled Evolution Strategies at the Hyperscale, by Bidipta Sarkar and 15 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.16652)\n> Abstract:We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations E\\in\\mathbb{R}^{m\\times n}and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices A\\in \\mathbb{R}^{m\\times r},\\ B\\in \\mathbb{R}^{n\\times r}with r\\ll \\min(m,n)to form a low-rank matrix perturbation A B^\\topthat are used in place of the full-rank perturbation E. As the overall update is an average across a population of Nworkers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from mnto r(m+n)per layer and the cost of a forward pass from \\mathcal{O}(mn)to \\mathcal{O}(r(m+n))when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast \\mathcal{O}\\left(\\frac{1}{r}\\right)rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.\n\nComments:48 pages, 12 figures, Website at [this https URL](https://eshyperscale.github.io/)\nSubjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI)\nCite as:[arXiv:2511.16652](https://arxiv.org/abs/2511.16652) [cs.LG]\n(or [arXiv:2511.16652v1](https://arxiv.org/abs/2511.16652v1) [cs.LG] for this version)\n[https://doi.org/10.48550/arXiv.2511.16652](https://doi.org/10.48550/arXiv.2511.16652)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite\n\nSubmission history\n------------------\n\n From: Bidipta Sarkar [[view email](https://arxiv.org/show-email/b82f33a9/2511.16652)] \n\n**[v1]** Thu, 20 Nov 2025 18:56:05 UTC (1,481 KB)\n\n[](https://arxiv.org/abs/2511.16652)Full-text links:\nAccess Paper:\n-------------\n\n View a PDF of the paper titled Evolution Strategies at the Hyperscale, by Bidipta Sarkar and 15 other authors\n\n*   [View PDF](https://arxiv.org/pdf/2511.16652)\n*   [TeX Source](https://arxiv.org/src/2511.16652)\n\n[![Image 5: license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/ \"Rights to this article\")\n\n Current browse context: \n\ncs.LG\n\n[<prev](https://arxiv.org/prevnext?id=2511.16652&function=prev&context=cs.LG \"previous in cs.LG (accesskey p)\") | [next>](https://arxiv.org/prevnext?id=2511.16652&function=next&context=cs.LG \"next in cs.LG (accesskey n)\")\n\n[new](https://arxiv.org/list/cs.LG/new) | [recent](https://arxiv.org/list/cs.LG/recent) | [2025-11](https://arxiv.org/list/cs.LG/2025-11)\n\n Change to browse by: \n\n[cs](https://arxiv.org/abs/2511.16652?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2511.16652?context=cs.AI)\n\n### References & Citations\n\n*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.16652)\n*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.16652)\n*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.16652)\n\nexport BibTeX citation Loading...\n\nBibTeX formatted citation\n-------------------------\n\n×\n\nData provided by: [](https://arxiv.org/abs/2511.16652)\n\n### Bookmark\n\n[![Image 6: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2511.16652&description=Evolution%20Strategies%20at%20the%20Hyperscale \"Bookmark on BibSonomy\")[![Image 7: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2511.16652&title=Evolution%20Strategies%20at%20the%20Hyperscale \"Bookmark on Reddit\")\n\nBibliographic Tools \n\nBibliographic and Citation Tools\n================================\n\n- [x] Bibliographic Explorer Toggle \n\nBibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\n- [x] Connected Papers Toggle \n\nConnected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_\n\n- [x] Litmaps Toggle \n\nLitmaps _([What is Litmaps?](https://www.litmaps.co/))_\n\n- [x] scite.ai Toggle \n\nscite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media \n\nCode, Data and Media Associated with this Article\n=================================================\n\n- [x] alphaXiv Toggle \n\nalphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_\n\n- [x] Links to Code Toggle \n\nCatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_\n\n- [x] DagsHub Toggle \n\nDagsHub _([What is DagsHub?](https://dagshub.com/))_\n\n- [x] GotitPub Toggle \n\nGotit.pub _([What is GotitPub?](http://gotit.pub/faq))_\n\n- [x] Huggingface Toggle \n\nHugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_\n\n- [x] Links to Code Toggle \n\nPapers with Code _([What is Papers with Code?](https://paperswithcode.com/))_\n\n- [x] ScienceCast Toggle \n\nScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos \n\nDemos\n=====\n\n- [x] Replicate Toggle \n\nReplicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\n- [x] Spaces Toggle \n\nHugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\n- [x] Spaces Toggle \n\nTXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_\n\nRelated Papers \n\nRecommenders and Search Tools\n=============================\n\n- [x] Link to Influence Flower \n\nInfluence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\n- [x] Core recommender toggle \n\nCORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_\n\n- [x] IArxiv recommender toggle \n\nIArxiv Recommender _([What is IArxiv?](https://iarxiv.org/about))_\n\n*   [Author](https://arxiv.org/abs/2511.16652)\n*   [Venue](https://arxiv.org/abs/2511.16652)\n*   [Institution](https://arxiv.org/abs/2511.16652)\n*   [Topic](https://arxiv.org/abs/2511.16652)\n\n About arXivLabs  \n\narXivLabs: experimental projects with community collaborators\n=============================================================\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.16652) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) \n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)",
      "images": []
    }
  ],
  "failed_results": [],
  "response_time": 4.38,
  "request_id": "f81f7114-e3e0-4007-afc8-a388dce3cdff"
}