<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/tavily_extract/0001_https_www.linkedin.com_feed_update_urn_li_activity_7399939023072358401.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/tavily_extract/0001_https_www.linkedin.com_feed_update_urn_li_activity_7399939023072358401.txt</h1></header>
  <main><pre>{
  &quot;results&quot;: [
    {
      &quot;url&quot;: &quot;<a href="https://www.linkedin.com/feed/update/urn:li:activity:7399939023072358401/&quot;">https://www.linkedin.com/feed/update/urn:li:activity:7399939023072358401/&quot;</a>,
      &quot;title&quot;: &quot;#ai #인공지능 #es #eggroll #언어모델 | Suk Hyun K. | 24 comments&quot;,
      &quot;raw_content&quot;: &quot;NVIDIA·Oxford의 충격적 논문...\n\&quot;딥러닝 시대의 2막을 열다 - 미분이 필요없는AI\&quot;\n\nNVIDIA와 옥스퍼드대가 새벽에 발표한 EGGROLL 논문은 표면적으로는 단순한 기술적 개선처럼 보인다. 그러나 그 이면을 자세히 들여다보면, 딥러닝의 기존 전제를 송두리째 뒤흔드는 패러다임의 변화가 담겨 있음을 알 수 있다.\n\n지금까지 우리는 ‘대규모 AI 모델을 훈련하려면 반드시 backpropagation과 미분 가능한 구조가 필요하다’는 믿음을 거의 종교처럼 받아들여 왔다. 그래서 모델을 키우고 싶은 사람들은 더 많은 GPU를 연결하고, 더 깊은 네트워크를 만들고, 더 효율적인 옵티마이저를 연구하며, 모든 개선을 “gradient 기반 최적화”라는 틀 안에서만 고민했다. \n\n하지만 EGGROLL은 완전히 다른 세계를 열어버렸다. “아니다. 미분 없이도, backprop 없이도, 거대한 모델을 충분히 잘 훈련할 수 있다.” 그리고 이 말이 농담이 아니라 수백만 규모의 계산 실험에서 그대로 성립한다는 것이 이번 연구가 가진 충격의 핵심이다.\n\n그들이 논문에서 제시한 진화 전략(ES)은 사실 매우 오래된 알고리즘이다. gradient 없이도 모델을 업데이트할 수 있고, 미분 불가능한 환경에서도 작동한다는 장점 덕분에 일정 기간 관심을 받았지만, 결국 학계의 주류가 되지 못했다. 이유는 매우 단순했다. 너무 비싸고, 너무 느리고, 스케일링이 되지 않았기 때문이다. \n\nES는 하나의 모델이 아니라 population 전체를 평가하는 방식이기 때문에, 개체 수가 늘어날수록 계산량이 폭발적으로 증가한다. 특히 기존 ES는 각 개체마다 거대한 full-rank 노이즈 행렬을 생성해야 했는데, 이는 대형 모델에서 메모리 이동량과 연산량을 감당할 수 없을 만큼 증가시키는 치명적 병목으로 작동했다. “좋은 아이디어지만 현실적으로 불가능한 접근법”이라는 평가가 ES에 늘 따라붙었다.\n\n그러나 이번 논문은 ES가 본질적으로 나쁜 알고리즘이 아니라는 사실을 명확하게 보여준다. 문제는 알고리즘이 아니라 구현 방식, 정확히 말하면 비용 구조였다. 우리는 ES를 ‘쓸모없다’고 결론 내리기 전에, ES를 가장 비싼 방식으로 실행하고 있었던 것이다. NVIDIA와 Oxford 연구진은 바로 이 지점에 집중했다. 그리고 기존 비용 구조의 핵심 병목을 정교하게 뜯어내어 완전히 새로운 계산 패턴을 만들어 냈다.\n\n핵심은 놀라울 만큼 직관적이다. 기존 ES의 full-rank perturbation 행렬을 그대로 만들지 말고, 두 개의 얇은(skinny) 행렬 A와 B를 곱해 low-rank perturbation을 만드는 방식을 도입하는 것이다. 이렇게 하면 계산량이 크게 줄어들 뿐 아니라, 메모리 이동량까지 극적으로 감소한다. \n\n그런데 진짜 놀라운 부분은 그 다음이다. population 전체에서 이런 low-rank perturbation 업데이트를 평균내면, 그 결과가 full-rank ES 업데이트처럼 작동한다는 사실이다. \n\n연구진은 이 사실을 이론적으로 증명하며, low-rank 업데이트가 full ES gradient에 1/r 속도로 수렴한다는 수식을 제시했다. 즉, rank r만 조절하면 계산 비용과 정확도를 필요에 따라 자유롭게 선택할 수 있는 완전히 새로운 학습 구조가 만들어지는 것이다. 그런데 사실 이 연구의 가장 충격적인 부분은 이론이 아니라 실험 결과다. \n\n연구진은 gradient도, backprop도, float 연산조차 쓰지 않고, 정수-only recurrent language model을 순수 ES만으로 처음부터 학습하는 데 성공했다. 그리고 놀랍게도 이 모델은 완전히 안정적으로 학습될 뿐 아니라, reasoning task에서 최근 화제가 된 GRPO급의 성능까지 따라잡는다. 즉, ES는 더 이상 ‘미분이 안 되는 환경에서 쓰는 보조 최적화 도구’가 아니다. 대규모 언어 모델을 훈련할 수 있는 독립적인 학습 패러다임으로 재탄생한 것이다.\n\n스케일링도 상식을 벗어난다. 기존 ES population 1,000명만 넘어가도 GPU 클러스터가 과부하되어 더 이상 확장할 수 없었다. full-rank perturbation을 전달하고 GPU 간에 교환하는 비용이 감당되지 않았기 때문이다. 그러나 EGGROLL 은perturbation을 아예 전달하지 않는다. counter-based RNG를 사용해, 각 워커(worker)가 자기 인덱스만으로 perturbation을 재현하도록 설계했기 때문이다. 메모리 이동도, 통신 비용도 사실상 0이 된다. 그 결과 ES는 population 규모 262,144명이라는 상상도 못 할 수준까지 확장된다. 기존 ES 스케일링의 200배 이상이며, 이 규모에서는 ES의 throughput이 거의 batched inference 수준까지 도달한다. gradient-free 방식이 gradient 기반 방식과 속도 경쟁을 하고 있는 것이다. 이는 단순한 효율 향상이 아니라, 패러다임 전환이다.\n\nEGGROLL이 의미하는 바는 아주 크다. 딥러닝이 지난 10년 동안 ‘미분 가능해야 한다’, ‘gradient가 존재해야 한다’는 전제를 거의 절대적인 진리로 받아들여 왔지만, 이 논문은 그 진리를 근본적으로 흔든다. 미분이 안 되는 시스템, discrete/hybrid 모델, massive simulation 기반 모델, integer-only 뉴럴넷, 강화학습의 정책 최적화, 그리고 기존 backprop이 구조적으로 어렵거나 불가능했던 모든 영역은 다시 한 번 연구할 가치가 있는 문제로 돌아온다. 배제되었던 문제의 공간 전체가 다시 열리는 것이다.\n\n논문은 하나의 강력한 메시지를 우리에게 던진다. “ES가 나빠서 포기한 게 아니다. 우리가 잘못된 방식으로 사용했기 때문에 포기한 것이다.” 그리고 NVIDIA와 Oxford는 그 잘못된 비용 구조를 제거했다. 이제 ES는 더 이상 과거의 유물이 아니라, 새로운 시대의 문을 여는 열쇠가 되었다.\n\n<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&amp;trk=public_post_embed-text">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&amp;trk=public_post_embed-text</a> <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSUaQtzTfmQVz&amp;trk=public_post_embed-text">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSUaQtzTfmQVz&amp;trk=public_post_embed-text</a> <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fes&amp;trk=public_post_embed-text\nhttps://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Feggroll&amp;trk=public_post_embed-text">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fes&amp;trk=public_post_embed-text\nhttps://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Feggroll&amp;trk=public_post_embed-text</a> <a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSLuSLqRIWQZC&amp;trk=public_post_embed-text">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSLuSLqRIWQZC&amp;trk=public_post_embed-text</a> \n<a href="https://www.linkedin.com/company/nvidia?trk=public_post_embed-text">https://www.linkedin.com/company/nvidia?trk=public_post_embed-text</a> <a href="https://uk.linkedin.com/school/oxforduni/?trk=public_post_embed-text&quot;">https://uk.linkedin.com/school/oxforduni/?trk=public_post_embed-text&quot;</a>,
      &quot;description&quot;: &quot;NVIDIA·Oxford의 충격적 논문...\n\&quot;딥러닝 시대의 2막을 열다 - 미분이 필요없는AI\&quot;\n\nNVIDIA와 옥스퍼드대가 새벽에 발표한 EGGROLL 논문은 표면적으로는 단순한 기술적 개선처럼 보인다. 그러나 그 이면을 자세히 들여다보면, 딥러닝의 기존 전제를 송두리째 뒤흔드는 패러다임의 변화가 담겨 있음을 알 수 있다.\n\n지금까지 우리는 ‘대규모 AI 모델을 훈련하려면 반드시 backpropagation과 미분 가능한 구조가 필요하다’는 믿음을 거의 종교처럼 받아들여 왔다. 그래서 모델을 키우고 싶은 사람들은 더 많은 GPU를 연결하고, 더 깊은 네트워크를 만들고, 더 효율적인 옵티마이저를 연구하며, 모든 개선을 “gradient 기반 최적화”라는 틀 안에서만 고민했다. \n\n하지만 EGGROLL은 완전히 다른 세계를 열어버렸다. “아니다. 미분 없이도, backprop 없이도, 거대한 모델을 충분히 잘 훈련할 수 있다.” 그리고 이 말이 농담이 아니라 수백만 규모의 계산 실험에서 그대로 성립한다는 것이 이번 연구가 가진 충격의 핵심이다.\n\n그들이 논문에서 제시한 진화 전략(ES)은 사실 매우 오래된 알고리즘이다. gradient 없이도 모델을 업데이트할 수 있고, 미분 불가능한 환경에서도 작동한다는 장점 덕분에 일정 기간 관심을 받았지만, 결국 학계의 주류가 되지 못했다. 이유는 매우 단순했다. 너무 비싸고, 너무 느리고, 스케일링이 되지 않았기 때문이다. \n\nES는 하나의 모델이 아니라 population 전체를 평가하는 방식이기 때문에, 개체 수가 늘어날수록 계산량이 폭발적으로 증가한다. 특히 기존 ES는 각 개체마다 거대한 full-rank 노이즈 행렬을 생성해야 했는데, 이는 대형 모델에서 메모리 이동량과 연산량을 감당할 수 없을 만큼 증가시키는 치명적 병목으로 작동했다. “좋은 아이디어지만 현실적으로 불가능한 접근법”이라는 평가가 ES에 늘 따라붙었다.\n\n그러나 이번 논문은 ES가 본질적으로 나쁜 알고리즘이 아니라는 사실을 명확하게 보여준다. 문제는 알고리즘이 아니라 구현 방식, 정확히 말하면 비용 구조였다. 우리는 ES를 ‘쓸모없다’고 결론 내리기 전에, ES를 가장 비싼 방식으로 실행하고 있었던 것이다. NVIDIA와 Oxford 연구진은 바로 이 지점에 집중했다. 그리고 기존 비용 구조의 핵심 병목을 정교하게 뜯어내어 완전히 새로운 계산 패턴을 만들어 냈다.\n\n핵심은 놀라울 만큼 직관적이다. 기존 ES의 full-rank perturbation 행렬을 그대로 만들지 말고, 두 개의 얇은(skinny) 행렬 A와 B를 곱해 low-rank perturbation을 만드는 방식을 도입하는 것이다. 이렇게 하면 계산량이 크게 줄어들 뿐 아니라, 메모리 이동량까지 극적으로 감소한다. \n\n그런데 진짜 놀라운 부분은 그 다음이다. population 전체에서 이런 low-rank perturbation 업데이트를 평균내면, 그 결과가 full-rank ES 업데이트처럼 작동한다는 사실이다. \n\n연구진은 이 사실을 이론적으로 증명하며, low-rank 업데이트가 full ES gradient에 1/r 속도로 수렴한다는 수식을 제시했다. 즉, rank r만 조절하면 계산 비용과 정확도를 필요에 따라 자유롭게 선택할 수 있는 완전히 새로운 학습 구조가 만들어지는 것이다. 그런데 사실 이 연구의 가장 충격적인 부분은 이론이 아니라 실험 결과다. \n\n연구진은 gradient도, backprop도, float 연산조차 쓰지 않고, 정수-only recurrent language model을 순수 ES만으로 처음부터 학습하는 데 성공했다. 그리고 놀랍게도 이 모델은 완전히 안정적으로 학습될 뿐 아니라, reasoning task에서 최근 화제가 된 GRPO급의 성능까지 따라잡는다. 즉, ES는 더 이상 ‘미분이 안 되는 환경에서 쓰는 보조 최적화 도구’가 아니다. 대규모 언어 모델을 훈련할 수 있는 독립적인 학습 패러다임으로 재탄생한 것이다.\n\n스케일링도 상식을 벗어난다. 기존 ES population 1,000명만 넘어가도 GPU 클러스터가 과부하되어 더 이상 확장할 수 없었다. full-rank perturbation을 전달하고 GPU 간에 교환하는 비용이 감당되지 않았기 때문이다. 그러나 EGGROLL 은perturbation을 아예 전달하지 않는다. counter-based RNG를 사용해, 각 워커(worker)가 자기 인덱스만으로 perturbation을 재현하도록 설계했기 때문이다. 메모리 이동도, 통신 비용도 사실상 0이 된다. 그 결과 ES는 population 규모 262,144명이라는 상상도 못 할 수준까지 확장된다. 기존 ES 스케일링의 200배 이상이며, 이 규모에서는 ES의 throughput이 거의 batched inference 수준까지 도달한다. gradient-free 방식이 gradient 기반 방식과 속도 경쟁을 하고 있는 것이다. 이는 단순한 효율 향상이 아니라, 패러다임 전환이다.\n\nEGGROLL이 의미하는 바는 아주 크다. 딥러닝이 지난 10년 동안 ‘미분 가능해야 한다’, ‘gradient가 존재해야 한다’는 전제를 거의 절대적인 진리로 받아들여 왔지만, 이 논문은 그 진리를 근본적으로 흔든다. 미분이 안 되는 시스템, discrete/hybrid 모델, massive simulation 기반 모델, integer-only 뉴럴넷, 강화학습의 정책 최적화, 그리고 기존 backprop이 구조적으로 어렵거나 불가능했던 모든 영역은 다시 한 번 연구할 가치가 있는 문제로 돌아온다. 배제되었던 문제의 공간 전체가 다시 열리는 것이다.\n\n논문은 하나의 강력한 메시지를 우리에게 던진다. “ES가 나빠서 포기한 게 아니다. 우리가 잘못된 방식으로 사용했기 때문에 포기한 것이다.” 그리고 NVIDIA와 Oxford는 그 잘못된 비용 구조를 제거했다. 이제 ES는 더 이상 과거의 유물이 아니라, 새로운 시대의 문을 여는 열쇠가 되었다.\n\n#AI #인공지능 #ES\n#EGGROLL #언어모델 \nNVIDIA University of Oxford  | 24 comments on LinkedIn&quot;,
      &quot;links&quot;: [
        &quot;<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&amp;trk=public_post_embed-text&quot;">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&amp;trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSUaQtzTfmQVz&amp;trk=public_post_embed-text&quot;">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSUaQtzTfmQVz&amp;trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fes&amp;trk=public_post_embed-text&quot;">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fes&amp;trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Feggroll&amp;trk=public_post_embed-text&quot;">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Feggroll&amp;trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSLuSLqRIWQZC&amp;trk=public_post_embed-text&quot;">https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2FSLuSLqRIWQZC&amp;trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://www.linkedin.com/company/nvidia?trk=public_post_embed-text&quot;">https://www.linkedin.com/company/nvidia?trk=public_post_embed-text&quot;</a>,
        &quot;<a href="https://uk.linkedin.com/school/oxforduni/?trk=public_post_embed-text&quot;">https://uk.linkedin.com/school/oxforduni/?trk=public_post_embed-text&quot;</a>
      ],
      &quot;images&quot;: [
        &quot;<a href="https://media.licdn.com/dms/image/v2/D4E03AQG1HAUz2iLWnA/profile-displayphoto-scale_400_400/B4EZlqPmoxHoAE-/0/1758424101046?e=2147483647&amp;v=beta&amp;t=UaEfUQyaDdY4ns2nPAPa4zw0-ggFcZ8ir6TeLAD08fE&quot;">https://media.licdn.com/dms/image/v2/D4E03AQG1HAUz2iLWnA/profile-displayphoto-scale_400_400/B4EZlqPmoxHoAE-/0/1758424101046?e=2147483647&amp;v=beta&amp;t=UaEfUQyaDdY4ns2nPAPa4zw0-ggFcZ8ir6TeLAD08fE&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/8fkga714vy9b2wk5auqo5reeb&quot;">https://static.licdn.com/aero-v1/sc/h/8fkga714vy9b2wk5auqo5reeb&quot;</a>,
        &quot;<a href="https://media.licdn.com/dms/image/v2/D4E22AQHboOWVgVTPNw/feedshare-shrink_800/B4EZrHdUo8KkAg-/0/1764282944407?e=2147483647&amp;v=beta&amp;t=JnasLOfiiPf_ZbIMzS_0F4z7vsI8A_dxBVMecu5gJZU&quot;">https://media.licdn.com/dms/image/v2/D4E22AQHboOWVgVTPNw/feedshare-shrink_800/B4EZrHdUo8KkAg-/0/1764282944407?e=2147483647&amp;v=beta&amp;t=JnasLOfiiPf_ZbIMzS_0F4z7vsI8A_dxBVMecu5gJZU&quot;</a>,
        &quot;<a href="https://media.licdn.com/dms/image/v2/D4E22AQHJ3TTN0hlQPg/feedshare-shrink_800/B4EZrHdUkqKoAg-/0/1764282944140?e=2147483647&amp;v=beta&amp;t=_A72Ci7lt8dD34CdJFdOjtvNYx_L2_uKtSni2c32o80&quot;">https://media.licdn.com/dms/image/v2/D4E22AQHJ3TTN0hlQPg/feedshare-shrink_800/B4EZrHdUkqKoAg-/0/1764282944140?e=2147483647&amp;v=beta&amp;t=_A72Ci7lt8dD34CdJFdOjtvNYx_L2_uKtSni2c32o80&quot;</a>,
        &quot;<a href="https://media.licdn.com/dms/image/v2/D4E22AQEFxrvler6M2A/feedshare-shrink_800/B4EZrHdU14HEAg-/0/1764282945146?e=2147483647&amp;v=beta&amp;t=QjlBErj5yViVYA-FKwMD2yRJQrDq14RIFe-P1gU-ZMA&quot;">https://media.licdn.com/dms/image/v2/D4E22AQEFxrvler6M2A/feedshare-shrink_800/B4EZrHdU14HEAg-/0/1764282945146?e=2147483647&amp;v=beta&amp;t=QjlBErj5yViVYA-FKwMD2yRJQrDq14RIFe-P1gU-ZMA&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671&quot;">https://static.licdn.com/aero-v1/sc/h/bn39hirwzjqj18ej1fkz55671&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu&quot;">https://static.licdn.com/aero-v1/sc/h/a0e8rff6djeoq8iympcysuqfu&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2&quot;">https://static.licdn.com/aero-v1/sc/h/asiqslyf4ooq7ggllg4fyo4o2&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/70k0g8kmgdfjjymflqqzipzxj&quot;">https://static.licdn.com/aero-v1/sc/h/70k0g8kmgdfjjymflqqzipzxj&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/4ol9mo4lxvobj5ww3va90wz1o&quot;">https://static.licdn.com/aero-v1/sc/h/4ol9mo4lxvobj5ww3va90wz1o&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/l7s88viq07vqke49pnrbl5e4&quot;">https://static.licdn.com/aero-v1/sc/h/l7s88viq07vqke49pnrbl5e4&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/gs508lg3t2o81tq7pmcgn6m2&quot;">https://static.licdn.com/aero-v1/sc/h/gs508lg3t2o81tq7pmcgn6m2&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/64x33s3lxd27lb5jrntc2qt3s&quot;">https://static.licdn.com/aero-v1/sc/h/64x33s3lxd27lb5jrntc2qt3s&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/a6mgx8l1bgv7yyvnzsn6mnxhn&quot;">https://static.licdn.com/aero-v1/sc/h/a6mgx8l1bgv7yyvnzsn6mnxhn&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/e97efymj9gvxv32086hxtsg5r&quot;">https://static.licdn.com/aero-v1/sc/h/e97efymj9gvxv32086hxtsg5r&quot;</a>,
        &quot;<a href="https://static.licdn.com/aero-v1/sc/h/985sqoiudwdfnuemt74raqc27&quot;">https://static.licdn.com/aero-v1/sc/h/985sqoiudwdfnuemt74raqc27&quot;</a>
      ],
      &quot;embed_url&quot;: &quot;<a href="https://www.linkedin.com/embed/feed/update/urn:li:activity:7399939023072358401&quot;">https://www.linkedin.com/embed/feed/update/urn:li:activity:7399939023072358401&quot;</a>,
      &quot;activity_id&quot;: &quot;7399939023072358401&quot;,
      &quot;extractor&quot;: &quot;linkedin_embed&quot;
    }
  ],
  &quot;failed_results&quot;: [],
  &quot;response_time&quot;: 0.0,
  &quot;request_id&quot;: &quot;local-linkedin-embed&quot;
}</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
