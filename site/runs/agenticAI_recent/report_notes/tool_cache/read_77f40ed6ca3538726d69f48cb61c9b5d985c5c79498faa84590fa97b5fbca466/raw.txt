

===== PAGE 1 =====
Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning
Xuejun Zhang
Aditi Tiwari
Zhenhailong Wang
Heng Ji
University of Illinois Urbana-Champaign
xuejunz2@illinois.edu,
hengji@illinois.edu
Abstract
Multi-image spatial reasoning remains challeng-
ing for current multimodal large language models
(MLLMs). While single-view perception is inher-
ently 2D, reasoning over multiple views requires
building a coherent scene understanding across
viewpoints. In particular, we study perspective
taking, where a model must build a coherent 3D
understanding from multi-view observations and
use it to reason from a new, language-specified
viewpoint. We introduce CAMCUE, a pose-aware
multi-image framework that uses camera pose as
an explicit geometric anchor for cross-view fu-
sion and novel-view reasoning. CAMCUE injects
per-view pose into visual tokens, grounds natural-
language viewpoint descriptions to a target cam-
era pose, and synthesizes a pose-conditioned
imagined target view to support answering. To
support this setting, we curate CAMCUE-DATA
with 27,668 training and 508 test instances pairing
multi-view images and poses with diverse target-
viewpoint descriptions and perspective-shift ques-
tions. We also include human annotated view-
point descriptions in the test split to evaluate gen-
eralization to human language. CAMCUE im-
proves overall accuracy by 9.06% and predicts
target poses from natural-language viewpoint de-
scriptions with over 90% rotation accuracy within
20◦and translation accuracy within a 0.5 error
threshold. This direct grounding avoids expensive
test-time search-and-match, reducing inference
time from 256.6s to 1.45s per example and en-
abling fast, interactive use in real-world scenarios.
Project page: https://xuejunzhang2002.
github.io/camcue/
1. Introduction
Spatial intelligence moves beyond single-image perception
and naive multi-image aggregation. Rather than treating
each view as an independent 2D snapshot, an agent needs to
connect views via their spatial relationships to form a coher-
ent 3D understanding that supports reasoning beyond the
observed images (Chen et al., 2024a; Gholami et al., 2025;
Wang et al., 2025; Yin et al., 2025; Zhao et al., 2025; Lee
et al., 2025; Yeh et al., 2025; Yang et al., 2025b). Humans
do this naturally: when told “sit on the sofa behind the black
table,” we can mentally relocate to that viewpoint and imag-
ine what we would see, then answer questions from that
perspective (Wang, 2012; Meilinger et al., 2011), which is
illustrated by Figure 1. However, current multimodal large
language models (MLLMs) still struggle with this kind of
perspective taking. Even with multiple context images, they
often fail to reliably ground a language-specified viewpoint
and reason from the intended perspective (Lee et al., 2025;
Yeh et al., 2025; Xu et al., 2025; Yin et al., 2025). This gap
motivates our study of language-guided viewpoint ground-
ing for multi-view spatial reasoning. We study perspective-
shift reasoning where the target viewpoint is specified in
natural language. Given multiple context images and a ques-
tion, the model needs to ground the description to a target
camera pose and answer from that perspective.
A recent line of work tackles perspective-shift reasoning by
augmenting MLLMs with generative world models that
actively synthesize additional observations at inference
time (Lee et al., 2025; Yang et al., 2025c). While promising,
existing pipelines are often built around a single reference
view and do not effectively integrate multiple contextual im-
ages as a unified source of evidence (Lee et al., 2025; Yang
et al., 2025c). In addition, most controllable generators are
largely query-agnostic, which can produce imagined views
that are irrelevant or even inconsistent with the downstream
question (Yang et al., 2025c). Many of these methods rely
on expensive test-time procedures such as iterative search
or multiple candidate rollouts to obtain a useful imagined
view, resulting in high latency and limited practicality. Fi-
nally, off-the-shelf novel-view synthesis is typically pose-
conditioned, whereas MLLMs do not reliably infer target
camera poses from natural language description, leaving a
mismatch between language-driven viewpoint specification
and pose-controlled generation (Jin et al., 2024; Zhou et al.,
2025).
1
arXiv:2602.06041v1  [cs.CV]  5 Feb 2026


===== PAGE 2 =====
Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning
Query: If I sit on the sofa behind the 
black table, is the plant visible next to 
the TV?
Mental Image
CAMCUE: Pose-Aware Imagination
Synthesized Target 
View
View 1
View 2
View 3
Baseline:
No viewpoint grounding
Context and Query (Input)
Model Output
Answer: Yes, the plant will 
be visible in your front-right.
CAMCUE successfully reasons
using the imagined view.
Answer: No, The plant is not 
visible.
Base model struggles to reason 
from a shifted perspective.
Figure 1. Perspective-shift reasoning with CamCue. Given multi-view context images, CamCue maps a natural-language viewpoint
description to an explicit target camera pose and synthesizes the corresponding target view for reliable spatial reasoning.
To address these limitations, we introduce CAMCUE, a pose-
aware multi-image MLLM framework that can predict the
camera pose of the language-specified target perspective.
Camera pose provides a compact, explicit representation of
viewpoint that situates each image in a shared 3D coordi-
nate frame, making inter-view geometry directly usable for
multimodal reasoning (Liao et al., 2025; Zhao et al., 2025).
Our key design principle is to make viewpoint an explicit
geometric anchor for multi-view reasoning. We start by
injecting per-view camera information into the correspond-
ing visual features, so that the model can align evidence
across images through geometry rather than treating each
image as individual input. We then interpret the natural-
language target-perspective description by mapping it to
a concrete target camera pose, which specifies where the
model should “mentally stand” to answer the question. Con-
ditioned on this predicted pose, we further synthesize the
corresponding target-view image and treat the imagined ob-
servation as additional evidence for answering. This tight
coupling between language-specified perspective, pose pre-
diction, and pose-conditioned view synthesis strengthens
multi-image fusion and substantially improves performance
on perspective-shift spatial reasoning.
To support this setting, we curate CAMCUE-DATA, a dataset
tailored to perspective-shift reasoning. CAMCUE-DATA
contains 27,668 training instances and 508 test instances,
and pairs multi-view images and per-view camera poses
with diverse natural-language target-perspective descrip-
tions, including human-annotated descriptions, and ques-
tions that require answering from the specified viewpoint.
On this benchmark, CAMCUE yields substantial gains on
perspective-shift spatial reasoning, improving overall accu-
racy by 9.06%. It also predicts target camera poses directly
from natural-language descriptions with strong accuracy,
achieving over 90% rotation accuracy within 20◦and transla-
tion accuracy at t@0.5. Moreover, by predicting an explicit
target pose, CAMCUE avoids expensive test-time search-
and-match used by prior methods, reducing inference time
from 256.6 seconds to 1.45 seconds per example and en-
abling fast, interactive use in real-world scenarios. Beyond
CAMCUE-DATA, CAMCUE also improves performance on
general multi-image spatial reasoning benchmarks such as
MindCube Tiny (Yin et al., 2025) and MMSI (Yang et al.,
2025b).
In summary, our contributions are listed as follows:
• We propose CAMCUE, a pose-aware multi-image
MLLM framework that injects per-view camera infor-
mation into the corresponding visual features, enabling
geometry-aware fusion across views for spatial reason-
ing.
• CAMCUE can map a natural-language target-view de-
scription to an explicit target camera pose, providing a
concrete viewpoint representation for answering from
the specified perspective.
• Conditioned on the predicted target pose, CAMCUE
synthesizes the corresponding target-view image and
feeds it back as additional evidence, substantially im-
proving perspective-shift reasoning ability.
• We curate CAMCUE-DATA, a dataset tailored to
perspective-shift reasoning that pairs multi-view im-
ages with diverse, detailed natural-language camera
viewpoint descriptions, including human-annotated de-
scriptions, and target-view questions that require rea-
soning from the described viewpoint.
2. Related Work
2.1. Multi-Image Spatial Reasoning Benchmarks
Multi-image spatial reasoning is a key probe for evaluat-
ing spatial intelligence in MLLMs, as it requires integrat-
ing partial observations from multiple viewpoints into a
coherent and viewpoint-consistent scene understanding. Re-
cent benchmarks reveal substantial gaps between current
2


===== PAGE 3 =====
Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning
MLLMs and human performance, with models often strug-
gling to fuse evidence across views and maintain consis-
tent spatial beliefs. Representative datasets include Mind-
Cube (Yin et al., 2025), SpatialBench (Xu et al., 2025),
MMSI-Bench (Yang et al., 2025b), All-Angles Bench (Yeh
et al., 2025), and ViewSpatial-Bench (Li et al., 2025). Sur-
veys and diagnostic studies further organize these bench-
marks by cognitive demands and emphasize that reliable
multi-view integration remains challenging (Liu et al., 2025;
Zhang et al., 2025; Yu et al., 2025). These findings moti-
vate methods that explicitly ground viewpoints and align
observations across views, such as pose-aware approaches
that use camera pose as a geometric anchor for multi-view
fusion and perspective-consistent reasoning (Chen et al.,
2024a; Liao et al., 2025).
2.2. Perspective-Taking and Allocentric Reasoning in
MLLMs
Beyond reasoning within a single image, many embodied
and multi-view tasks require perspective taking, where the
model answers questions from an alternative viewpoint that
is unobserved and specified in natural language. This setting
calls for allocentric scene understanding and reliable view-
point grounding, yet current MLLMs can be brittle under
perspective shifts even with multiple context images (Ma
et al., 2023; Yin et al., 2025; Yang et al., 2025b; Yeh et al.,
2025; Li et al., 2025). Recent approaches attempt to bridge
this gap via mental imagery or generative rollouts that syn-
thesize missing observations at inference time (Lee et al.,
2025; Yang et al., 2025c; Cao et al., 2025). While effective
in some cases, these pipelines typically do not explicitly
ground the language-specified viewpoint to a concrete target
pose, and instead rely on searching over candidate motions
and viewpoints. As a result, synthesized observations may
drift from the intended viewpoint described in language,
producing evidence that is misaligned with the target per-
spective. Moreover, searching over many candidates can be
computationally expensive, making such approaches less
suitable for applications that require timely, interactive feed-
back.
2.3. Language-Grounded Viewpoint Imagination
A key challenge underlying perspective taking is to imagine
a faithful observation from a language-specified, unobserved
viewpoint and use it to support cross-view alignment and
spatial reasoning. Existing approaches largely fall into two
lines. One line relies on pose-free image generation and
editing models to synthesize a new view directly from text
and contextual images (Team et al., 2023; Wu et al., 2025;
Achiam et al., 2023). However, such generations provide
no explicit control over camera information and may not be
reliably 3D-consistent under perspective shifts, making the
imagined evidence brittle for viewpoint-sensitive reasoning.
The other line uses pose-conditioned novel-view synthesis
models, which can produce geometrically consistent ren-
derings when a target pose is given (Zhou et al., 2025; Jin
et al., 2024; Yu et al., 2021; Charatan et al., 2024; Chen
et al., 2024b; Wu et al., 2024; Gao et al., 2024), but they do
not address the crucial missing step in our setting: mapping
a natural-language viewpoint description to the target cam-
era pose. CAMCUE bridges these two lines by learning to
predict the target camera pose from language and using it
as an explicit geometric anchor for token-level fusion and
image imagination.
3. Method
In this section, we introduce CAMCUE, a pose-aware multi-
image framework for perspective-shift spatial reasoning.
Figure 2 provides an overview of the CAMCUE pipeline.
Given a text prompt T that contains a natural-language de-
scription of a target perspective and a question, together with
a set of V contextual images I = {Ii}V
i=1 and their associ-
ated camera poses P = {Pi}V
i=1, CAMCUE predicts the an-
swer under the specified target perspective. We first present
the CAMCUE model architecture in Sec. 3.1. We then de-
scribe the construction of CAMCUE-DATA in Sec. 3.2.
3.1. Architecture
Pl¨ucker encoder
As shown in Fig. 2, each contextual
view provides camera extrinsics Ci ∈R4×4 and intrinsics
Ki.
Following prior work (Jiang et al., 2025), we transform
(Ci, Ki) into a pixel-aligned Pl¨ucker ray map
Ri = Pl¨ucker(Ci, Ki) ∈RH×W ×6,
(1)
which represents the camera pose information as dense rays
aligned with image pixels.
We then encode Ri into patch-aligned camera tokens
Zi = Epose(Ri) ∈RS×d,
(2)
where S = HpWp is the number of patch tokens under
the backbone’s canonical resolution, with Hp = H′/p and
Wp = W ′/p for patch size p.
Epose is a lightweight Pl¨ucker encoder that follows the same
patchification and spatial aggregation scheme as the vision
backbone. Specifically, Epose follows the same tokenization
pipeline as the vision encoder: it first resizes Ri to the back-
bone’s canonical resolution, then patchifies it and applies
a patch embedding to convert each local ray patch into a
d-dimensional token. This yields a patch-aligned token grid
that is spatially aligned with the image patch tokens for
subsequent fusion.
3


===== PAGE 4 =====
Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning
Input views & 
Camera Poses
Vision
Encoder
Plücker
Encoder
Image
Tokens
Camera
Tokens
MLLM
Pose
Adapter
Yes, the trash bin is on 
your right.
Image Decoder
Predicted 
Camera Pose
Imagined 
Target View
ℒpose
Ground Truth
Camera Pose
ℒlang
Ground Truth
Answer
Question: If I am standing in front 
of the door. Is the trash bin on my 
right side?
+
Fusion MLP
Pose Fusion
Figure 2. Given multiple contextual images with their camera poses and a natural-language target-viewpoint description plus question,
CamCue encodes visual content and pixel-aligned camera pose features, fuses them into pose-aware visual tokens, and uses an MLLM
with a pose adapter to jointly generate the answer and predict the target camera pose. The predicted pose can further condition an image
decoder to synthesize an imagined target view, which is fed back as additional evidence for answering.
Pose-aware token fusion.
Given the image patch tokens
Xi ∈RS×d from the vision backbone and the correspond-
ing Pl¨ucker camera tokens Zi ∈RS×d, we fuse pose in-
formation into the visual representation in a patch-aligned
manner. We concatenate tokens at the same patch index and
apply a lightweight MLP projection to produce a residual
update:
˜Xi = Xi + W [Zi; Xi],
(3)
where [·; ·] denotes feature-wise concatenation and W ∈
Rd×2d. This design preserves the backbone token layout
while injecting per-patch geometric cues, yielding fused
tokens ˜Xi ∈RS×d for subsequent multi-view reasoning.
Target pose prediction.
As shown in Fig. 2 (the Pose
Adapter branch), given the fused multi-view scene tokens
˜X ∈RTvis×d and the text hidden states H ∈RTtext×d, we
predict the target camera pose with a query