==
V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval
White sofa with mottled pillows instead of a dark brown sofa with white pillows and books down
Candidate 1
Query
Candidate 2
<think>The objective is to identify a candidate that reflects 
specific visual transformations from the query: replacing a 
dark brown sofa with white pillows and books underneath with 
a white sofa and mottled pillows.Initially, we can perform a 
pre-screening to filter the candidates. Candidate 2 is an exact 
match for the negative example described in the text—a dark 
brown sofa with white pillows and books below—so it is the 
least relevant . . . Candidate 3 is a simple sofa lacking the 
mottled texture and the storage theme , making it less 
suitable. =Candidate 4 shows a traditional white sofa with 
colorful mottled pillows and a book lying on the floor. 
Candidate 4 stands out as a strong match due to its clear 
presentation of a white sofa and prominent mottled pillows 
that align well with the textual request. While Candidate 1 
also fits the . . . Candidate 4's aesthetic emphasis on the 
white fabric and vibrant pillow patterns makes it a highly 
relevant choice for this specific transformation.Following 
these two, Candidate 3 remains more relevant than the 
remaining options , followed by the bed-like arrangement of 
Candidate 5 , and finally Candidate 2 as the explicit counter-
example.</think>
<answer>[4, 1, 3, 5, 2]</answer>
Text-based Chain of thought
Candidate 4
Candidate 3
Candidate 5
<think>Based on your requirements, I will analyze the query and candidate images to provide a ranked 
list . . . I will use the selection tool to compare them more closely. </think>
<tool_call>
{"name": "select_images", "arguments": {"target_images": [1, 4]}}
</tool_call>
<think>From the comparison: Features a modern white sofa with distinct mottled/zebra-patterned 
pillows . . . I will perform a crop on the shelving area.</think>
<tool_call>
{"name": "crop_image", "arguments": {"bbox_2d": [290, 80, 380, 200], "target_image": 1}}
</tool_call>
<think>Candidate 4: Meets the color and pillow criteria but lacks the integrated storage design . . . 
Based on this analysis, the final ranking is as follows:</think>
<answer>[1, 4, 3, 5, 2]</answer>
Multimodal Interleaved Chain of thought
Candidate 1
Candidate 4
Candidate 1
Zoom in
Figure 1. Comparison between text-based CoT (left) and multimodal interleaved CoT (right) for multimodal retrieval. Text-based
CoT relies on language-driven inference over static visual representations, often failing to resolve fine-grained differences. In contrast,
V-Retrver performs multimodal interleaved CoT reasoning by invoking visual tools to inspect candidate images, enabling grounded
reasoning and more reliable ranking decisions.
Training such an evidence-gathering retrieval agent requires
not only strong reasoning ability but also effective alignment
between retrieval performance and visual tool usage. We
therefore adopt a curriculum-based training strategy con-
sisting of three stages. First, a cold-start supervised stage
initializes the model with basic reasoning capabilities and
operation formatting using synthesized high-quality CoT
data. Second, rejection sampling fine-tuning consolidates
high-quality reasoning trajectories and improves structural
compliance. Finally, we introduce Evidence-Aligned Pol-
icy Optimization (EAPO), instantiated via Group Relative
Policy Optimization (GRPO) (Guo et al., 2025), which re-
inforces correct ranking decisions while encouraging infor-
mative visual verification and discouraging redundant tool
usage.
Extensive experiments on the universal multimodal retrieval
benchmark M-BEIR, as well as multiple out-of-domain
datasets, demonstrate that V-Retrver consistently outper-
forms strong baselines across diverse retrieval settings. The
results show that V-Retrver achieves higher retrieval ac-
curacy, more reliable perception-grounded reasoning, and
stronger generalization ability, validating the effectiveness
of interleaved visual reasoning for multimodal retrieval. In
summary, our contributions are three-fold:
• We propose V-Retrver, an evidence-driven agentic re-
trieval framework that enables MLLMs to actively ac-
quire visual evidence during multimodal reasoning.
• We introduce a curriculum-based training strategy with
an evidence-aligned reinforcement learning objective
that jointly improves reasoning quality, ranking accu-
racy, and efficient visua