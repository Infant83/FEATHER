- **제목**: V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval
- **저자**: Dongyang Chen 외 8명
- **출처**: [GitHub](https://github.com/chendy25/V-Retrver), [Hugging Face](https://huggingface.co/V-Retrver)

### 요약
- MLLMs가 다중 모달 검색에 적용되며, Chain-of-Thought (CoT) 추론이 후보 재정렬을 개선함.
- 기존 접근법은 언어 중심이며, 시각적 증거를 능동적으로 검증하지 못함.
- V-Retrver는 시각 검증 기반의 에비던스 주도 검색 프레임워크로, MLLM이 외부 시각 도구를 통해 시각적 증거를 선택적으로 획득하도록 함.
- 이 프레임워크는 가설 생성과 목표 시각 검증을 번갈아 수행하여 시각적 모호성을 해결함.
- 교육 전략으로는 감독된 추론 활성화, 거부 기반 정제, 증거 정렬 목표를 가진 강화 학습을 채택함.
- 여러 다중 모달 검색 벤치마크에서 평균 23.0%의 검색 정확도 향상을 포함한 일관된 개선을 입증함.

### 주요 내용
- 특정 시각적 변환을 반영하는 후보 이미지 식별.
- 후보 2는 부정적인 예시와 일치하여 가장 관련성이 낮음.
- 후보 4는 강력한 선택으로 평가됨.
- V-Retrver는 시각적 도구를 사용하여 후보 이미지를 검사하고 더 신뢰할 수 있는 순위 결정을 가능하게 함.
- 세 단계의 커리큘럼 기반 훈련 전략 채택.
- V-Retrver는 M-BEIR 벤치마크에서 강력한 기준선을 초과 달성하며, 높은 검색 정확도와 신뢰할 수 있는 인식 기반 추론을 입증함.

### 기여
- V-Retrver 프레임워크 제안.
- 증거 정렬 강화 학습 목표를 통한 추론 품질 및 순위 정확도 개선.

출처: [chunk_001.txt], [chunk_002.txt], [chunk_003.txt], [chunk_004.txt]