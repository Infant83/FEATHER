

===== PAGE 1 =====
Shared LoRA Subspaces for almost Strict Continual Learning
Prakhar Kaushik*†, Ankit Vaidya*, Shravan Chaudhari, Rama Chellappa, Alan Yuille
Department of Computer Science
Johns Hopkins University
Baltimore, MD, USA
{pkaushi1,schaud35,avaidya7,rchella4,ayuille1}@jhu.edu
https://toshi2k2.github.io/share/
Abstract
Adapting large pretrained models to new tasks efficiently
and continually is crucial for real-world deployment but re-
mains challenging due to catastrophic forgetting and the
high cost of retraining. While parameter-efficient tuning
methods like low rank adaptation (LoRA) reduce compu-
tational demands, they lack mechanisms for strict contin-
ual learning and knowledge integration, without relying on
data replay, or multiple adapters.
We propose Share, a
novel approach to parameter efficient continual finetuning
that learns and dynamically updates a single, shared low-
rank subspace, enabling seamless adaptation across multi-
ple tasks and modalities. Share constructs a foundational
subspace that extracts core knowledge from past tasks and
incrementally integrates new information by identifying es-
sential subspace directions. Knowledge from each new task
is incorporated into this evolving subspace, facilitating for-
ward knowledge transfer, while minimizing catastrophic in-
terference. This approach achieves up to 100× parameter
reduction and 281× memory savings over traditional LoRA
methods, maintaining performance comparable to jointly
trained models. A single Share model can replace hundreds
of task-specific LoRA adapters, supporting scalable, asyn-
chronous continual learning.
Experiments across image
classification, natural language understanding, 3D pose es-
timation, and text-to-image generation validate its effective-
ness, making Share a practical and scalable solution for
lifelong learning in large-scale AI systems.
1. Introduction
Adapting large pretrained models, like LLMs, VLMs and
Diffusion models, for continual learning presents signif-
*equal contribution
†Corresponding author: prakhark2@gmail.com
Figure 1. Evidence of a Shared Foundational Subspace in Con-
tinual Learning. Linear CKA similarity analysis reveals a univer-
sal weight subspace (orange) emerging during sequential learning.
Three independent trajectories (red, green, blue), starting from dif-
ferent GLUE task subsets, show monotonic convergence to this
shared subspace, reaching near-perfect alignment (> 0.95) by task
T = 5. Shaded regions show standard deviation across exper-
iments. These results demonstrate: (1) the existence of a com-
mon foundational weight subspace that efficiently encodes cross-
task knowledge, and (2) our method’s ability to discover it through
continual adaptation without catastrophic forgetting. This conver-
gence reveals how low-rank adapters naturally bias models toward
shared weight structures that generalize across diverse tasks.
icant challenges, notably catastrophic forgetting and the
substantial resources required for retraining.
Traditional
fine-tuning methods often require retraining all model pa-
rameters, leading to inefficiencies, especially as model
sizes increase.
As these models scale, they require
more resources and memory, making them inaccessible
to ordinary researchers and increasing environmental im-
pact [23]. Parameter-efficient finetuning techniques, such
as LoRA [10], address some of these issues by introducing
trainable low-rank matrices into each layer of the model,
arXiv:2602.06043v1  [cs.LG]  5 Feb 2026


===== PAGE 2 =====
Figure 2. Share. Our continual reparameterization where only principal coefficients ϵt are trained. a. Initialization We initialize the
principal factors (α0, β0) of our Share model using available LoRA [10] adapters (A, B). b. Continual Adaptation Few top φ ≪k
factors, shown as α0→1, β0→1, and temporary coefficients ϵ0→1 are fine-tuned when new data is incrementally received. Merging & Fine-
tuning The factors α0, β0 and temporary factors α0→1, β0→1 are merged using the initialization procedure, and α1, β1, ϵi
α,β
∀i ∈[0, 1]
are analytically recalculated. ϵ1 can then be further fine-tuned to boost performance.
effectively reducing the number of parameters that need ad-
justment during finetuning. However, while LoRA reduces
computational demands, it lacks mechanisms for continual
learning and knowledge integration, often requiring sepa-
rate adapters for each task, which can be inefficient and
hinders cross-task knowledge sharing which