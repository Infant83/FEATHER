- **제목**: Shared LoRA Subspaces for almost Strict Continual Learning
- **저자**: Prakhar Kaushik 외 (Johns Hopkins University)
- **주요 내용**:
  - 대규모 사전 훈련 모델의 효율적 적응 필요성 및 재훈련 비용과 재앙적 망각 문제 언급.
  - LoRA와 같은 파라미터 효율적 조정 방법의 한계.
  - **Share**라는 새로운 접근법 제안: 단일 공유 저차원 서브스페이스를 학습하고 업데이트하여 여러 작업에 적응.
  - Share는 과거 작업의 지식을 추출하고 새로운 정보를 통합하여 재앙적 간섭 최소화.
  - 최대 100배의 파라미터 감소 및 281배의 메모리 절약 달성, 성능은 공동 훈련 모델과 유사.
  - 수백 개의 작업별 LoRA 어댑터를 대체 가능, 비동기적 지속적 학습 지원.
  - 다양한 실험(이미지 분류, 자연어 이해 등)을 통해 효과성 검증.

- **그림 설명**:
  - 그림 1: 공유된 기초 서브스페이스의 증거를 보여주는 분석.
  - 그림 2: Share의 지속적 재매개변수화 과정 설명.

출처: [chunk_001.txt]

- 기존 방법들은 지속적 학습과 지식 통합 메커니즘 부족.
- O-LoRA와 같은 방법은 잊어버림 완화를 위해 직교 서브스페이스에서 학습 제안.
- Strict Continual Learning 요구: 데이터 재생, 모델 추가 없이 지속적 학습.
- Universal Weight Subspace Hypothesis에 따라 신경망 가중치는 작업 간 공유 서브스페이스로 수렴.
- Share는 동적으로 저차원 서브스페이스를 업데이트하여 여러 작업에 적응 가능.
- 다양한 모델과 모달리티에서 적용 가능, 효율적 지속적 학습 솔루션.

출처: [chunk_002.txt]

- **연구 주제**: Share 방법론을 통한 지속적 학습.
  
- **관련 연구**:
  - **효율적인 리플레이 없는 지속적 학습**: 대규모 모델에서의 적용 어려움.
  - **모델 병합**: 특정 도메인에 국한되거나 지속적 학습 능력 부족.
  - **저랭크 적응**: LoRA 및 변형들은 지속적 지식 통합 메커니즘 부족.

- **방법론**:
  - **문제 설정**: 최소한의 파라미터 오버헤드로 연속 작업에 적응.
  - **Share의 가설**: 유사한 작업에 대해 LoRA 어댑터가 공통 저랭크 서브스페이스 공유.
  - **Share의 작동 방식**: 초기화, 지속적 적응, 병합 및 미세 조정의 세 단계로 구성.

- **결론**: Share는 지속적 학습을 위한 확장 가능하고 적응적인 솔루션.

출처: [chunk_003.txt]

- 학습 과정은 초기화, 지속적 적응, 병합 및 미세 조정의 세 단계로 구성됨.
- **초기화 단계**: LoRA 어댑터를 사용하여 저차원 서브스페이스 형성.
- **지속적 적응 단계**: 기존 서브스페이스 보존하며 새로운 기저 벡터 추가.
- **병합 및 미세 조정 단계**: 임시 기저 벡터를 기존 기저 벡터와 병합하고 미세 조정.

출처: [chunk_004.txt]