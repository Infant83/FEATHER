.
remains diverse and natural in aspects that are not specified.
Overview. Figure 2 illustrates our three-stage paradigm.
First, we train an expert policy \pi _E for large-scale HOI mo-
tion imitation, incorporating data augmentation, physical
perturbations, and shaped rewards to promote stable whole-
body coordination and precise grasping across diverse con-
figurations (Sec. 3.2). Second, we distill the expert into a
masked conditional variational policy \pi that maps sparse
goal inputs to a multi-modal distribution (Sec. 3.3). Third,
we finetune this policy \pi using RL to enhance robustness
under unseen configurations, employing failure-state resets
to encourage recovery behaviors (Sec. 3.4).
Each stage
is modeled as a Markov Decision Process (MDP), which
shares a consistent input formulation comprising observa-
tions and goal conditioning, as well as an output action cor-
responding to low-level actuation commands (Sec. 3.1).
3.1. Policy States and Actions
Observation. The policy input at time t includes an ob-
servation that aggregates human kinematics, object kine-
matics, and their interaction and contact states,  \
b
o
ld
s y mb
o l  {x
} _ t=
\
b
i
g 
[
\,\un
d er
br ac
e {\b
ol ds
y
m
b
ol
 
{r}^h_
t ,\b ol
d sy m
bol {\theta
 
}^h_t,\boldsymbol {\dot r}^h_t,\boldsymbol {\dot \theta }^h_t}_{\text {human}},\; \underbrace {\boldsymbol {r}^o_t,\boldsymbol {\theta }^o_t,\boldsymbol {\dot r}^o_t,\boldsymbol {\dot \theta }^o_t}_{\text {object}},\; \underbrace {\boldsymbol {D}_t,\boldsymbol {C}_t}_{\text {interaction}}\,\big ]. Here, the su-
perscripts h and o denote human and object quantities, re-
spectively. \protect \boldsymbol  {r} and \protect \boldsymbol  {\theta } denote positions and orientations, re-
spectively; the dotted terms indicate linear and angular ve-
locities.
The interaction terms include signed distances
from body segments to object surfaces \protect \boldsymbol  {D}_t and binary con-
tacts \protect \boldsymbol  {C}_t derived from simulator contact forces, follow-
ing [87].
All continuous quantities are normalized in a
human root-centric and local heading frame for invariance
to global placement. The human-related terms contain 52
components for the SMPL humanoid [35] and 39 for the
3


===== PAGE 4 =====
Unitree G1 robot [64]. Each rigid body contributes one el-
ement to human-related variables in \protect \boldsymbol  {x}_t, including \protect \boldsymbol  {D}_t and
\protect \boldsymbol  {C}_t, e.g., \p r otect \boldsymbol  {D}_t \in \mathbb {R}^{39 \times 3} for G1. Objects are all rigid.
Goal Conditioning. The policy is also conditioned on a
set of future goals that specify desired human-object con-
figurations at different horizons. During training, we ex-
tract goals from reference, where each reference yt shares
the same state space as observation xt, including human,
object, and contact components. A corresponding binary
mask mt indicates which components of the reference are
provided to the policy [58].
To capture both near-term
and distant intentions, we employ two types of goal con-
ditioning: (I) a short-ho