<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/youtube/transcripts/youtu.be-nm-OnKyBNbk-Game_Theory_and_the_Dynamics_of_Complex_Agentic_Systems.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/youtube/transcripts/youtu.be-nm-OnKyBNbk-Game_Theory_and_the_Dynamics_of_Complex_Agentic_Systems.txt</h1></header>
  <main><p><em>Truncated view for readability.</em></p><pre>Title: Game Theory and the Dynamics of Complex Agentic Systems
URL: <a href="https://www.youtube.com/watch?v=nm-OnKyBNbk">https://www.youtube.com/watch?v=nm-OnKyBNbk</a>
Video ID: nm-OnKyBNbk
Channel: Lev Selector
Published: 2026-01-23T21:34:18Z
Tags: -
Hashtags: -
Summary: Game Theory and the Dynamics of Complex Agentic Systems By Maxim Yakimenko Assisted by Gemini 2.5 pro Slides on GDrive: <a href="https://drive.google.com/drive/folder">https://drive.google.com/drive/folder</a>... Slides on GitHub - <a href="https://github.com/lselector/seminar">https://github.com/lselector/seminar</a> (click on pptx file, then on &quot;raw&quot; or download button on the right) Multi-agent systems (MAS) fail with standard reinforcement learning because agent outcomes depend on each other&#x27;s a
Source: direct_url

[00:02] You can start.
[00:03] &gt;&gt; Hello. All right. Perfect. Okay. So, uh
[00:06] let&#x27;s start. So, hello everyone. Uh it&#x27;s
[00:09] a great honor to participate in love
[00:12] selectors uh zoom call. I&#x27;m here
[00:15] temporarily as in like 15 minutes. And
[00:18] uh I&#x27;m here to present a pretty
[00:20] interesting topic at least from my point
[00:21] of view that is a bit different from
[00:23] what you would normally be covering here
[00:25] with lev. This is a more generalistic
[00:28] deep dive or deep relatively deep dive
[00:30] into game theory, dynamics and complex
[00:32] agentic systems and what they are about,
[00:35] what their limits are, what their
[00:37] benefits are and where we are currently.
[00:39] Uh my name is Maxim. Uh I am a Concordia
[00:42] student last year of computer science.
[00:44] I&#x27;ve been participating in artificial
[00:46] intelligence development for about a
[00:48] year now uh across different areas.
[00:51] Parts of it is in Montreal, parts of it
[00:53] is in uh United States, but I still are
[00:57] technically a student. So there&#x27;s a lot
[00:58] of things I do not know, but I am
[01:00] confident in presenting this topic. And
[01:02] if you have any questions, please feel
[01:04] free to discuss it at the end of the
[01:05] presentation where we will probably have
[01:07] a few moments to clarify a few things.
[01:11] So let&#x27;s give a quick summary of what&#x27;s
[01:13] going on here.
[01:15] So first of all, these slides are
[01:16] obviously generated by Gemini, but I
[01:18] have reviewed them and they&#x27;re mostly
[01:20] correct with what is necessary to
[01:21] discuss. So let&#x27;s quickly run through
[01:26] the thing. So most agent systems fail
[01:29] with standard reinforcement learning
[01:30] because agent outcomes depend on each
[01:32] other&#x27;s actions in constantly changing
[01:34] environments. Use of game theory to
[01:37] model interdependencies. That&#x27;s the
[01:39] solution. And obviously uh the main
[01:43] point of all this is that for a very
[01:46] long time when it comes to evolutionary
[01:47] theory uh which was obviously
[01:50] popularized by Darwin but it was only
[01:53] recently rediscovered and implemented in
[01:55] energetic systems the assumption was
[01:57] that the most optimal theoretical model
[02:01] was the best. In reality it&#x27;s the
[02:03] flattest model that is the best and we
[02:06] are going to discuss exactly what that
[02:08] means.
[02:10] So on our agenda, we have some key
[02:12] terminology that I would like to delve
[02:14] into very quickly just to make sure that
[02:15] everyone is on the same page and we&#x27;re
[02:17] not missing or misinterpreting certain
[02:19] things. This is generally just a quick
[02:22] thing for everyone to be on the same
[02:24] page. But then I&#x27;m going to go into the
[02:26] very basic premises of game theory and
[02:29] agentic systems. Then we&#x27;re going to go
[02:31] into evolutionary game theory which is a
[02:33] broader concept. We&#x27;re actually going to
[02:35] go into a small case study there as
[02:37] well. And then we&#x27;re going to quickly
[02:38] discuss some of the issues with the
[02:40] approach and obviously the problems with
[02:42] the idea. Finally, there&#x27;s going to be a
[02:44] conclusion and further reading.
[02:47] So key terminology. So these are some of
[02:50] the main ter pieces of terminology we&#x27;re
[02:53] going to be using such a systems.
[02:56] Gentex systems are technically uh normal
[02:59] uh either AI based uh in
[03:03] semi-independent programs that run on
[03:05] either machines or other kind of
[03:07] computational software. In the some of
[03:10] the versions we&#x27;re going to look at are
[03:12] super simplified. They&#x27;re not like large
[03:14] language models. They&#x27;re just basic
[03:16] machine learning algorithms. But this is
[03:18] still relatively important.
[03:20] Reinforcement learning. I think everyone
[03:21] here understands what that is. social
[03:24] ecological systems. This is basically
[03:26] the equivalent of an ecological
[03:28] environment and there is a lot of
[03:31] specificities about this but basically
[03:33] you can uh summarize it as whatever is
[03:37] going on where the agents are actually
[03:40] technically playing together and agent
[03:43] based models obviously there&#x27;s many
[03:45] different ways of uh interpreting this
[03:48] one too. So evolution and game tree
[03:51] which is the main subject here EGT
[03:54] evolutionally stable strategies this is
[03:56] the one we&#x27;re going to focus on a little
[03:57] bit more because uh the thing that is
[04:00] best is not necessarily stable at least
[04:03] in theory and practice the ESS are the
[04:06] ones that are left over after a lot of
[04:08] processes and optoises is one of the
[04:12] more important factors here uh I don&#x27;t
[04:14] think it&#x27;s going to come up in this form
[04:16] again but this is the thing that would
[04:19] make an agent and their strategy is
[04:22] actually consistent with game theory and
[04:25] evolutionary theory aka they can
[04:26] replicate and stay around for a while.
[04:31] So uh let&#x27;s explain the premise uh what
[04:35] is this all about? So if you have delved
[04:41] into the uh current environment of
[04:43] agentic systems and their many
[04:45] complexities, you will probably have
[04:48] found some things that are flawed. In
[04:53] particular, the idea is that a lot of
[04:55] agents are static
[04:57] and also that their environment is not
[05:00] itself static. So what happens is that
[05:03] regardless of how you train or
[05:04] regardless how you try to organize the
[05:07] system, something eventually breaks.
[05:09] Either some kind of safeguard doesn&#x27;t
[05:10] work or the agentic system simply
[05:13] doesn&#x27;t perform a job in the way that
[05:15] you want it to. So that is the limits of
[05:19] a simple reinforcement learning which is
[05:21] just that there is so only so much an
[05:25] agent can actually relearn within its
[05:27] own weights before the environment
[05:29] breaks it. So the solution is to provide
[05:31] a better methano effect framework which
[05:33] is the game theory aspects of it all. So
[05:36] now we&#x27;re modeling for quality. Uh so
[05:40] the idea of games is sometimes a little
[05:42] bit deceiving in that games are just a
[05:45] set of rules. It&#x27;s not actually a game
[05:46] you&#x27;re supposed to play. But it does
[05:49] enable the ability to establish exactly
[05:52] what is the incentive, what are the
[05:54] constraints and what the agent is
[05:57] allowed to do and not. Obviously, Game
[05:59] Fury is most popular thanks to the
[06:01] prisoner dilemma discussion, but in this
[06:03] case, it&#x27;s not really the main focus
[06:07] improving efficiency. So, in ideally,
[06:10] you would use these things to improve
[06:13] the systems. So, let&#x27;s read out a little
[06:15] bit helps us understand the dynamics of
[06:18] learning in a multi- aent system, not
[06:20] just the final equilibrium state. So,
[06:22] that&#x27;s the point. Even though you can
[06:24] technically model the ideal state where
[06:26] your agents will perform exactly the way
[06:28] you want them to, you do not necessarily
[06:31] have the ability to model the dynamics
[06:33] and how it gets to that state or if it
[06:35] ever will reach it in the first place.
[06:38] So by understanding these we can
[06:40] technically create things like
[06:41] replicators which is part of the initial
[06:44] point that I was telling you in the
[06:46] definitions.
[06:47] So the idea is of of course is that with
[06:50] all the C efficiency you would then try
[06:52] to simulate and figure out what works
[06:55] best in a complicated environment where
[06:57] math kind of just isn&#x27;t really all that
[07:00] you can use in order to figure out the
[07:03] best possibilities. So now we&#x27;re going
[07:05] to go into the broader concepts. So the
[07:08] idea here is that a lot of models
[07:12] uh are built around the idea that you
[07:15] can be rational aka you can logically
[07:18] deduce the best way forward.
[07:20] Unfortunately, logical deduction relies
[07:23] on perfect information clarity, which a
[07:26] complex system that is constantly
[07:28] immutation and transient formats
[07:32] cannot really have. Even if you have
[07:35] full transparency in an agentic system
[07:37] across many different things, just the
[07:39] time it takes for a system to readjust
[07:42] itself, if something changed is already
[07:45] a moment of
[07:47] uh loss essentially where the clarity
[07:49] isn&#x27;t quite there even though the
[07:51] numbers technically are
[07:55] replicated dynamics are a bit different.
[07:57] So this is the main principle behind
[08:00] this whole concept. The growth rate of a
[08:03] strategy within the population is
[08:04] proportional to how much it payoff
[08:06] exceeds the average payoff of the entire
[08:08] population. Essentially what works gets
[08:10] replicated. The simple rule can produce
[08:13] highly complex evolutionary
[08:14] trajectories. So this is the point that
[08:17] uh is important to highlight because
[08:19] what works isn&#x27;t always constant and
[08:22] what&#x27;s worse it takes a little bit of
[08:24] time for what works to overtake what
[08:27] doesn&#x27;t. And in that soup is where
[08:30] evolutionary theory and game theory take
[08:32] hold.
[08:34] So pure mathematical ag often assumes
[08:38] infinite population and zero mutation
[08:41] which is unrealistic. Correct. Uh we are
[08:44] not working with infinite resources and
[08:46] often we have only so much we can work
[08:48] with. Agent based models allow us to
[08:51] introduce these crucial factors. So
[08:53] finite population and mutations. These
[08:56] factors can lead to evolutionary
[08:57] outcomes that are impossible to predict
[08:59] with step mathematical equations.
[09:01] Survival of the flattest. This is where
[09:03] I&#x27;m going to explain what flattest
[09:05] means. The realistic setting in which
[09:09] there is a lot of things going on uh at
[09:11] the same time. The flattest strategy is
[09:13] the one that is the most resilient.
[09:16] Uh in evolution we often times consider
[09:20] that the species the most well adapted
[09:23] to the environment is the one that&#x27;s
[09:24] going to survive and thrive which is
[09:26] true if the environment doesn&#x27;t change
[09:29] regularly. So a good example is the
[09:32] dodo. The dodo was perfectly uh evolved
[09:35] to live on an island by itself with no
[09:37] predators. The moment humans came along
[09:40] none of that worked and the dodo just
[09:42] went extinct. And that&#x27;s kind of kind
[09:44] the story with agents. If a system is
[09:48] overly stable, it&#x27;s not representative
[09:50] of the real world and is thus
[09:52] unreliable. A environment where you
[09:55] cannot mutate into a perfect state is
[09:58] usually what you&#x27;re actually looking at.
[10:00] Thus, the inefficient in this point, the
[10:03] inefficient yet resilient form is
[10:06] usually what you&#x27;re looking for, which
[10:07] is the flattest.
[10:09] And here is a quick list of common
[10:12] agentic behaviors that were explored in
[10:14] some of the two papers that I was mostly
[10:16] referencing here. So there&#x27;s the
[10:18] learning adapt uh adapting exploration
[10:22] and exploitation decision making uh
[10:24] punishment self-optimization and genetic
[10:27] evolution. There&#x27;s another page here and
[10:30] I&#x27;m going to read turn to the other
[10:31] slide quickly and uh these are strategic
[10:34] interactions. This is a little bit of a
[10:36] different concept. Some of these are uh
[10:38] created by hand. But importantly, we
[10:41] need to like look at tit for tat. This
[10:43] one in particular was shown to be most
[10:45] efficient through other game theory uh
[10:49] experiments. Overall, tit for tat has
[10:51] shown that over a long course of time.
[10:55] It usually accumulates more points than
[10:57] anything else. Again, this assumes low
[11:00] mutation and uh low variability, but it
[11:04] generally is the most effective way that
[11:07] even people use most often in politics
[11:10] and in large social groups, obviously
[11:12] communication. So, this is just a
[11:15] shorthand for what&#x27;s going on. These
[11:17] strategies, I don&#x27;t think we have too
[11:19] much time to get into in detail and why
[11:22] they work and why they&#x27;re not. But the
[11:25] idea is that uh you want your agents to
[11:28] work together but just normal cohesion
[11:31] is not necessarily the best especially
[11:33] when you need something more
[11:34] hierarchical and you also need to have
[11:37] an idea of who is responsible for
[11:40] correcting with mistakes. So you do need
[11:43] to figure out what is responsible for
[11:45] what. So let&#x27;s go real quick into the uh
[11:49] quick case study that came from the
[11:51] paper evolutionary game theory using
[11:53] agentic based methods. All the links and
[11:56] uh references will be provided in the
[11:57] end of the slides. So essentially this
[12:00] is the reformulation of what we&#x27;ve said
[12:02] so far. the theoretical stability is
[12:07] potentially high but in practice it&#x27;s
[12:10] actually unstable because it&#x27;s easy to
[12:12] destabilize in the environment having a
[12:14] lot of mutators could potentially uh
[12:16] break it down very easily. Meanwhile,
[12:19] there&#x27;s this concept of quasi species
[12:22] which have involved a completely
[12:24] separate strategy that works for an
[12:26] environment that usually self-supports
[12:29] have completely mostly randomized sets
[12:32] of rules inside of them yet are for some
[12:34] reason incredibly highly stable yet
[12:36] relatively inefficient compared to the
[12:38] optimal strategy that the researchers
[12:41] devised. And this is a constant not only
[12:44] in agentic situations but just in
[12:46] general in nature too.
[12:49] So issues and limits
[12:53] in mathematics we really really like
[12:55] usually things that are clean but that&#x27;s
[12:57] very rarely what we can handle. Uh
[13:00] mathematics model the real world but the
[13:03] real world is extremely messy. So
[13:07] we need to work with mathematical
[13:10] baselines but we need to figure out what
[13:12] works in practice and technically agent
[13:16] dynamics is one of the ways we can
[13:18] actually figure out and test these
[13:19] things in practice.
[13:21] So but what are the main issues
[13:24] to do these practice in these
[13:26] experiments? Well, it&#x27;s incredibly
[13:28] computationally demanding. you&#x27;re
[13:30] working with multiple highle cognition
[13:33] models regardless of which uh machine
[13:36] learning model you prefer and all of
[13:39] them have extremely heavy data
[13:41] requirements and complexity. Not only
[13:43] can you have difficulty modeling all
[13:46] this, just running the damn thing for
[13:48] long enough to actually get fancy
[13:50] results is just straight up expensive.
[13:53] Now, especially so
[13:56] now
[13:58] the main thing that people cannot figure
[14:00] out with a genetic systems is all the
[14:02] poes. Because even if you figure out
[14:05] something that works, something that
[14:07] self-replicates and sustains itself is
[14:10] still far beyond what most res</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
