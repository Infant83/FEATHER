Title: Game Theory and the Dynamics of Complex Agentic Systems
URL: https://www.youtube.com/watch?v=nm-OnKyBNbk
Video ID: nm-OnKyBNbk
Channel: Lev Selector
Published: 2026-01-23T21:34:18Z
Tags: -
Hashtags: -
Summary: Game Theory and the Dynamics of Complex Agentic Systems By Maxim Yakimenko Assisted by Gemini 2.5 pro Slides on GDrive: https://drive.google.com/drive/folder... Slides on GitHub - https://github.com/lselector/seminar (click on pptx file, then on "raw" or download button on the right) Multi-agent systems (MAS) fail with standard reinforcement learning because agent outcomes depend on each other's a
Source: direct_url

[00:02] You can start.
[00:03] >> Hello. All right. Perfect. Okay. So, uh
[00:06] let's start. So, hello everyone. Uh it's
[00:09] a great honor to participate in love
[00:12] selectors uh zoom call. I'm here
[00:15] temporarily as in like 15 minutes. And
[00:18] uh I'm here to present a pretty
[00:20] interesting topic at least from my point
[00:21] of view that is a bit different from
[00:23] what you would normally be covering here
[00:25] with lev. This is a more generalistic
[00:28] deep dive or deep relatively deep dive
[00:30] into game theory, dynamics and complex
[00:32] agentic systems and what they are about,
[00:35] what their limits are, what their
[00:37] benefits are and where we are currently.
[00:39] Uh my name is Maxim. Uh I am a Concordia
[00:42] student last year of computer science.
[00:44] I've been participating in artificial
[00:46] intelligence development for about a
[00:48] year now uh across different areas.
[00:51] Parts of it is in Montreal, parts of it
[00:53] is in uh United States, but I still are
[00:57] technically a student. So there's a lot
[00:58] of things I do not know, but I am
[01:00] confident in presenting this topic. And
[01:02] if you have any questions, please feel
[01:04] free to discuss it at the end of the
[01:05] presentation where we will probably have
[01:07] a few moments to clarify a few things.
[01:11] So let's give a quick summary of what's
[01:13] going on here.
[01:15] So first of all, these slides are
[01:16] obviously generated by Gemini, but I
[01:18] have reviewed them and they're mostly
[01:20] correct with what is necessary to
[01:21] discuss. So let's quickly run through
[01:26] the thing. So most agent systems fail
[01:29] with standard reinforcement learning
[01:30] because agent outcomes depend on each
[01:32] other's actions in constantly changing
[01:34] environments. Use of game theory to
[01:37] model interdependencies. That's the
[01:39] solution. And obviously uh the main
[01:43] point of all this is that for a very
[01:46] long time when it comes to evolutionary
[01:47] theory uh which was obviously
[01:50] popularized by Darwin but it was only
[01:53] recently rediscovered and implemented in
[01:55] energetic systems the assumption was
[01:57] that the most optimal theoretical model
[02:01] was the best. In reality it's the
[02:03] flattest model that is the best and we
[02:06] are going to discuss exactly what that
[02:08] means.
[02:10] So on our agenda, we have some key
[02:12] terminology that I would like to delve
[02:14] into very quickly just to make sure that
[02:15] everyone is on the same page and we're
[02:17] not missing or misinterpreting certain
[02:19] things. This is generally just a quick
[02:22] thing for everyone to be on the same
[02:24] page. But then I'm going to go into the
[02:26] very basic premises of game theory and
[02:29] agentic systems. Then we're going to go
[02:31] into evolutionary game theory which is a
[02:33] broader concept. We're actually going to
[02:35] go into a small case study there as
[02:37] well. And then we're going to quickly
[02:38] discuss some of the issues with the
[02:40] approach and obviously the problems with
[02:42] the idea. Finally, there's going to be a
[02:44] conclusion and further reading.
[02:47] So key terminology. So these are some of
[02:50] the main ter pieces of terminology we're
[02:53] going to be using such a systems.
[02:56] Gentex systems are technically uh normal
[02:59] uh either AI based uh in
[03:03] semi-independent programs that run on
[03:05] either machines or other kind of
[03:07] computational software. In the some of
[03:10] the versions we're going to look at are
[03:12] super simplified. They're not like large
[03:14] language models. They're just basic
[03:16] machine learning algorithms. But this is
[03:18] still relatively important.
[03:20] Reinforcement learning. I think everyone
[03:21] here understands what that is. social
[03:24] ecological systems. This is basically
[03:26] the equivalent of an ecological
[03:28] environment and there is a lot of
[03:31] specificities about this but basically
[03:33] you can uh summarize it as whatever is
[03:37] going on where the agents are actually
[03:40] technically playing together and agent
[03:43] based models obviously there's many
[03:45] different ways of uh interpreting this
[03:48] one too. So evolution and game tree
[03:51] which is the main subject here EGT
[03:54] evolutionally stable strategies this is
[03:56] the one we're going to focus on a little
[03:57] bit more because uh the thing that is
[04:00] best is not necessarily stable at least
[04:03] in theory and practice the ESS are the
[04:06] ones that are left over after a lot of
[04:08] processes and optoises is one of the
[04:12] more important factors here uh I don't
[04:14] think it's going to come up in this form
[04:16] again but this is the thing that would
[04:19] make an agent and their strategy is
[04:22] actually consistent with game theory and
[04:25] evolutionary theory aka they can
[04:26] replicate and stay around for a while.
[04:31] So uh let's explain the premise uh what
[04:35] is this all about? So if you have delved
[04:41] into the uh current environment of
[04:43] agentic systems and their many
[04:45] complexities, you will probably have
[04:48] found some things that are flawed. In
[04:53] particular, the idea is that a lot of
[04:55] agents are static
[04:57] and also that their environment is not
[05:00] itself static. So what happens is that
[05:03] regardless of how you train or
[05:04] regardless how you try to organize the
[05:07] system, something eventually breaks.
[05:09] Either some kind of safeguard doesn't
[05:10] work or the agentic system simply
[05:13] doesn't perform a job in the way that
[05:15] you want it to. So that is the limits of
[05:19] a simple reinforcement learning which is
[05:21] just that there is so only so much an
[05:25] agent can actually relearn within its
[05:27] own weights before the environment
[05:29] breaks it. So the solution is to provide
[05:31] a better methano effect framework which
[05:33] is the game theory aspects of it all. So
[05:36] now we're modeling for quality. Uh so
[05:40] the idea of games is sometimes a little
[05:42] bit deceiving in that games are just a
[05:45] set of rules. It's not actually a game
[05:46] you're supposed to play. But it does
[05:49] enable the ability to establish exactly
[05:52] what is the incentive, what are the
[05:54] constraints and what the agent is
[05:57] allowed to do and not. Obviously, Game
[05:59] Fury is most popular thanks to the
[06:01] prisoner dilemma discussion, but in this
[06:03] case, it's not really the main focus
[06:07] improving efficiency. So, in ideally,
[06:10] you would use these things to improve
[06:13] the systems. So, let's read out a little
[06:15] bit helps us understand the dynamics of
[06:18] learning in a multi- aent system, not
[06:20] just the final equilibrium state. So,
[06:22] that's the point. Even though you can
[06:24] technically model the ideal state where
[06:26] your agents will perform exactly the way
[06:28] you want them to, you do not necessarily
[06:31] have the ability to model the dynamics
[06:33] and how it gets to that state or if it
[06:35] ever will reach it in the first place.
[06:38] So by understanding these we can
[06:40] technically create things like
[06:41] replicators which is part of the initial
[06:44] point that I was telling you in the
[06:46] definitions.
[06:47] So the idea is of of course is that with
[06:50] all the C efficiency you would then try
[06:52] to simulate and figure out what works
[06:55] best in a complicated environment where
[06:57] math kind of just isn't really all that
[07:00] you can use in order to figure out the
[07:03] best possibilities. So now we're going
[07:05] to go into the broader concepts. So the
[07:08] idea here is that a lot of models
[07:12] uh are built around the idea that you
[07:15] can be rational aka you can logically
[07:18] deduce the best way forward.
[07:20] Unfortunately, logical deduction relies
[07:23] on perfect information clarity, which a
[07:26] complex system that is constantly
[07:28] immutation and transient formats
[07:32] cannot really have. Even if you have
[07:35] full transparency in an agentic system
[07:37] across many different things, just the
[07:39] time it takes for a system to readjust
[07:42] itself, if something changed is already
[07:45] a moment of
[07:47] uh loss essentially where the clarity
[07:49] isn't quite there even though the
[07:51] numbers technically are
[07:55] replicated dynamics are a bit different.
[07:57] So this is the main principle behind
[08:00] this whole concept. The growth rate of a
[08:03] strategy within the population is
[08:04] proportional to how much it payoff
[08:06] exceeds the average payoff of the entire
[08:08] population. Essentially what works gets
[08:10] replicated. The simple rule can produce
[08:13] highly complex evolutionary
[08:14] trajectories. So this is the point that
[08:17] uh is important to highlight because
[08:19] what works isn't always constant and
[08:22] what's worse it takes a little bit of
[08:24] time for what works to overtake what
[08:27] doesn't. And in that soup is where
[08:30] evolutionary theory and game theory take
[08:32] hold.
[08:34] So pure mathematical ag often assumes
[08:38] infinite population and zero mutation
[08:41] which is unrealistic. Correct. Uh we are
[08:44] not working with infinite resources and
[08:46] often we have only so much we can work
[08:48] with. Agent based models allow us to
[08:51] introduce these crucial factors. So
[08:53] finite population and mutations. These
[08:56] factors can lead to evolutionary
[08:57] outcomes that are impossible to predict
[08:59] with step mathematical equations.
[09:01] Survival of the flattest. This is where
[09:03] I'm going to explain what flattest
[09:05] means. The realistic setting in which
[09:09] there is a lot of things going on uh at
[09:11] the same time. The flattest strategy is
[09:13] the one that is the most resilient.
[09:16] Uh in evolution we often times consider
[09:20] that the species the most well adapted
[09:23] to the environment is the one that's
[09:24] going to survive and thrive which is
[09:26] true if the environment doesn't change
[09:29] regularly. So a good example is the
[09:32] dodo. The dodo was perfectly uh evolved
[09:35] to live on an island by itself with no
[09:37] predators. The moment humans came along
[09:40] none of that worked and the dodo just
[09:42] went extinct. And that's kind of kind
[09:44] the story with agents. If a system is
[09:48] overly stable, it's not representative
[09:50] of the real world and is thus
[09:52] unreliable. A environment where you
[09:55] cannot mutate into a perfect state is
[09:58] usually what you're actually looking at.
[10:00] Thus, the inefficient in this point, the
[10:03] inefficient yet resilient form is
[10:06] usually what you're looking for, which
[10:07] is the flattest.
[10:09] And here is a quick list of common
[10:12] agentic behaviors that were explored in
[10:14] some of the two papers that I was mostly
[10:16] referencing here. So there's the
[10:18] learning adapt uh adapting exploration
[10:22] and exploitation decision making uh
[10:24] punishment self-optimization and genetic
[10:27] evolution. There's another page here and
[10:30] I'm going to read turn to the other
[10:31] slide quickly and uh these are strategic
[10:34] interactions. This is a little bit of a
[10:36] different concept. Some of these are uh
[10:38] created by hand. But importantly, we
[10:41] need to like look at tit for tat. This
[10:43] one in particular was shown to be most
[10:45] efficient through other game theory uh
[10:49] experiments. Overall, tit for tat has
[10:51] shown that over a long course of time.
[10:55] It usually accumulates more points than
[10:57] anything else. Again, this assumes low
[11:00] mutation and uh low variability, but it
[11:04] generally is the most effective way that
[11:07] even people use most often in politics
[11:10] and in large social groups, obviously
[11:12] communication. So, this is just a
[11:15] shorthand for what's going on. These
[11:17] strategies, I don't think we have too
[11:19] much time to get into in detail and why
[11:22] they work and why they're not. But the
[11:25] idea is that uh you want your agents to
[11:28] work together but just normal cohesion
[11:31] is not necessarily the best especially
[11:33] when you need something more
[11:34] hierarchical and you also need to have
[11:37] an idea of who is responsible for
[11:40] correcting with mistakes. So you do need
[11:43] to figure out what is responsible for
[11:45] what. So let's go real quick into the uh
[11:49] quick case study that came from the
[11:51] paper evolutionary game theory using
[11:53] agentic based methods. All the links and
[11:56] uh references will be provided in the
[11:57] end of the slides. So essentially this
[12:00] is the reformulation of what we've said
[12:02] so far. the theoretical stability is
[12:07] potentially high but in practice it's
[12:10] actually unstable because it's easy to
[12:12] destabilize in the environment having a
[12:14] lot of mutators could potentially uh
[12:16] break it down very easily. Meanwhile,
[12:19] there's this concept of quasi species
[12:22] which have involved a completely
[12:24] separate strategy that works for an
[12:26] environment that usually self-supports
[12:29] have completely mostly randomized sets
[12:32] of rules inside of them yet are for some
[12:34] reason incredibly highly stable yet
[12:36] relatively inefficient compared to the
[12:38] optimal strategy that the researchers
[12:41] devised. And this is a constant not only
[12:44] in agentic situations but just in
[12:46] general in nature too.
[12:49] So issues and limits
[12:53] in mathematics we really really like
[12:55] usually things that are clean but that's
[12:57] very rarely what we can handle. Uh
[13:00] mathematics model the real world but the
[13:03] real world is extremely messy. So
[13:07] we need to work with mathematical
[13:10] baselines but we need to figure out what
[13:12] works in practice and technically agent
[13:16] dynamics is one of the ways we can
[13:18] actually figure out and test these
[13:19] things in practice.
[13:21] So but what are the main issues
[13:24] to do these practice in these
[13:26] experiments? Well, it's incredibly
[13:28] computationally demanding. you're
[13:30] working with multiple highle cognition
[13:33] models regardless of which uh machine
[13:36] learning model you prefer and all of
[13:39] them have extremely heavy data
[13:41] requirements and complexity. Not only
[13:43] can you have difficulty modeling all
[13:46] this, just running the damn thing for
[13:48] long enough to actually get fancy
[13:50] results is just straight up expensive.
[13:53] Now, especially so
[13:56] now
[13:58] the main thing that people cannot figure
[14:00] out with a genetic systems is all the
[14:02] poes. Because even if you figure out
[14:05] something that works, something that
[14:07] self-replicates and sustains itself is
[14:10] still far beyond what most researchers
[14:14] can demonstrate and handle. Not just
[14:17] because it's a question of complexity,
[14:20] but also ethically it's difficult to
[14:22] justify creating something that mimics
[14:25] life so closely that it could actually
[14:27] be a runaway experiment. There has been
[14:29] a lot of fear and fearongering around
[14:31] this concept. That's why it's
[14:33] particularly touchy and tabooish in
[14:35] certain circles.
[14:38] And obviously the interability
[14:40] I wanted to mention this specifically
[14:42] because there's actually a computational
[14:44] limit within uh the architectures we
[14:46] normally use for these experiments which
[14:48] is the problem of Alan Turing's model of
[14:52] computing. Uh,
[14:55] I do not currently possess a solution to
[14:58] this problem and probably no one does.
[15:02] But technically speaking, things like
[15:04] the halting problem simply reinforce the
[15:06] idea that not just agents but computers
[15:08] in general are limited by a scope of
[15:13] solvable problems. And these complexity
[15:15] environments are usually sometimes
[15:19] exactly outside of that solvability
[15:21] because again a computer cannot
[15:24] necessarily know how a program actually
[15:27] ends or not. It's simple but yet
[15:30] extremely effective demonstration of the
[15:32] concept. So in conclusion, evolutionary
[15:36] theory in agendic systems provides a
[15:40] very interesting deep dive into how to
[15:43] socially organize a bunch of independent
[15:46] computers or independent agentic systems
[15:49] that try to resolve a problem as a
[15:52] communal resource aka your GPU and your
[15:55] electricity gets used.
[15:58] The effectiveness of many strategies
[16:00] that have been discussed over time over
[16:02] the last months, years have been
[16:04] variable. But in the end, the biggest
[16:07] issue is that all of it is so abstract
[16:11] and so distanced from the actual social
[16:15] functional game theory logic that is
[16:17] difficult to justify how it currently
[16:19] functions and is incredibly inefficient.
[16:21] But given enough time, given enough
[16:24] injection and better model architecture,
[16:27] it is technically possible to use all
[16:28] these things to get better results. So
[16:32] I'm going to just quickly show you some
[16:34] of the resources and sources used here.
[16:37] Uh we have systematic review of agendic
[16:40] based system dynamics and evolutionary
[16:42] game theory in multi- aent reinforcement
[16:44] learning. These are just a few main
[16:48] resources that I've been using here and
[16:50] obviously for the reading the books. Uh
[16:53] I have mostly just briefly touched on
[16:56] these for the sake of interest and
[16:58] curiosity but obviously the building
[17:00] blocks here are much more complex than I
[17:02] can explain within 15 minutes. So thank
[17:06] you very much for listening to me.
[17:07] Hopefully I did not go over the time too
[17:09] much.
