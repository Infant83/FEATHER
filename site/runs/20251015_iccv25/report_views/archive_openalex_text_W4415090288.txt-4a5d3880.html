<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/openalex/text/W4415090288.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/openalex/text/W4415090288.txt</h1></header>
  <main><p><em>Truncated view for readability.</em></p><div class="meta-block"><p><strong>Title:</strong> The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results</p><p><strong>Authors:</strong> Qiuyu Chen, Xin Jin, Yue Song, Xihui Liu, Shuai Yang, Tao Yang, Ziqiang Li, Jianguo Huang, Yuntao Wei, Bowen Xie, Nicu Sebe, Wenjun, Zeng, Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amir Habibian, Auke Wiggers, Masato Kobayashi, Ning Ding, Toru Tamaki, Marzieh Gheisari, Auguste Genovesio, Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu, Junhao Geng, Lin Lv, Jianxin Lin, Han Liang, Jie Zhou, Xiaoyuan Chen, Jinbao Wang, Can Gao, Zhangyi Wang, Zongze Li, Bihan Wen, Yanyan Gao, Xiaohan Pan, Xin Li, Zhibo Chen, Bo Peng, Z. J. Chen, Haoran Jin</p><p><strong>Journal:</strong> arXiv (Cornell University)</p><p><strong>Published:</strong> 2025-08-15</p><p><strong>Source:</strong> <a href="http://arxiv.org/abs/2509.10463">http://arxiv.org/abs/2509.10463</a></p><p><strong>PDF:</strong> <a href="../archive/openalex/pdf/W4415090288.pdf">./archive/openalex/pdf/W4415090288.pdf</a></p><p><strong>Summary:</strong><br />This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop&#x27;s objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.</p></div><pre>

===== PAGE 1 =====
The 1st International Workshop on Disentangled
Representation Learning for Controllable
Generation (DRL4Real): Methods and Results
Qiuyu Chen†
Xin Jin†,*
Yue Song†
Xihui Liu†
Shuai Yang†
Tao Yang†
Ziqiang Li†
Jianguo Huang†
Yuntao Wei†
Ba’ao Xie†
Nicu Sebe†
Wenjun (Kevin) Zeng†
Jooyeol Yun
Davide Abati
Mohamed Omran
Jaegul Choo
Amir Habibian
Auke Wiggers
Masato Kobayashi
Ning Ding
Toru Tamaki
Marzieh Gheisari Auguste Genovesio
Yuheng Chen
Dingkun Liu
Xinyao Yang
Xinping Xu
Baicheng Chen
Dongrui Wu
Junhao Geng
Lexiang Lv
Jianxin Lin
Hanzhe Liang
Jie Zhou
Xuanxin Chen
Jinbao Wang
Can Gao
Zhangyi Wang
Zongze Li
Bihan Wen
Yixin Gao
Xiaohan Pan
Xin Li
Zhibo Chen
Baorui Peng
Zhongming Chen
Haoran Jin
Abstract—This paper reviews the 1st International Workshop on
Disentangled Representation Learning for Controllable Generation
(DRL4Real), held in conjunction with ICCV 2025. The workshop
aimed to bridge the gap between the theoretical promise of Disen-
tangled Representation Learning (DRL) and its application in real-
istic scenarios, moving beyond synthetic benchmarks. DRL4Real fo-
cused on evaluating DRL methods in practical applications such as
controllable generation, exploring advancements in model robust-
ness, interpretability, and generalization. The workshop accepted 9
papers covering a broad range of topics, including the integration of
novel inductive biases (e.g., language), the application of diffusion
models to DRL, 3D-aware disentanglement, and the expansion
of DRL into specialized domains like autonomous driving and
EEG analysis. This summary details the workshop’s objectives, the
themes of the accepted papers, and provides an overview of the
methodologies proposed by the authors.
I. INTRODUCTION
Disentangled Representation Learning (DRL) is a critical
area of research aimed at enabling AI systems to decom-
pose observed data into underlying, interpretable factors of
variation. By decoupling complex entities into independent
latent factors, DRL holds the potential to address fundamental
challenges in AI, such as enhancing the controllability and
interpretability of generative systems and improving model
generalization.
Despite significant academic interest and progress in DRL
methodologies, primarily based on Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs), the
field has largely remained confined to synthetic datasets. The
transition to realistic scenarios has been hindered by the
complex nature of real-world data and the lack of robust
benchmarks and unified evaluation metrics. Traditional DRL
methods often struggle when faced with the weaker inductive
biases present in real-world environments.
The ICCV 2025 DRL4Real Workshop was organized to
address this gap. It aimed to foster the development of novel,
†Workshop Organizers. *Corresponding author: jinxin@eitech.edu.cn.
The DRL4Real website: <a href="https://drl-for-real.github.io/DRL-for-Real/index">https://drl-for-real.github.io/DRL-for-Real/index</a>.
html
The Competition website: <a href="https://eval.ai/web/challenges/challenge-page/2527/">https://eval.ai/web/challenges/challenge-page/2527/</a>
overview
realistic datasets and comprehensive benchmarks for evalu-
ating DRL methods in practical applications. The workshop
encouraged submissions exploring how DRL can advance
model capabilities, with a focus on key areas including con-
trollable generation. This summary provides an overview of
the workshop and the contributions of the accepted papers.
II. WORKSHOP OVERVIEW
The DRL4Real workshop aimed to achieve two primary
goals: (1) to provide a comprehensive review of recent de-
velopments in applying DRL to realistic scenarios, and (2)
to serve as a forum for researchers to explore the challenges
and opportunities in controllable generation using disentangled
representations.
The workshop attracted diverse submissions spanning vari-
ous modalities and applications. Following a rigorous review
process, 9 papers were accepted for presentation.
III. WORKSHOP PAPERS AND THEMES
The accepted papers showcased several innovative tech-
niques and highlighted emerging trends in the application
of DRL to realistic challenges. We summarize the principal
themes observed across the submissions:
1) DRL for Precise Controllable Generation and Edit-
ing. A major focus was on leveraging DRL to achieve
fine-grained control in generation and editing tasks.
This included using pre-trained DRL models to extract
semantic priors that explicitly constrain edits [8] and
modeling spatial reasoning for plausible object place-
ment [1].
2) Leveraging Diffusion Models and Novel Inductive
Biases. Recognizing the limitations of traditional regu-
larization, many authors integrated DRL with Diffusion
Probabilistic Models (DPMs) and introduced novel in-
ductive biases. This included using textual semantics as
a regularization prior [5], leveraging inherent diffusion
properties like time-varying bottlenecks [3], and incor-
porating structural prompts [7].
arXiv:2509.10463v1  [cs.LG]  15 Aug 2025


===== PAGE 2 =====
3) 3D-Aware and Sequential Disentanglement. Several
papers addressed the challenge of disentangling factors
in complex spatial and temporal data. Methods ex-
plored 3D-aware generation for autonomous driving [9],
diffusion-based video factorization [3], reducing static
bias in action recognition [2], and semantic isolation
theory for 3D anomaly detection [6].
4) Expanding DRL to Specialized Domains. The work-
shop demonstrated the broadening scope of DRL into
specialized, realistic domains. Contributions included
applications in autonomous driving [9], 3D anomaly
detection [6], and EEG analysis for Brain-Computer
Interfaces [4].
5) Foundation Models for Compact Representations.
The interplay between large foundation models and rep-
resentation learning was also explored, notably in using
multimodal LLMs (GPT-4o) to generate high-fidelity
images from highly compact textual representations for
compression [7].
IV. ACCEPTED PAPERS
This section details the methodologies and contributions of
the papers accepted at the DRL4Real workshop, organized
thematically.
A. DRL for Controllable Image Generation and Editing
1) A Guided Fine-tuning Framework for Diffusion Models
with Disentangled Semantic Priors for Multi-Factor Image
Editing [8]: This paper addresses the challenge of unintended
alterations in complex, multi-factor image editing using diffu-
sion models. They propose a Guided Fine-tuning Framework
that incorporates disentangled semantic priors as structural
constraints.
Description. The framework (Figure 1) introduces a dual-
conditioning approach. While text prompts guide what to
change, a disentangled semantic prior guides what to preserve.
1) Semantic Prior Extraction: A pre-trained disentangle-
ment model (EncDiff) is used as a Semantic Encoder
(τϕ) to extract a set of independent semantic concept
tokens (S) from the input image.
2) Guidance Adapter: A lightweight, trainable adapter
module is integrated into a pre-trained editing model
(InfEdit). This adapter fuses the text prompt (P) and the
semantic prior (S) using Multi-Head Cross-Attention,
where Ep serves as Query and Es serves as Key/Value.
The resulting structurally-aware prompts (P refined) guide the
editing model more precisely.
Implementation Details. The framework is validated on
the ’DRL for Real’ competition Multi-Factor Track dataset.
By fine-tuning only the adapter module, the method reduces
computational costs. Evaluations using CLIP Score, LPIPS,
and FID demonstrated that the framework significantly reduces
unintended attribute changes while maximizing desired edits.
Fig. 1. Proposed Guided Fine-tuning Framework. (a) Existing methods. (b)
Our method using Semantic Encoder and Guidance Adapter. (c) Detailed
architecture of the Guidance Adapter. (Source: Paper 9 [8])
2) Textual Semantics Matters: Unsupervised Representa-
tion Disentanglement in Realistic Scenarios with Language
Inductive Bias (TA-Dis) [5]: This paper proposes Text-Aided
Disentanglement (TA-Dis), a framework that leverages the in-
herently disentangled nature of textual semantics to regularize
DRL in the visual domain, addressing the limitations of purely
visual constraints in realistic scenarios.
Description. TA-Dis is built upon the Latent Diffusion
Model (LDM) in a two-stage approach (Figure 2). This novel
use of language as a regularizer builds upon recent trends in
unsupervised disentanglement that leverage diffusion model
properties [14], [15], sparse transformations [16], and graph-
based reasoning with large language models [17], all aiming
for more robust and meaningful latent representations [18].
The first stage establishes a semantic projector Psem to obtain
a semantic code Zsem with primary disentanglement capabil-
ity. The core innovation is the second stage, introducing Text-
Aided Regularization based on CLIP scores to further enhance
the disentanglement capability of Zsem. Three language induc-
tive biases are designed: Image-Text Alignment (using Lpull
and Lpush), Cycle Consistency (Order Loss Lo), and Shift
Equivariance (Exchangeable Loss Le). These losses regularize
the disentanglement process by enforcing constraints in the
image-text space.
Fig. 2.
Pipeline of the TA-Dis framework. (a) LDM backbone with a
semantic projector Psem. (b) Text-aided regularization via Order Loss and
Exchangeable Loss. (Source: Paper 5 [5])
Implementation Details. The framework utilizes a pre-
trained VAE and U-Net, optimizing the semantic projector


===== PAGE 3 =====
using AdamW. Experiments demonstrated superior disentan-
glement (measured by TAD) compared to VAE- and other
Diffusion-based DRL methods on realistic datasets.
3) Imagining the Unseen: Generative Location Modeling
for Object Placement [1]: This paper tackles the problem of
location modeling—determining plausible locations for non-
existing objects in a scene. They propose a generative approach
to handle the inherent ambiguity and data sparsity of the task.
Description. The authors reframe location modeling as a
generative task, P(Y |X, C), using an autoregressive trans-
former (Figure 3, left). The input image and target object class
condition the model, which sequentially generates bounding
box coordinates. To utilize negative annotations (implausible
locations), they incorporate Direct Preference Optimization
(DPO), as shown in Figure 3 (right). DPO fine-tunes the model
by maximizing the likelihood that positive locations (Y +) are
preferred over negative locations (Y −) based on the Bradley-
Terry model.
Fig. 3. Overview of the generative location model. Left: The autoregressive
transformer generates bounding box coordinates. Right: Direct Preference
Optimization (DPO) refines the model by aligning with positive and negative
location preferences. (Source: Paper 1 [1])
Implementation Details. The model uses a small GPT-2
architecture, pretrained on the PIPE dataset and fine-tuned
on OPA. The generative model achieved superior placement
accuracy on OPA and improved visual coherence in object
insertion tasks compared to instruction-tuned editing methods.
B. Sequential and 3D Disentanglement
1) DiViD: Disentangled Video Diffusion for Static-Dynamic
Factorization [3]: This work introduces DiViD, the first
end-to-end video diffusion framework designed explicitly for
static-dynamic factorization, aiming to overcome the informa-
tion leakage common in VAE/GAN approaches.
Description. DiViD incorporates several key inductive bi-
ases within a DDPM framework (Figure 4). The sequence
encoder employs an Architectural Bias by extracting the
static token (s) from the first frame (f1) and dynamic tokens
(di) from the residuals (fi −f1), explicitly removing static
content from the motion code. The decoder utilizes Diffusion-
driven Inductive Biases: a Time-Varying Information Bottle-
neck inherent to the diffusion process, and Cross-Attention
Interaction in the U-Net to route global static and local
dynamic information appropriately.
Fig. 4.
Overview of DiViD. The sequence encoder uses residual encoding
(subtracting f1) to separate static (s) and dynamic (d1:N) tokens, which
condition the Denoising U-Net. (Source: Paper 3 [3])
Implementation Details. DiViD is trained end-to-end using
the standard DDPM loss augmented by an orthogonality
regularization term between static and dynamic tokens. Eval-
uations on MHAD and MEAD showed superior performance
compared to SOTA methods.
2) Disentangling Static and Dynamic Information for Re-
ducing Static Bias in Action Recognition [2]: This paper
addresses the problem of static bias in action recognition,
where models rely excessively on static cues rather than
dynamic motion.
Description. A two-stream architecture is proposed to sep-
arate unbiased features (fu, dynamics) from biased features
(fb, static cues), as illustrated in Figure 5. The biased stream
is designed to be inaccessible to temporal information (either
via architecture or input manipulation). Disentanglement is
achieved through two mechanisms: (1) Statistical Indepen-
dence Loss (Lind) using the Hilbert-Schmidt Independence
Criterion (HSIC) to minimize dependence between fu and fb.
(2) Adversarial Scene Prediction Loss (LS), using a Gradient
Reversal Layer (GRL) on the unbiased stream to force it to fail
at scene prediction, thereby removing background information
from fu.
Implementation Details. Experiments on datasets empha-
sizing temporal information demonstrated that the method
effectively reduces static bias metrics.
3) Controllable Generation with Disentangled Representa-
tive Learning of Multiple Perspectives in Autonomous Driv-
ing [9]: This paper presents a framework for controllable
multi-view image generation in autonomous driving scenarios,
focusing on disentangling key semantic factors from the scene
representation and viewpoint.
Description. The proposed method employs a structured la-
tent representation that decomposes the generative process into
three factors: scene content (zsc), weather (zw), and speed (zs).
These codes are derived via variational encoding of semantic
labels. The architecture utilizes a triplane-based 3D generator
conditioned on these latent codes. The triplane representation
efficiently maps the latent codes into a compact 3D scene rep-
resentation. A factor-aware decoder then estimates color and
density (c, σ) for sampled 3D locations, which are synthesized
into 2D views via differentiable volumetric rendering (similar
to NeRF). This formulation allows independent control over
weather, motion, and viewpoint.


===== PAGE 4 =====
Fig. 5.
The proposed two-stream architecture for disentanglement. An
unbiased stream processes temporal dynamics, while a biased stream focuses
on static cues. Disentanglement is enforced via an independence loss (Lind)
and an adversarial scene prediction loss. (Source: Paper 2 [2])
Implementation Details. The model is trained with a hybrid
objective including reconstruction loss, semantic consistency
loss across latent-modified samples, and volumetric rendering-
based regularization. The approach was validated on a custom
multi-view driving dataset, demonstrating high-fidelity, seman-
tically controllable view synthesis, including view completion.
4) Fence Theorem: Towards Dual-Objective Semantic-
Structure Isolation in Preprocessing Phase for 3D Anomaly
Detection [6]: This paper addresses the lack of a unified the-
oretical foundation for preprocessing design in 3D Anomaly
Detection (AD). It establishes the Fence Theorem and pro-
poses Patch3D as an implementation.
Description. The Fence Theorem formalizes preprocessing
as a dual-objective semantic isolator: (1) mitigating cross-
semantic interference, and (2) confining anomaly judgments
to aligned semantic spaces to establish intra-semantic compa-
rability. The theorem posits that preprocessing aims to divide
the point cloud into mutually non-interfering (orthogonal) se-
mantic spaces. Guided by this theorem, the authors implement
Patch3D (Figure 6):
1) Patch-Cutting: Uses FPS and K-Means to segment a
single point cloud into mul</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
