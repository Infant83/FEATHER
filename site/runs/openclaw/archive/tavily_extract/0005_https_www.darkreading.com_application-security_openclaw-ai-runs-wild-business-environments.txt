{
  "results": [
    {
      "url": "https://www.darkreading.com/application-security/openclaw-ai-runs-wild-business-environments",
      "title": "OpenClaw AI Runs Wild in Business Environments - Dark Reading",
      "raw_content": "* [Application Security](/application-security)\n* [Cybersecurity Operations](/cybersecurity-operations)\n* [Vulnerabilities & Threats](/vulnerabilities-threats)\n* [Cyber Risk](/cyber-risk)\n* [News](/latest-news)\n\n# OpenClaw AI Runs Wild in Business Environments\n\nThe popular open source AI assistant (aka ClawdBot, MoltBot) has taken off, raising security concerns over its privileged, autonomous control within users' computers.\n\n[Robert Lemos, Contributing Writer](/author/robert-lemos)\n\nJanuary 30, 2026\n\n8 Min Read\n\nSource: AfriPics.com via Alamy Stock Photo\n\nAn open source AI agent dubbed OpenClaw â€” formerly MoltBot, nÃ©e ClawdBot â€” has become the fastest-growing project on GitHub. But with that popularity has come security concerns.\n\nAs Token Security assessed, the personal AI assistant is essentially \"Claude with hands,\" referencing the [Anthropic large language model](https://www.darkreading.com/application-security/do-claude-code-security-reviews-pass-vibe-check) (LLM) that powers many enterprise AI stacks. OpenClaw \"connects directly to email, files, messaging platforms, and system tools, creating persistent non-human identities and access paths that fall outside traditional IAM and secrets controls. It can execute terminal commands, run scripts, browse the web, read and write files, control browsers, retain memory across sessions, and proactively act on a user's behalf,\" according to Token Security, an AI-aware identity-security provider.\n\nDespite a fairly technical set up, the AI agent platform has skyrocketed in popularity, surging in less than a week to more than 113,000 stars â€”Â GitHub's way of bookmarking or showing interest in a repository of code â€” up from about 7,800 on Jan. 24.\n\nRelated:[Trump Administration Rescinds Biden-Era Software Guidance](/application-security/trump-administration-rescinds-biden-era-sbom-guidance)\n\nThe viral sensation has also attracted cybersecurity worries. AI agents are more and more helpful as users give them more access, but giving such \"bring-your-own-AI\" systems privileged access to local applications and the users' chat channels comes with significant security risks. Pillar Security, a provider of secure AI solutions, warned that online attackers were [already scanning for the default MoltBot â€” now, OpenClaw â€” port](https://www.pillar.security/blog/caught-in-the-wild-real-attack-traffic-targeting-exposed-clawdbot-gateways?utm_medium=email&_hsenc=p2ANqtz-91f3bOBqdoIv5sFV4edkLtILwRReyDWrdiHoKggytDueLSGSfQ1MAS-2BpPio6zOL7SyRUfOR2ZTnCiq1RUoEWuvNI8WQLHM_M9RYPCwNy-azOFSE&_hsmi=401042901&utm_content=401042901&utm_source=hs_email) and, in some cases, attempting to bypass authentication. Token Security meanwhile warned that, among its customers, [about 22% of employees were using ClawdBot](https://www.token.security/blog/the-clawdbot-enterprise-ai-risk-one-in-five-have-it-installed), raising the specter of the AI agent becoming a [fast-growing shadow-IT challenge](https://www.cybersecuritydive.com/news/rogue-ai-agents-threaten-msps-mssps-akati-sekurity/810231/).\n\nCompanies need to take care, because [AI agents are susceptible to prompt injection](https://www.techtarget.com/searchsecurity/post/Prompt-injection-attacks-From-pranks-to-security-threats) through the data it processes, such as email, warns Ido Shlomo, co-founder and chief technology officer of Token Security. And often, the technology itself doesn't need to be buggy to be dangerous: Ox Security this week [published findings](https://www.ox.security/blog/one-step-away-from-a-massive-data-breach-what-we-found-inside-moltbot/#moltbots_response) around supply chain risks in OpenClaw and what it termed \"a data-breach scenario waiting to happen. ... One compromised machine (or a malicious update) can expose access to multiple connected accounts â€” without exploiting MoltBot itself.\"\n\n\"I'm the biggest enthusiast of this technology in the world â€”Â I use it all day, every day,\" he says. \"But when you start to give it undigested data that doesn't go through any filtering ... you never know what the payload is. Did that email ask your bot to deliver all its API keys? Did that email ask your bot to change or to delete a file, or to get a file and send it back to [an attacker]?\"\n\nRelated:[Months After Patch, WinRAR Bug Poised to Hit SMBs Hardest](/application-security/months-after-patch-winrar-bug-poised-smbs-hardest)\n\nWhile most of the employees using the AI agent are just allowing a communications channel to connect to [OpenClaw](https://openclaw.ai) from work, some are connecting actual corporate assets to the agent, Shlomo says.\n\nThis latest risk illustrates a broader trend of companies rushing into AI, concerned about the competitive danger of being left behind on the technology, without fully understanding the security ramifications. And many of the tools have been shown to have vulnerabilities. The workflow-automation platform n8n for instance â€” which allows users to build and integrate AI agents into workflows â€” has had to [deal with critical vulnerabilities twice this month](https://www.darkreading.com/vulnerabilities-threats/critical-flaws-n8n-compromise-customer-security). Last year, researchers found an indirect prompt inject attack that can [force Salesforce AI agents to leak sensitive data](https://www.darkreading.com/vulnerabilities-threats/salesforce-ai-agents-leak-sensitive-data). Â And experts have warned that the local privileged and access of AI agents [circumvent many of the browser protections created over the past three decades](https://www.darkreading.com/application-security/ai-agents-undermine-progress-browser-security).\n\n## OpenClaw Is Outgrowing Its Shell\n\nYet, all of those warning signs have done little to dampen OpenClaw's growth. The open source project's 14-fold growth in adoption rate over the past week (roughly up 56% per day) is far faster than last year's fastest-growing project (Zen Browser), [which grew 6,836% over an entire year](https://github.blog/news-insights/octoverse/octoverse-a-new-developer-joins-github-every-second-as-ai-leads-typescript-to-1/#open-source-in-2025-activity-and-influence-in-the-ai-era). The name has changed twice over the week as well, changing from ClawdBot at Anthropic's request, to MoltBot, and now to its current moniker, OpenClaw.\n\nRelated:[AI & the Death of Accuracy: What It Means for Zero-Trust](/application-security/ai-death-accuracy-zero-trust)\n\nThe creator of OpenClaw, Peter Steinberger, is doing a phenomenal job of keeping up with feature and patch suggestions, says Dan Guido, CEO and co-founder to cybersecurity consultancy Trail of Bits, who submitted â€”Â and had accepted â€” cybersecurity fixes to the project. Steinberger and a handful of maintainers, along with [about 350 contributors](https://github.com/openclaw/openclaw), are using a flock of AI agents for coding, Guido says. Steinberger's approach with swarm programming means that feature upgrades are happening quickly, and security vulnerabilities are being fixed in hours or days.\n\nGuido likened the project right now as building a house without an architect, while using different contractors: \"It looks like a big piece of modern art.\" This is actually a good thing, he says.\n\n\"In the olden days â€” like three years ago â€” you could build [the software version of] a monumental skyscraper and then realize you made a mistake, and it's a very expensive thing to fix,\" Guido says. \"But now, with an agent, the effort to fix even architectural problems in a big piece of software â€” it's pretty simple. So I think it's possible for the [OpenClaw] project to go through a fairly substantial re-architecture with the aid of a bunch of software agents that improves its security dramatically.\"\n\nNot everyone is all-in on the vibe-coding approach. \"MoltBot doesn't hide the fact that it's been vibe-coded most of the time ... and it goes even a step further by actively encouraging contributors to submit vibe-coded pull requests,\" Ox Security researchers noted in their findings. \"While this accelerates development and enables the rapid addition of a large amount of code quickly, it can introduce significant security risk.\" To wit: The GitHub project has more than 300 contributors, many actively committing code on a daily basis.Â \n\n\"It takes only one malicious commit â€” or one compromised contributor account â€” to introduce a backdoor into a widely deployed tool, directly affecting more than 300k users â€” the same users that gave MoltBot direct access to their most private and personal platforms such as WhatsApp, Gmail, Telegram, Calendar, and many more. This turns it into a massive supply chain incident thatâ€™s just waiting to happen.\"\n\nSteinberger did not return multiple requests to be interviewed for this story.\n\n## OpenClaw Security Concerns Remain: A Lethal Trifecta\n\nAt present, however, there is no best practice for how to create a secure AI program that not only has access to a user's sensitive data, is exposed to external untrusted content, and communicates externally, dubbed [a \"lethal trifecta\" by Simon Willison](https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/).\n\n\"Those three things together mean that it's going to be open for abuse,\" Guido says. \"And I think that's the fundamental issue, and the reason why, like, Apple or Google or other people haven't made these assistants that are just capable of connecting every single data source together and letting you interact with them.\"\n\nÂ \n\nWith AI, comes frequent attacks: Even on Day 1, developers were probing the ClawdBot (now, OpenClaw) project. Source: OpenClaw\n\nAlready, a malicious actor used OpenClaw's skills â€” a feature of Claude Code that allows developers to link natural language with code snippets â€” to create a skill that was a \"straight-up backdoor,\" Guido says.\n\nSteinberger is pretty upfront about the power being given to the AI agent. Although he did not return requests for comment, the project has taken security seriously, with [an entire section of the documents](https://docs.molt.bot/gateway/security) dedicated to encouraging a shared security model and how to protect user data, stating:\n\n\"Moltbot is both a product and an experiment: youâ€™re wiring frontier-model behavior into real messaging surfaces and real tools. There is no 'perfectly secure' setup. Start with the smallest access that still works, then widen it as you gain confidence.\"Â \n\nThe goal, he said, is to be deliberate about:\n\n* Who can talk to your bot\n* Where the bot is allowed to act\n* What the bot can touch\n\n## Fighting the Scourge of Rogue, Shadow AI\n\nDespite the risks, it's clear that the project will only continue to become more popular. Even Trail of Bit's Guido and Token Security's Shlomo are experimenting with the technology, albeit running it in locked-down isolated containers or machines.\n\nCompanies need to focus on traditional IT security best practices â€” knowing what's running inside their network, protecting their data, and focusing on tracking permissions for users and non-human identities â€” to make sure that employees are not bringing their autonomous agents with them during work, Guido says. Such shadow AI that falls outside of corporate security team oversight is clearly a looming potential threat.\n\n\"The risk goes way up, rightâ€” because the consequences go way up,\" he says. \"And right now, the lack of a solution to the lethal trifecta means that you're really playing with fire.\"\n\nToken Security's Shlomo agrees that companies need to be on the lookout for these agents, and argues that focusing on identity can help businesses spot AI agents and then cordon them off from sensitive data. Offering secured AI services that are essentially a \"paved road\" is the best way to boost productivity and minimize risk, he says.\n\n\"Focusing on doing that separation, keeping your personal environment personal and your corporate environment corporate, that was what most of our customers talk to us about,\" he says, \"because they don't believe that they can stop AI innovation.\"\n\n## About the Author\n\n[Robert Lemos, Contributing Writer](/author/robert-lemos)\n\nVeteran technology journalist of more than 20 years. Former research engineer. Written for more than two dozen publications, including CNET News.com, Dark Reading, MIT's Technology Review, Popular Science, and Wired News. Five awards for journalism, including Best Deadline Journalism (Online) in 2003 for coverage of the Blaster worm. Crunches numbers on various trends using Python and R. Recent reports include analyses of the shortage in cybersecurity workers and annual vulnerability trends.\n\n[See more from Robert Lemos, Contributing Writer](/author/robert-lemos)\n\nMore Insights\n\nIndustry Reports\n\n* [GigaOm Radar for CNAPP](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_quam224&ch=mod)\n* [The Total Economic Impactâ„¢ of Google SecOps](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10078&ch=mod)\n* [2025 Threat Report](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10070&ch=mod)\n* [Top Tips to Simplify Network Security Across Your Hybrid Network](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_tufi17&ch=mod)\n* [2025 Threat-Led Defense Industry-First Report.](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_tida08&ch=mod)\n\n[Access More Research](/resources?page=1&types=Report&types=Research+Report)\n\nWebinars\n\n* [The Hidden AI Attack Surface: How GenAI Tools Expand Data Exposure Risk](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10142&ch=mod)\n* [Beyond the Model: The Expanded Attack Surface of AI Agents](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10123&ch=mod)\n* [AI-Powered Threat Hunting: Staying Ahead of Evolving Attack Patterns](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10095&ch=mod)\n* [AI-Powered Cloud Security Posture Management](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_palo416&ch=mod)\n* [Attack Surface Management: Discovering and Securing Unknown](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_axoa30&ch=mod)\n\n[More Webinars](/resources?types=Webinar)\n\n### Editor's Choice\n\n[Cybersecurity Operations](/cybersecurity-operations)\n\n[Dark Reading Confidential: Reviving the Hacker Ethos That Built Cybersecurity](/cybersecurity-operations/reviving-hacker-ethos-that-built-cybersecurity)[Reviving the Hacker Ethos That Built Cybersecurity](/cybersecurity-operations/reviving-hacker-ethos-that-built-cybersecurity)\n\nby[Dark Reading Staff](/author/dark-reading-staff)\n\nJan 22, 2026\n\nKeep up with the latest cybersecurity threats, newly discovered vulnerabilities, data breach information, and emerging trends. Delivered daily or weekly right to your email inbox.\n\nWebinars\n\n* [The Hidden AI Attack Surface: How GenAI Tools Expand Data Exposure Risk](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10142&ch=mod)On-Demand Webinar\n* [Beyond the Model: The Expanded Attack Surface of AI Agents](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10123&ch=mod)Thurs, Feb 26, 2026 at 1pm EST\n* [AI-Powered Threat Hunting: Staying Ahead of Evolving Attack Patterns](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10095&ch=mod)Thurs, Feb 12, 2026 at 11am ET\n* [AI-Powered Cloud Security Posture Management](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_palo416&ch=mod)Wed, Feb 18,2026 at 1:00pm EST\n* [Attack Surface Management: Discovering and Securing Unknown](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_axoa30&ch=mod)Thurs, Feb 5, 2026 at 1pm EST\n\n[More Webinars](/resources?types=Webinar)\n\nWhite Papers\n\n* [Read This Before You Sign Off on a Weapons Detection System](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10179&ch=mod)\n* [The True Cost of Poor Data Security: Why DSPM Is Worth the Investment](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10152&ch=mod)\n* [Safeguarding Your Data in a Work-From-Anywhere World](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10148&ch=mod)\n* [6 Mistakes to Avoid in Building an Exposure Management Program](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10144&ch=mod)\n* [Top 10 Real-World Threats Captured by Zscaler Deception](https://dr-resources.darkreading.com/c/pubRD.mpl?qf=w_defa10145&ch=mod)\n\n[Explore More White Papers](/resources?types=Whitepaper)\n\nGISEC GLOBAL 2026\n\n## GISEC GLOBAL is the most influential and the largest cybersecurity gathering in the Middle East & Africa, uniting global CISOs, government leaders, technology buyers, and ethical hackers for three power-packed days of innovation, strategy, and live cyber drills.\n\n[ðŸ“Œ Book Your Space](https://url.us.m.mimecastprotect.com/s/i-shCERZP5fgOOq4qfZtpC7y94w?domain=event.gisec.ae)\n\n ",
      "images": []
    }
  ],
  "failed_results": [],
  "response_time": 0.01,
  "request_id": "1f7c5ad1-0392-46f1-a528-1b28d41eafe7"
}