<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/arxiv/papers.jsonl</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/arxiv/papers.jsonl</h1></header>
  <main><pre>[
  {
    &quot;arxiv_id&quot;: &quot;2101.00001v1&quot;,
    &quot;title&quot;: &quot;Etat de l&#x27;art sur l&#x27;application des bandits multi-bras&quot;,
    &quot;authors&quot;: [
      &quot;Djallel Bouneffouf&quot;
    ],
    &quot;published&quot;: &quot;2021-01-04T18:12:28+00:00&quot;,
    &quot;updated&quot;: &quot;2021-01-04T18:12:28+00:00&quot;,
    &quot;summary&quot;: &quot;The Multi-armed bandit offer the advantage to learn and exploit the already learnt knowledge at the same time. This capability allows this approach to be applied in different domains, going from clinical trials where the goal is investigating the effects of different experimental treatments while minimizing patient losses, to adaptive routing where the goal is to minimize the delays in a network. This article provides a review of the recent results on applying bandit to real-life scenario and summarize the state of the art for each of these fields. Different techniques has been proposed to solve this problem setting, like epsilon-greedy, Upper confident bound (UCB) and Thompson Sampling (TS). We are showing here how this algorithms were adapted to solve the different problems of exploration exploitation.&quot;,
    &quot;primary_category&quot;: &quot;cs.LG&quot;,
    &quot;categories&quot;: [
      &quot;cs.LG&quot;,
      &quot;cs.AI&quot;
    ],
    &quot;doi&quot;: null,
    &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2101.00001v1&quot;">https://arxiv.org/pdf/2101.00001v1&quot;</a>,
    &quot;entry_id&quot;: &quot;<a href="http://arxiv.org/abs/2101.00001v1&quot;">http://arxiv.org/abs/2101.00001v1&quot;</a>
  }
]</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
