<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/openalex/works.jsonl</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/openalex/works.jsonl</h1></header>
  <main><p><em>Truncated view for readability.</em></p><pre>[
  {
    &quot;query&quot;: &quot;iccv 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4417156065&quot;">https://openalex.org/W4417156065&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4417156065&quot;,
      &quot;title&quot;: &quot;R-LiViT: A LiDAR-Visual-Thermal Dataset&quot;,
      &quot;authors&quot;: [
        &quot;Jonas Mirlach&quot;,
        &quot;Lei Wan&quot;,
        &quot;Andreas Wiedholz&quot;,
        &quot;Hannan Ejaz Keen&quot;,
        &quot;Andreas Eich&quot;
      ],
      &quot;published&quot;: &quot;2025-07-23&quot;,
      &quot;abstract&quot;: &quot;The dataset publication to the paper R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception <a href="https://arxiv.org/abs/2503.17122">https://arxiv.org/abs/2503.17122</a>. that was accepted at the ICCV 2025. The dataset includes RGB, LiDAR und Thermal data from a roadside perspective for autonomous driving. For more information, we refer to the paper and the READMEs in the data you can download here.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.5281/zenodo.16356714&quot;">https://doi.org/10.5281/zenodo.16356714&quot;</a>,
      &quot;journal&quot;: &quot;arXiv (Cornell University)&quot;,
      &quot;cited_by_count&quot;: 0,
      &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2503.17122&quot;">https://arxiv.org/pdf/2503.17122&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://arxiv.org/pdf/2503.17122&quot;">https://arxiv.org/pdf/2503.17122&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="http://arxiv.org/abs/2503.17122&quot;">http://arxiv.org/abs/2503.17122&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;green&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2503.17122&quot;">https://arxiv.org/pdf/2503.17122&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;iccv 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4417409712&quot;">https://openalex.org/W4417409712&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4417409712&quot;,
      &quot;title&quot;: &quot;VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results&quot;,
      &quot;authors&quot;: [
        &quot;Dasong Li&quot;,
        &quot;Sizhuo Ma&quot;,
        &quot;Hang Hua&quot;,
        &quot;Wenjie Li&quot;,
        &quot;Jianliang Wang&quot;,
        &quot;Cunliu Zhou&quot;,
        &quot;Fengbin Guan&quot;,
        &quot;Xin Li&quot;,
        &quot;Zihao Yu&quot;,
        &quot;Yiting Lu&quot;,
        &quot;Ru-Ling Liao&quot;,
        &quot;Ye Yan&quot;,
        &quot;Zhibo Chen&quot;,
        &quot;Wei Sun&quot;,
        &quot;Linhan Cao&quot;,
        &quot;Yuqin Cao&quot;,
        &quot;Weixia Zhang&quot;,
        &quot;Wen Wen&quot;,
        &quot;Kaiwei Zhang&quot;,
        &quot;Zhong‚ÄêXiu Chen&quot;,
        &quot;Fangfang Lu&quot;,
        &quot;Xiongkuo Min&quot;,
        &quot;Erjia Xiao&quot;,
        &quot;Lingfeng Zhang&quot;,
        &quot;Zhan Su&quot;,
        &quot;Hao Cheng&quot;,
        &quot;Yu Liu&quot;,
        &quot;Renjing Xu&quot;,
        &quot;Long Chen&quot;,
        &quot;Xiaoshuai Hao&quot;,
        &quot;Zhimin Zeng&quot;,
        &quot;Jianqin Wu&quot;,
        &quot;Xuxu Wang&quot;,
        &quot;Qian Yu&quot;,
        &quot;Bo Hu&quot;,
        &quot;Weiwei Wang&quot;,
        &quot;Pinxin Liu&quot;,
        &quot;Yunlong Tang&quot;,
        &quot;Luchuan Song&quot;,
        &quot;Jian‚ÄêJun He&quot;,
        &quot;Jiaru Wu&quot;,
        &quot;Hanjia Lyu&quot;,
        &quot;Lyu, Hanjia&quot;
      ],
      &quot;published&quot;: &quot;2025-09-03&quot;,
      &quot;abstract&quot;: &quot;This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.48550/arxiv.2509.02969&quot;">https://doi.org/10.48550/arxiv.2509.02969&quot;</a>,
      &quot;journal&quot;: &quot;arXiv (Cornell University)&quot;,
      &quot;cited_by_count&quot;: 0,
      &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.02969&quot;">https://arxiv.org/pdf/2509.02969&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://arxiv.org/pdf/2509.02969&quot;">https://arxiv.org/pdf/2509.02969&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="http://arxiv.org/abs/2509.02969&quot;">http://arxiv.org/abs/2509.02969&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;green&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.02969&quot;">https://arxiv.org/pdf/2509.02969&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;iccv 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4415090288&quot;">https://openalex.org/W4415090288&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4415090288&quot;,
      &quot;title&quot;: &quot;The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results&quot;,
      &quot;authors&quot;: [
        &quot;Qiuyu Chen&quot;,
        &quot;Xin Jin&quot;,
        &quot;Yue Song&quot;,
        &quot;Xihui Liu&quot;,
        &quot;Shuai Yang&quot;,
        &quot;Tao Yang&quot;,
        &quot;Ziqiang Li&quot;,
        &quot;Jianguo Huang&quot;,
        &quot;Yuntao Wei&quot;,
        &quot;Bowen Xie&quot;,
        &quot;Nicu Sebe&quot;,
        &quot;Wenjun&quot;,
        &quot;Zeng&quot;,
        &quot;Jooyeol Yun&quot;,
        &quot;Davide Abati&quot;,
        &quot;Mohamed Omran&quot;,
        &quot;Jaegul Choo&quot;,
        &quot;Amir Habibian&quot;,
        &quot;Auke Wiggers&quot;,
        &quot;Masato Kobayashi&quot;,
        &quot;Ning Ding&quot;,
        &quot;Toru Tamaki&quot;,
        &quot;Marzieh Gheisari&quot;,
        &quot;Auguste Genovesio&quot;,
        &quot;Yuheng Chen&quot;,
        &quot;Dingkun Liu&quot;,
        &quot;Xinyao Yang&quot;,
        &quot;Xinping Xu&quot;,
        &quot;Baicheng Chen&quot;,
        &quot;Dongrui Wu&quot;,
        &quot;Junhao Geng&quot;,
        &quot;Lin Lv&quot;,
        &quot;Jianxin Lin&quot;,
        &quot;Han Liang&quot;,
        &quot;Jie Zhou&quot;,
        &quot;Xiaoyuan Chen&quot;,
        &quot;Jinbao Wang&quot;,
        &quot;Can Gao&quot;,
        &quot;Zhangyi Wang&quot;,
        &quot;Zongze Li&quot;,
        &quot;Bihan Wen&quot;,
        &quot;Yanyan Gao&quot;,
        &quot;Xiaohan Pan&quot;,
        &quot;Xin Li&quot;,
        &quot;Zhibo Chen&quot;,
        &quot;Bo Peng&quot;,
        &quot;Z. J. Chen&quot;,
        &quot;Haoran Jin&quot;
      ],
      &quot;published&quot;: &quot;2025-08-15&quot;,
      &quot;abstract&quot;: &quot;This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop&#x27;s objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.48550/arxiv.2509.10463&quot;">https://doi.org/10.48550/arxiv.2509.10463&quot;</a>,
      &quot;journal&quot;: &quot;arXiv (Cornell University)&quot;,
      &quot;cited_by_count&quot;: 0,
      &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.10463&quot;">https://arxiv.org/pdf/2509.10463&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://arxiv.org/pdf/2509.10463&quot;">https://arxiv.org/pdf/2509.10463&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="http://arxiv.org/abs/2509.10463&quot;">http://arxiv.org/abs/2509.10463&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;green&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.10463&quot;">https://arxiv.org/pdf/2509.10463&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;iccv 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4414756887&quot;">https://openalex.org/W4414756887&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4414756887&quot;,
      &quot;title&quot;: &quot;VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results&quot;,
      &quot;authors&quot;: [
        &quot;Yixiao Li&quot;,
        &quot;Fang Li&quot;,
        &quot;Cunliu Zhou&quot;,
        &quot;S. Xing&quot;,
        &quot;Hadi Amirpour&quot;,
        &quot;Xiaoshuai Hao&quot;,
        &quot;Guanghui Yue&quot;,
        &quot;Baoquan Zhao&quot;,
        &quot;Weide Liu&quot;,
        &quot;Xiaoyuan Yang&quot;,
        &quot;Zhengzhong Tu&quot;,
        &quot;Xinyu Li&quot;,
        &quot;Chuanbiao Song&quot;,
        &quot;Chenqi Zhang&quot;,
        &quot;Jun Lan&quot;,
        &quot;Huijia Zhu&quot;,
        &quot;Weiqiang Wang&quot;,
        &quot;Xiaoyan Sun&quot;,
        &quot;Shishun Tian&quot;,
        &quot;Dong Yan&quot;,
        &quot;Weixia Zhang&quot;,
        &quot;Junlin Chen&quot;,
        &quot;Wei Sun&quot;,
        &quot;Zhihua Wang&quot;,
        &quot;Zhuohang Shi&quot;,
        &quot;Zhongtao Luo&quot;,
        &quot;Hang Ouyang&quot;,
        &quot;Tianxin Xiao&quot;,
        &quot;Fan Yang&quot;,
        &quot;Zhaowang Wu&quot;,
        &quot;Kaixin Deng&quot;
      ],
      &quot;published&quot;: &quot;2025-09-08&quot;,
      &quot;abstract&quot;: &quot;This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: <a href="https://github.com/Lighting-YXLI/ISRGen-QA.&quot;">https://github.com/Lighting-YXLI/ISRGen-QA.&quot;</a>,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.48550/arxiv.2509.06413&quot;">https://doi.org/10.48550/arxiv.2509.06413&quot;</a>,
      &quot;journal&quot;: &quot;arXiv (Cornell University)&quot;,
      &quot;cited_by_count&quot;: 0,
      &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.06413&quot;">https://arxiv.org/pdf/2509.06413&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://arxiv.org/pdf/2509.06413&quot;">https://arxiv.org/pdf/2509.06413&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="http://arxiv.org/abs/2509.06413&quot;">http://arxiv.org/abs/2509.06413&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;green&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.06413&quot;">https://arxiv.org/pdf/2509.06413&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;iccv 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4415254644&quot;">https://openalex.org/W4415254644&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4415254644&quot;,
      &quot;title&quot;: &quot;SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge&quot;,
      &quot;authors&quot;: [
        &quot;Yujie Xie&quot;,
        &quot;Hongyang Zhang&quot;,
        &quot;Zhihui Liu&quot;,
        &quot;S. N. Ruan&quot;
      ],
      &quot;published&quot;: &quot;2025-09-22&quot;,
      &quot;abstract&quot;: &quot;Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &amp;amp;F in the test-set leaderboard.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.48550/arxiv.2509.17500&quot;">https://doi.org/10.48550/arxiv.2509.17500&quot;</a>,
      &quot;journal&quot;: &quot;arXiv (Cornell University)&quot;,
      &quot;cited_by_count&quot;: 0,
      &quot;pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.17500&quot;">https://arxiv.org/pdf/2509.17500&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://arxiv.org/pdf/2509.17500&quot;">https://arxiv.org/pdf/2509.17500&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="http://arxiv.org/abs/2509.17500&quot;">http://arxiv.org/abs/2509.17500&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;green&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://arxiv.org/pdf/2509.17500&quot;">https://arxiv.org/pdf/2509.17500&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;international conference on computer vision 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4410841160&quot;">https://openalex.org/W4410841160&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4410841160&quot;,
      &quot;title&quot;: &quot;OpenCV for Computer Vision Applications&quot;,
      &quot;authors&quot;: [
        &quot;Ekta Sinha&quot;,
        &quot;Ashwani Kumar&quot;,
        &quot;Abhinav Tyagi&quot;
      ],
      &quot;published&quot;: &quot;2025-05-26&quot;,
      &quot;abstract&quot;: &quot;OpenCV is a powerful, open-source toolkit designed for handling real-time computer vision challenges and advanced image processing tasks. It enables automated systems to interpret images and videos effectively. This paper explores the capabilities of OpenCV for real-time applications such as surveillance, face authentication, edge detection, and mobile video processing. By leveraging OpenCV modules, systems can perform advanced tasks like filtering, transformation, tracking, and detecting features. The discussed applications, implemented in real-world scenarios, demonstrate OpenCV‚Äôs efficiency, scalability, and suitability for intelligent systems across diverse platforms.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.36948/ijfmr.2025.v07i03.44280&quot;">https://doi.org/10.36948/ijfmr.2025.v07i03.44280&quot;</a>,
      &quot;journal&quot;: &quot;International Journal For Multidisciplinary Research&quot;,
      &quot;cited_by_count&quot;: 59,
      &quot;pdf_url&quot;: &quot;<a href="https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;">https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;">https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="https://doi.org/10.36948/ijfmr.2025.v07i03.44280&quot;">https://doi.org/10.36948/ijfmr.2025.v07i03.44280&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;hybrid&quot;,
      &quot;downloaded_pdf_url&quot;: &quot;<a href="https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;">https://www.ijfmr.com/papers/2025/3/44280.pdf&quot;</a>
    }
  },
  {
    &quot;query&quot;: &quot;international conference on computer vision 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4412158322&quot;">https://openalex.org/W4412158322&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4412158322&quot;,
      &quot;title&quot;: &quot;üßúSiren‚Äôs Song in the AI Ocean: A Survey on Hallucination in Large Language Models&quot;,
      &quot;authors&quot;: [
        &quot;Yue Zhang&quot;,
        &quot;Yafu Li&quot;,
        &quot;Leyang Cui&quot;,
        &quot;Cai Deng&quot;,
        &quot;Lemao Liu&quot;,
        &quot;Tingchen Fu&quot;,
        &quot;Xinting Huang&quot;,
        &quot;Enbo Zhao&quot;,
        &quot;Yanwen Zhang&quot;,
        &quot;Yulong Chen&quot;,
        &quot;Longyue Wang&quot;,
        &quot;Ahn Tuan Luu&quot;,
        &quot;Wei Bi&quot;,
        &quot;Freda Shi&quot;,
        &quot;S. Shi&quot;
      ],
      &quot;published&quot;: &quot;2025-07-10&quot;,
      &quot;abstract&quot;: &quot;Abstract While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this article, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.&quot;,
      &quot;doi&quot;: &quot;<a href="https://doi.org/10.1162/coli.a.16&quot;">https://doi.org/10.1162/coli.a.16&quot;</a>,
      &quot;journal&quot;: &quot;Computational Linguistics&quot;,
      &quot;cited_by_count&quot;: 56,
      &quot;pdf_url&quot;: &quot;<a href="https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf&quot;">https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf&quot;</a>,
      &quot;pdf_urls&quot;: [
        &quot;<a href="https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf&quot;">https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf&quot;</a>
      ],
      &quot;landing_page_url&quot;: &quot;<a href="https://doi.org/10.1162/coli.a.16&quot;">https://doi.org/10.1162/coli.a.16&quot;</a>,
      &quot;is_oa&quot;: true,
      &quot;oa_status&quot;: &quot;diamond&quot;
    }
  },
  {
    &quot;query&quot;: &quot;international conference on computer vision 2025&quot;,
    &quot;work&quot;: {
      &quot;openalex_id&quot;: &quot;<a href="https://openalex.org/W4411166620&quot;">https://openalex.org/W4411166620&quot;</a>,
      &quot;openalex_id_short&quot;: &quot;W4411166620&quot;,
      &quot;title&quot;: &quot;Pedagogical Applications of Generative AI in Higher Education: A Systematic Review of the Field&quot;,
      &quot;authors&quot;: [
        &quot;Yufeng Qian&quot;
      ],
      &quot;published&quot;: &quot;2025-06-10&quot;,
      &quot;abstract&quot;: &quot;Abstract The release of ChatGPT in late 2022 marked the beginning of a rapid transformation in higher education, soon followed by the development of multimodal generative AI programs. As this technology becomes increasingly integrated into teaching and learning, it is crucial to evaluate its current use and impact. This systematic literature review captures the initial academic response to generative AI, providing insights into how higher education has adopted this transformative technology in its first two years. The findings indicate that while some themes from the pre-ChatGPT era persist, new and emerging trends‚Äîparticularly in fostering creativity, critical thinking, learning autonomy, and prompt literacy‚Äîare now taking shape. This shift underscores a growing emphasis on the pedagogical integration of generative AI. However, the review also highlights a key tension: while generative AI enhances efficiency, it raises concerns about overreliance, potentially leading to the outsourc</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
