<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/openalex/text/W4414756887.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/openalex/text/W4414756887.txt</h1></header>
  <main><p><em>Truncated view for readability.</em></p><div class="meta-block"><p><strong>Title:</strong> VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results</p><p><strong>Authors:</strong> Yixiao Li, Fang Li, Cunliu Zhou, S. Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan Zhao, Weide Liu, Xiaoyuan Yang, Zhengzhong Tu, Xinyu Li, Chuanbiao Song, Chenqi Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Xiaoyan Sun, Shishun Tian, Dong Yan, Weixia Zhang, Junlin Chen, Wei Sun, Zhihua Wang, Zhuohang Shi, Zhongtao Luo, Hang Ouyang, Tianxin Xiao, Fan Yang, Zhaowang Wu, Kaixin Deng</p><p><strong>Journal:</strong> arXiv (Cornell University)</p><p><strong>Published:</strong> 2025-09-08</p><p><strong>Source:</strong> <a href="http://arxiv.org/abs/2509.06413">http://arxiv.org/abs/2509.06413</a></p><p><strong>PDF:</strong> <a href="../archive/openalex/pdf/W4414756887.pdf">./archive/openalex/pdf/W4414756887.pdf</a></p><p><strong>Summary:</strong><br />This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: <a href="https://github.com/Lighting-YXLI/ISRGen-QA">https://github.com/Lighting-YXLI/ISRGen-QA</a>.</p></div><pre>

===== PAGE 1 =====
VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality
Assessment: Methods and Results
Yixiao Li*
Xin Li*
Chris Wei Zhou*
Shuo Xing
Hadi Amirpour
Xiaoshuai Hao
Guanghui Yue
Baoquan Zhao
Weide Liu
Xiaoyuan Yang
Zhengzhong Tu
Xinyu Li
Chuanbiao Song
Chenqi Zhang
Jun Lan
Huijia Zhu
Weiqiang Wang
Xiaoyan Sun
Shishun Tian
Dongyang Yan
Weixia Zhang
Junlin Chen
Wei Sun
Zhihua Wang
Zhuohang Shi
Zhizun Luo
Hang Ouyang
Tianxin Xiao
Fan Yang
Zhaowang Wu
Kaixin Deng
Abstract
This paper presents the ISRGC-Q Challenge, built upon
the Image Super-Resolution Generated Content Quality As-
sessment (ISRGen-QA) dataset, and organized as part of
the Visual Quality Assessment (VQualA) Competition at the
ICCV 2025 Workshops. Unlike existing Super-Resolution
Image Quality Assessment (SR-IQA) datasets, ISRGen-QA
places a greater emphasis on SR images generated by the
latest generative approaches, including Generative Adver-
sarial Networks (GANs) and diffusion models.
The pri-
mary goal of this challenge is to analyze the unique ar-
tifacts introduced by modern super-resolution techniques
and to evaluate their perceptual quality effectively. A to-
tal of 108 participants registered for the challenge, with 4
teams submitting valid solutions and fact sheets for the fi-
nal testing phase. These submissions demonstrated state-
of-the-art (SOTA) performance on the ISRGen-QA dataset.
The project is publicly available at: <a href="https://github">https://github</a>.
com/Lighting-YXLI/ISRGen-QA.
1. Introduction
Super-resolution (SR) image quality assessment metrics
aim to evaluate the perceptual quality of SR images from
a human-centric perspective. This necessity arises from the
*Yixiao Li (18335310648@163.com), Xin Li (xin.li@ustc.edu.cn), and
Chris Wei Zhou (zhouw26@cardiff.ac.uk) are the challenge organizers.
Shuo Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan
Zhao, Weide Liu, Xiaoyuan Yang, and Zhenzhong Tu are the technical
supporters of the challenge. (Corresponding author: Chris Wei Zhou).
The other authors are participants of the VQualA 2025 Challenge on Image
Super-Resolution Generated Content Quality Assessment.
VQualA webpage: <a href="https://vquala.github.io/">https://vquala.github.io/</a>
The ISRGen-QA dataset:
<a href="https://github.com/Lighting-">https://github.com/Lighting-</a>
YXLI/ISRGen-QA
inherently ill-posed nature of the SR task, where a single
low-resolution image may correspond to multiple plausi-
ble high-resolution reconstructions. As a result, SR faces a
fundamental challenge in balancing fidelity (i.e., similarity
to the ground truth) and naturalness (i.e., perceptual real-
ism) [53, 55]. Moreover, SR images exhibit distinct distor-
tion characteristics that differ significantly from those found
in traditionally degraded images. Conventional distortions
(e.g., blur, noise, and compression artifacts) [56] typically
stem from information loss and lead to perceptual degrada-
tion. In contrast, SR methods often introduce enhancement-
induced artifacts, including over-sharpened edges, halluci-
nated or false textures, and unnatural reconstruction pat-
terns.
Consequently, accurately assessing the perceptual
quality of SR images is crucial [16, 54]—not only for eval-
uating but also for guiding the design and optimization of
next-generation super-resolution algorithms.
To assess the SR-specific distortions, several SR-IQA
datasets have been built, including the QADS [52], Wa-
terloo [41], SISR-IQA [32], CVIU [23], RealSRQ [7], and
SISAR [51]. The QADS [52] database contains 20 original
HR references selected from the MDID database [28], and
980 SR images created by 21 SR algorithms, including 4
interpolation-based, 11 dictionary-based, and 6 DNN-based
SR models, with upsampling factors equaling 2, 3, and 4.
Each SR image is associated with the mean opinion score
(MOS) collected from 100 individuals. In the CVIU [23]
database, 1,620 SR images are produced by 9 SR ap-
proaches from 30 HR references. The HR reference images
are selected from BSD200 according to the PSNR values.
Six pairs of scaling factors (i.e., 2, 3, 4, 5, 6, 8) and kernel
widths (i.e., 0.8, 1.0, 1.2, 1.6, 1.8, 2.0) are adopted, where a
larger subsampling factor corresponds to a larger blur ker-
nel width. Each image is rated by 50 subjects, and the mean
of the median 40 scores is calculated for each image as the
arXiv:2509.06413v1  [cs.CV]  8 Sep 2025


===== PAGE 2 =====
MOS. The Waterloo [41] database involves 13 original HR
references at 512×512 resolution, 39 low-resolution (LR)
references, and 312 interpolated SR images generated by
8 interpolation algorithms, with upsampling factors of 2, 4,
and 8. Subjective scores were collected from 30 participants
aged 20–30 (17 males and 13 females). The SISR-IQA [32]
database contains 15 ground-truth HR images selected from
Set5, Set14, and BSD100, and 360 SR images reconstructed
using 8 SR algorithms (e.g., DRCN [8] and VDSR [9]) with
upsampling factors of 2, 3, and 4. LR images were gen-
erated via nearest neighbor interpolation. Subjective qual-
ity scores for all 360 SR images were collected from 16
participants, who were unaware of the HR references and
SR methods. The RealSRQ [7] database contains 60 real-
world HR references and 180 corresponding LR images at
three scaling factors (2, 3, 4). A total of 1,620 SR images
were generated using 10 SISR algorithms, including 5 non-
deep methods and 5 deep models (SRCNN [5], CSCN [37],
VDSR [9], SRGAN [10], and USRnet [43]). Subjective
scores were collected from 60 participants (32 males and
28 females).
The SISAR [51] database contains 12,600
SR images generated from 100 natural LR images. These
images were processed using 10 SR algorithms or combi-
nations, including 2 interpolation-based, 4 learning-based
(SRCNN [5], VDSR [9], RCAN [50], SAN [3]), and 4 hy-
brid methods (e.g., SRCNN+BICUBIC). SR images were
generated at 6 different scaling factors (i.e., 1.5, 2, 2.7, 3, 4,
3.6). Subjective scores were collected from 23 participants
aged 20–30 with normal vision. Despite the progress in
constructing SR-IQA databases, the super-resolution algo-
rithms employed in these datasets have failed to keep pace
with the rapid advancements in the field. For example, the
most recent methods included are USRNet [43] (2020) and
SAN [3] (2019), with SRGAN being the only generative ad-
versarial network (GAN)-based approach represented. This
lag significantly limits the effectiveness and generalizabil-
ity of the resulting SR-IQA metrics in evaluating modern
SR techniques.
With the rapid advances of the generative methods in
SR tasks, recent SR models have increasingly incorporated
generative priors, particularly through GAN- and diffusion-
based models [11, 15, 25, 40, 46]. While these methods
have shown promising results, striking an effective bal-
ance between perceptual realism and reconstruction fidelity
remains challenging. GAN-based approaches often yield
high-fidelity metrics but may fail to capture vivid textures
due to their unstable adversarial training and tendency to-
ward over-optimization [39]. While diffusion models can
produce detailed textures by leveraging powerful generative
priors, their reliance on stochastic noise sampling and mis-
matches between prior and LR distributions can undermine
pixel-level accuracy [42]. Consequently, the construction of
subjective quality assessment datasets specifically involv-
ing SR images produced by the latest generative models is
of paramount importance for facilitating the further refine-
ment and advancement of SR techniques.
In conjunction with the ICCV 2025 Workshop, we
present the ISRGC-Q Challenge on image super-resolution
generated content quality assessment.
The goal of this
challenge is to automatically evaluate the perceptual qual-
ity of super-resolved (SR) images, ensuring that the pre-
dicted scores align as closely as possible with human vi-
sual perception.
This challenge is one of the VQualA
2025 Workshop associated challenges on: FIQA: Face Im-
age Quality Assessment Challenge [24], ISRGC-Q: Image
Super-Resolution Generated Content Quality Assessment
Challenge [17], EVQA-SnapUGC: Engagement Prediction
for Short Videos Challenge [12], Visual Quality Compari-
son for Large Multimodal Models [57], DIQA: Document
Image Enhancement Quality Assessment Challenge [6],
GenAI-Bench AIGC Video Quality Assessment [2].
In
the following sections, we describe the challenge in detail,
present and analyze the results, and provide an overview of
the participating methods.
2. VQualA 2025 Challenge on ISRGC-Q
The VQualA 2025 Challenge on Image Super-Resolution
Generated Content Quality Assessment (ISRGC-Q) is the
first challenge to be organized to advance the development
of assessing super-resolved images, especially those gen-
erated by the latest GAN- and diffusion-based approaches.
The details of the whole challenge are as follows:
2.1. ISRGen-QA database
ISRGen-QA is a super-resolution (SR) image quality as-
sessment database that contains sufficient SR images gen-
erated by the latest generative models, including GAN-
and diffusion-based methods.
It consists of 720 super-
resolved images at approximately 2K resolution (2040 ×
1152 ∼2040 × 1440), covering four typical upscaling
factors (×2, ×3, ×4, and ×8).
A total of 15 advanced
SR algorithms are used to generate the images, including
4 GAN-based (i.e., ESRGAN [34], Real-ESRGAN [35],
BSRGAN [44], and SeD [11]), 5 diffusion-based (i.e.,
SR3 [26], IDM [4], SRDiff [13], CDFormer [20], and
SAM-DiffSR [33]), 4 transformer-based (i.e., SRNO [38],
ATD-SR [45], SwinIR [18], and CAMixerSR [36]), 1 flow-
based method (i.e., BFSR [30]), and 1 CNN-based method
(i.e., EDSR [19]). The SR images are derived from 19 high-
resolution (HR) reference images and 76 low-resolution
(LR) reference images created via four down-sampling
scales (×2, ×3, ×4, and ×8). The HR references are se-
lected from the DIV2K [1], and the utilized down-sampling
method is the Bicubic. To ensure the reliability of percep-
tual quality annotations, subjective scores were collected
from 23 human participants (11 female, 12 male, from 5


===== PAGE 3 =====
different countries and various ages), with anomaly filter-
ing yielding valid scores from 21 participants. The dataset
is divided into training (576 images, 80%), validation (72
images, 10%), and test (72 images, 10%) sets, facilitating
reproducible model development and benchmarking.
2.2. Evaluation Protocol
This challenge utilized two metrics to measure the cor-
relation of the quality predictions and the mean opinion
scores (MOS), including Spearman rank-order correlation
coefficient (SRCC) and Pearson linear correlation coeffi-
cient (PLCC). SRCC and PLCC are employed to assess the
monotonicity and accuracy of predictions, respectively. An
ideal quality metric would have SRCC and PLCC values
close to one. The final score used for ranking is computed
by reweighting the above metrics as :
Score = 0.6 × SRCC + 0.4 × PLCC.
(1)
2.3. Challenge Phases
There are two phases in this challenge, i.e., the development
and testing phases. The details are as follows:
2.3.1. Development Phase:
In the development phase, we release 576 SR images and
their corresponding high-resolution and low-resolution ref-
erence images in our ISRGen-QA dataset to support each
team in developing their algorithms.
Moreover, we re-
lease 72 SR images without their MOS scores for vali-
dation. Each participant can upload their quality predic-
tions of the validation set to the challenge platform (Co-
dalab: <a href="https://codalab.lisn.upsaclay.fr/">https://codalab.lisn.upsaclay.fr/</a>
competitions/22924). Then they can obtain the cor-
responding final score, SRCC, and PLCC. In the develop-
ment phases, we received 193 submissions from 12 teams
in total.
2.3.2. Testing Phase:
In the testing phases, we release 72 SR images and their
corresponding high-resolution and low-resolution reference
images in our ISRGen-QA dataset for testing. The final
ranking is achieved with the score in Eq. 1. In the test
stage, 5 teams submitted their final results to the challenge
platform. At the end of this competition, we received the
fact sheets and source codes from 4 teams, which are uti-
lized for the final ranking.
3. Challenge Results
The main results from the 4 participating teams (Team
MICV, Team ydy, Team QA-Veteran, and Team 2077
Agent) are summarized in Table 1, as well as the detailed
information on their methods. Figures 1 and 2 present il-
lustrations of the performance achieved by the submitted
methods.
Figure 1. The performance of the methods submitted by different
teams on the testing set.
3.1. Results Analysis
As presented in Table 1, all participating teams demon-
strated exceptional performance, achieving overall scores
exceeding 0.9, which indicates strong alignment with hu-
man perceptual quality judgments.
A consistent pattern
emerges across all teams: PLCC scores consistently exceed
SRCC values (ranging from 0.9476 to 0.9714 for PLCC ver-
sus 0.9096 to 0.9588 for SRCC). This discrepancy reveals
important insights about the methods’ characteristics:
• Linear relationship capture: The higher PLCC scores
indicate that all methods excel at capturing linear correla-
tions between predicted and ground truth quality scores.
• Rank-order consistency: The relatively lower SRCC
scores suggest some challenges in maintaining perfect
monotonic rank consistency across the entire quality
spectrum.
• Practical implications: While methods may occasion-
ally misorder samples with similar quality levels, they
maintain strong overall quality prediction accuracy.
Figure 1 shows the performance histogram comparing
PLCC and SRCC scores across the 4 participating teams.
The consistently high performance scores, tightly clustered
between 0.91 and 0.97, highlight the effectiveness of the
submitted super-resolution quality assessment methods.
Furthermore, Figure 2 shows the scatter plots of pre-
dicted scores versus the MOS for all 4 team methods on the
testing set. The curves are obtained through fourth-order
polynomial nonlinear fitting. We can observe that the pre-
dicted scores obtained by the top-performing team methods
demonstrate higher correlations with the MOS values, as
evidenced by the tighter clustering of data points around the
fitted curves.


===== PAGE 4 =====
Table 1. Quantitative results from the VQualA 2025 Image Super-Resolution Generated Content Quality Assessment Challenge, including
detailed information on the methods used by the 4 participating teams. The best performances are highlighted in bold. Note that GFlops
are calculated relative to Input Size.
Rank Team
Leader
Overall SRCC ↑PLCC ↑Params. (M)
Input Size
GFlops (G) Ensemble Extra Data
1
MICV
Chuanbiao Song
0.9638
0.9588
0.9714
6
(448, 448, 3)
1000
✗
✗
2
ydy
Shishun Tian
0.9429
0.9333
0.9572
161
(128, 128, 3)
6
✗
✗
3
QA-Veteran Weixia Zhang
0.9409
0.9277
0.9608
375.32
(1280, 1280, 3)
428.83
✗
✗
4
2077 Agent
Zhuohang Shi
0.9248
0.9096
0.9476
91.56
(2040, 1152, 3)
322.73
✗
✗
Figure 2. Scatter plots of predicted scores versus MOS for all participating teams on the testing set. The curves are obtained by a fourth-
order polynomial nonlinear fitting.
4. Teams and Methods
4.1. MICV Team
The MICV team proposes the Hybrid Vision Transformer
(ViT) and Convolutional Neural Network (CNN) for SR
Image Quality Assessment [14]. The proposed method fo-
cuses solely on the SR images as input, leveraging the ViT
to capture global dependencies and the CNN to extract spa-
tial features. The architecture of the method is illustrated in
Figure 3. To adaptively model the visual features of SR im-
ages, they introduce a multi-stage attention mechanism that
enhances hierarchical feature fusion through self-attention
and transposed self-attention.
Specifically, the ViT takes the SR image as input and
generates high-level image tokens. These tokens are first
processed by a self-attention module to capture global con-
textual relationships.
Subsequently, the output is trans-
posed and fed into a second self-attention layer, enabling
the</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
