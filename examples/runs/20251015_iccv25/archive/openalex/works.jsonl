{"query": "iccv 2025", "work": {"openalex_id": "https://openalex.org/W4417156065", "openalex_id_short": "W4417156065", "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset", "authors": ["Jonas Mirlach", "Lei Wan", "Andreas Wiedholz", "Hannan Ejaz Keen", "Andreas Eich"], "published": "2025-07-23", "abstract": "The dataset publication to the paper R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception https://arxiv.org/abs/2503.17122. that was accepted at the ICCV 2025. The dataset includes RGB, LiDAR und Thermal data from a roadside perspective for autonomous driving. For more information, we refer to the paper and the READMEs in the data you can download here.", "doi": "https://doi.org/10.5281/zenodo.16356714", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2503.17122", "pdf_urls": ["https://arxiv.org/pdf/2503.17122"], "landing_page_url": "http://arxiv.org/abs/2503.17122", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2503.17122"}}
{"query": "iccv 2025", "work": {"openalex_id": "https://openalex.org/W4417409712", "openalex_id_short": "W4417409712", "title": "VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results", "authors": ["Dasong Li", "Sizhuo Ma", "Hang Hua", "Wenjie Li", "Jianliang Wang", "Cunliu Zhou", "Fengbin Guan", "Xin Li", "Zihao Yu", "Yiting Lu", "Ru-Ling Liao", "Ye Yan", "Zhibo Chen", "Wei Sun", "Linhan Cao", "Yuqin Cao", "Weixia Zhang", "Wen Wen", "Kaiwei Zhang", "Zhong‚ÄêXiu Chen", "Fangfang Lu", "Xiongkuo Min", "Erjia Xiao", "Lingfeng Zhang", "Zhan Su", "Hao Cheng", "Yu Liu", "Renjing Xu", "Long Chen", "Xiaoshuai Hao", "Zhimin Zeng", "Jianqin Wu", "Xuxu Wang", "Qian Yu", "Bo Hu", "Weiwei Wang", "Pinxin Liu", "Yunlong Tang", "Luchuan Song", "Jian‚ÄêJun He", "Jiaru Wu", "Hanjia Lyu", "Lyu, Hanjia"], "published": "2025-09-03", "abstract": "This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.", "doi": "https://doi.org/10.48550/arxiv.2509.02969", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2509.02969", "pdf_urls": ["https://arxiv.org/pdf/2509.02969"], "landing_page_url": "http://arxiv.org/abs/2509.02969", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2509.02969"}}
{"query": "iccv 2025", "work": {"openalex_id": "https://openalex.org/W4415090288", "openalex_id_short": "W4415090288", "title": "The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results", "authors": ["Qiuyu Chen", "Xin Jin", "Yue Song", "Xihui Liu", "Shuai Yang", "Tao Yang", "Ziqiang Li", "Jianguo Huang", "Yuntao Wei", "Bowen Xie", "Nicu Sebe", "Wenjun", "Zeng", "Jooyeol Yun", "Davide Abati", "Mohamed Omran", "Jaegul Choo", "Amir Habibian", "Auke Wiggers", "Masato Kobayashi", "Ning Ding", "Toru Tamaki", "Marzieh Gheisari", "Auguste Genovesio", "Yuheng Chen", "Dingkun Liu", "Xinyao Yang", "Xinping Xu", "Baicheng Chen", "Dongrui Wu", "Junhao Geng", "Lin Lv", "Jianxin Lin", "Han Liang", "Jie Zhou", "Xiaoyuan Chen", "Jinbao Wang", "Can Gao", "Zhangyi Wang", "Zongze Li", "Bihan Wen", "Yanyan Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen", "Bo Peng", "Z. J. Chen", "Haoran Jin"], "published": "2025-08-15", "abstract": "This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop's objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.", "doi": "https://doi.org/10.48550/arxiv.2509.10463", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2509.10463", "pdf_urls": ["https://arxiv.org/pdf/2509.10463"], "landing_page_url": "http://arxiv.org/abs/2509.10463", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2509.10463"}}
{"query": "iccv 2025", "work": {"openalex_id": "https://openalex.org/W4414756887", "openalex_id_short": "W4414756887", "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results", "authors": ["Yixiao Li", "Fang Li", "Cunliu Zhou", "S. Xing", "Hadi Amirpour", "Xiaoshuai Hao", "Guanghui Yue", "Baoquan Zhao", "Weide Liu", "Xiaoyuan Yang", "Zhengzhong Tu", "Xinyu Li", "Chuanbiao Song", "Chenqi Zhang", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Xiaoyan Sun", "Shishun Tian", "Dong Yan", "Weixia Zhang", "Junlin Chen", "Wei Sun", "Zhihua Wang", "Zhuohang Shi", "Zhongtao Luo", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhaowang Wu", "Kaixin Deng"], "published": "2025-09-08", "abstract": "This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.", "doi": "https://doi.org/10.48550/arxiv.2509.06413", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2509.06413", "pdf_urls": ["https://arxiv.org/pdf/2509.06413"], "landing_page_url": "http://arxiv.org/abs/2509.06413", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2509.06413"}}
{"query": "iccv 2025", "work": {"openalex_id": "https://openalex.org/W4415254644", "openalex_id_short": "W4415254644", "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge", "authors": ["Yujie Xie", "Hongyang Zhang", "Zhihui Liu", "S. N. Ruan"], "published": "2025-09-22", "abstract": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &amp;F in the test-set leaderboard.", "doi": "https://doi.org/10.48550/arxiv.2509.17500", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2509.17500", "pdf_urls": ["https://arxiv.org/pdf/2509.17500"], "landing_page_url": "http://arxiv.org/abs/2509.17500", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2509.17500"}}
{"query": "international conference on computer vision 2025", "work": {"openalex_id": "https://openalex.org/W4410841160", "openalex_id_short": "W4410841160", "title": "OpenCV for Computer Vision Applications", "authors": ["Ekta Sinha", "Ashwani Kumar", "Abhinav Tyagi"], "published": "2025-05-26", "abstract": "OpenCV is a powerful, open-source toolkit designed for handling real-time computer vision challenges and advanced image processing tasks. It enables automated systems to interpret images and videos effectively. This paper explores the capabilities of OpenCV for real-time applications such as surveillance, face authentication, edge detection, and mobile video processing. By leveraging OpenCV modules, systems can perform advanced tasks like filtering, transformation, tracking, and detecting features. The discussed applications, implemented in real-world scenarios, demonstrate OpenCV‚Äôs efficiency, scalability, and suitability for intelligent systems across diverse platforms.", "doi": "https://doi.org/10.36948/ijfmr.2025.v07i03.44280", "journal": "International Journal For Multidisciplinary Research", "cited_by_count": 59, "pdf_url": "https://www.ijfmr.com/papers/2025/3/44280.pdf", "pdf_urls": ["https://www.ijfmr.com/papers/2025/3/44280.pdf"], "landing_page_url": "https://doi.org/10.36948/ijfmr.2025.v07i03.44280", "is_oa": true, "oa_status": "hybrid", "downloaded_pdf_url": "https://www.ijfmr.com/papers/2025/3/44280.pdf"}}
{"query": "international conference on computer vision 2025", "work": {"openalex_id": "https://openalex.org/W4412158322", "openalex_id_short": "W4412158322", "title": "üßúSiren‚Äôs Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "authors": ["Yue Zhang", "Yafu Li", "Leyang Cui", "Cai Deng", "Lemao Liu", "Tingchen Fu", "Xinting Huang", "Enbo Zhao", "Yanwen Zhang", "Yulong Chen", "Longyue Wang", "Ahn Tuan Luu", "Wei Bi", "Freda Shi", "S. Shi"], "published": "2025-07-10", "abstract": "Abstract While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this article, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "doi": "https://doi.org/10.1162/coli.a.16", "journal": "Computational Linguistics", "cited_by_count": 56, "pdf_url": "https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf", "pdf_urls": ["https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli.a.16/2535477/coli.a.16.pdf"], "landing_page_url": "https://doi.org/10.1162/coli.a.16", "is_oa": true, "oa_status": "diamond"}}
{"query": "international conference on computer vision 2025", "work": {"openalex_id": "https://openalex.org/W4411166620", "openalex_id_short": "W4411166620", "title": "Pedagogical Applications of Generative AI in Higher Education: A Systematic Review of the Field", "authors": ["Yufeng Qian"], "published": "2025-06-10", "abstract": "Abstract The release of ChatGPT in late 2022 marked the beginning of a rapid transformation in higher education, soon followed by the development of multimodal generative AI programs. As this technology becomes increasingly integrated into teaching and learning, it is crucial to evaluate its current use and impact. This systematic literature review captures the initial academic response to generative AI, providing insights into how higher education has adopted this transformative technology in its first two years. The findings indicate that while some themes from the pre-ChatGPT era persist, new and emerging trends‚Äîparticularly in fostering creativity, critical thinking, learning autonomy, and prompt literacy‚Äîare now taking shape. This shift underscores a growing emphasis on the pedagogical integration of generative AI. However, the review also highlights a key tension: while generative AI enhances efficiency, it raises concerns about overreliance, potentially leading to the outsourcing of critical cognitive and metacognitive skills. To address these challenges and fully harness the potential of generative AI, future research should focus on exploring multimodal generative AI tools and fostering student‚Äìteacher-AI collaboration.", "doi": "https://doi.org/10.1007/s11528-025-01100-1", "journal": "TechTrends", "cited_by_count": 22, "pdf_url": "https://link.springer.com/content/pdf/10.1007/s11528-025-01100-1.pdf", "pdf_urls": ["https://link.springer.com/content/pdf/10.1007/s11528-025-01100-1.pdf"], "landing_page_url": "https://doi.org/10.1007/s11528-025-01100-1", "is_oa": true, "oa_status": "hybrid", "downloaded_pdf_url": "https://link.springer.com/content/pdf/10.1007/s11528-025-01100-1.pdf"}}
{"query": "international conference on computer vision 2025", "work": {"openalex_id": "https://openalex.org/W4411466105", "openalex_id_short": "W4411466105", "title": "Research on Multi-Modal Retrieval System of E-Commerce Platform Based on Pre-Training Model", "authors": ["Bingbing Zhang", "Yi Han", "Xiaofei Han"], "published": "2025-06-12", "abstract": "In this paper, a multi-modal retrieval system for e-commerce platform is proposed, which integrates three advanced pre-training models: BLIP, CLIP and CLIP Interrogator. The system solves the challenge of traditional keyword-based product search by realizing more accurate and efficient graphic matching. We trained and evaluated our approach using 413, 000 image-text pairs from the Google conceptual Captions dataset. Our method introduces a novel feature fusion mechanism and combines the advantages of several pre-trained models to realize comprehensive visual semantic understanding. The system shows strong performance in daily business scenes and complex artistic product description. Experimental results show that our proposed method can effectively generate detailed and context-aware descriptions and accurately match user queries and product pictures. The adaptability and semantic understanding of the system make it of special value in improving the user experience of e-commerce applications. This research has contributed to the development of intelligent shopping platform by bridging the gap between text query and visual content. It is worth emphasizing that the integration of the CLIP model significantly enhances the e-commerce retrieval system's understanding of user intent and product semantics, thereby making product recommendations more accurate and the search process more targeted.", "doi": "https://doi.org/10.70711/aitr.v2i9.6879", "journal": "Artificial intelligence technology research.", "cited_by_count": 18, "pdf_url": "https://ojs.scineer-pub.com/index.php/AITR/article/download/6879/6579", "pdf_urls": ["https://ojs.scineer-pub.com/index.php/AITR/article/download/6879/6579"], "landing_page_url": "https://doi.org/10.70711/aitr.v2i9.6879", "is_oa": true, "oa_status": "diamond", "downloaded_pdf_url": "https://ojs.scineer-pub.com/index.php/AITR/article/download/6879/6579"}}
{"query": "international conference on computer vision 2025", "work": {"openalex_id": "https://openalex.org/W4411100445", "openalex_id_short": "W4411100445", "title": "Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology", "authors": ["Dyke Ferber", "Omar S. M. El Nahhas", "Georg W√∂lflein", "Isabella C. Wiest", "Jan Clusmann", "Marie-Elisabeth Le√ümann", "Sebastian Foersch", "Jacqueline Lammert", "Maximilian Tschochohei", "Dirk Jaeger", "Manuel Salto‚ÄêTellez", "Nikolaus Schultz", "Daniel Truhn", "Jakob Nikolas Kather"], "published": "2025-06-06", "abstract": "Abstract Clinical decision-making in oncology is complex, requiring the integration of multimodal data and multidomain expertise. We developed and evaluated an autonomous clinical artificial intelligence (AI) agent leveraging GPT-4 with multimodal precision oncology tools to support personalized clinical decision-making. The system incorporates vision transformers for detecting microsatellite instability and KRAS and BRAF mutations from histopathology slides, MedSAM for radiological image segmentation and web-based search tools such as OncoKB, PubMed and Google. Evaluated on 20 realistic multimodal patient cases, the AI agent autonomously used appropriate tools with 87.5% accuracy, reached correct clinical conclusions in 91.0% of cases and accurately cited relevant oncology guidelines 75.5% of the time. Compared to GPT-4 alone, the integrated AI agent drastically improved decision-making accuracy from 30.3% to 87.2%. These findings demonstrate that integrating language models with precision oncology and search tools substantially enhances clinical accuracy, establishing a robust foundation for deploying AI-driven personalized oncology support systems.", "doi": "https://doi.org/10.1038/s43018-025-00991-6", "journal": "Nature Cancer", "cited_by_count": 41, "pdf_url": "https://www.nature.com/articles/s43018-025-00991-6.pdf", "pdf_urls": ["https://www.nature.com/articles/s43018-025-00991-6.pdf"], "landing_page_url": "https://doi.org/10.1038/s43018-025-00991-6", "is_oa": true, "oa_status": "hybrid", "downloaded_pdf_url": "https://www.nature.com/articles/s43018-025-00991-6.pdf"}}
{"query": "iccv 2025 open access", "work": {"openalex_id": "https://openalex.org/W4409904069", "openalex_id_short": "W4409904069", "title": "A Survey of the Multi-Sensor Fusion Object Detection Task in Autonomous Driving", "authors": ["Hai Wang", "Junhao Liu", "Haoran Dong", "Zheng Shao"], "published": "2025-04-29", "abstract": "Multi-sensor fusion object detection is an advanced method that improves object recognition and tracking accuracy by integrating data from different types of sensors. As it can overcome the limitations of a single sensor in complex environments, the method has been widely applied in fields such as autonomous driving, intelligent monitoring, robot navigation, drone flight and so on. In the field of autonomous driving, multi-sensor fusion object detection has become a hot research topic. To further explore the future development trends of multi-sensor fusion object detection, we introduce the mainstream framework Transformer model of the multi-sensor fusion object detection algorithm, and we also provide a comprehensive summary of the feature fusion algorithms used in multi-sensor fusion object detection, specifically focusing on the fusion of camera and LiDAR data. This article provides an overview of feature fusion‚Äôs development into feature-level fusion and proposal-level fusion, and it specifically reviews multiple related algorithms. We discuss the application of current multi-sensor object detection algorithms. In the future, with the continuous advancement of sensor technology and the development of artificial intelligence algorithms, multi-sensor fusion object detection will show great potential in more fields.", "doi": "https://doi.org/10.3390/s25092794", "journal": "Sensors", "cited_by_count": 10, "pdf_url": "https://www.mdpi.com/1424-8220/25/9/2794/pdf?version=1745915382", "pdf_urls": ["https://www.mdpi.com/1424-8220/25/9/2794/pdf?version=1745915382"], "landing_page_url": "https://doi.org/10.3390/s25092794", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 open access", "work": {"openalex_id": "https://openalex.org/W4410367179", "openalex_id_short": "W4410367179", "title": "WRRT-DETR: Weather-Robust RT-DETR for Drone-View Object Detection in Adverse Weather", "authors": ["Bei Liu", "Jiangliang Jin", "Yihong Zhang", "Chen Sun"], "published": "2025-05-14", "abstract": "With the rapid advancement of UAV technology, robust object detection under adverse weather conditions has become critical for enhancing UAVs‚Äô environmental perception. However, object detection in such challenging conditions remains a significant hurdle, and standardized evaluation benchmarks are still lacking. To bridge this gap, we introduce the Adverse Weather Object Detection (AWOD) dataset‚Äîa large-scale dataset tailored for object detection in complex maritime environments. The AWOD dataset comprises 20,000 images captured under three representative adverse weather conditions: foggy, flare, and low-light. To address the challenges of scale variation and visual degradation introduced by harsh weather, we propose WRRT-DETR, a weather-robust object detection framework optimized for small objects. Within this framework, we design a gated single-head global‚Äìlocal attention backbone block (GLCE) to fuse local convolutional features with global attention, enhancing small object distinguishability. Additionally, a Frequency‚ÄìSpatial Feature Augmentation Module (FSAE) is introduced to incorporate frequency-domain information for improved robustness, while an Attention-based Cross-Fusion Module (ACFM) facilitates the integration of multi-scale features. Experimental results demonstrate that WRRT-DETR outperforms SOTA methods on the AWOD dataset, exhibiting superior robustness and detection accuracy in complex weather conditions.", "doi": "https://doi.org/10.3390/drones9050369", "journal": "Drones", "cited_by_count": 10, "pdf_url": "https://www.mdpi.com/2504-446X/9/5/369/pdf?version=1747218861", "pdf_urls": ["https://www.mdpi.com/2504-446X/9/5/369/pdf?version=1747218861"], "landing_page_url": "https://doi.org/10.3390/drones9050369", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 open access", "work": {"openalex_id": "https://openalex.org/W4412197197", "openalex_id_short": "W4412197197", "title": "Multimodal Artificial Intelligence in Medical Diagnostics", "authors": ["Bassem Jandoubi", "Moulay A. Akhloufi"], "published": "2025-07-09", "abstract": "The integration of artificial intelligence into healthcare has advanced rapidly in recent years, with multimodal approaches emerging as promising tools for improving diagnostic accuracy and clinical decision making. These approaches combine heterogeneous data sources such as medical images, electronic health records, physiological signals, and clinical notes to better capture the complexity of disease processes. Despite this progress, only a limited number of studies offer a unified view of multimodal AI applications in medicine. In this review, we provide a comprehensive and up-to-date analysis of machine learning and deep learning-based multimodal architectures, fusion strategies, and their performance across a range of diagnostic tasks. We begin by summarizing publicly available datasets and examining the preprocessing pipelines required for harmonizing heterogeneous medical data. We then categorize key fusion strategies used to integrate information from multiple modalities and overview representative model architectures, from hybrid designs and transformer-based vision-language models to optimization-driven and EHR-centric frameworks. Finally, we highlight the challenges present in existing works. Our analysis shows that multimodal approaches tend to outperform unimodal systems in diagnostic performance, robustness, and generalization. This review provides a unified view of the field and opens up future research directions aimed at building clinically usable, interpretable, and scalable multimodal diagnostic systems.", "doi": "https://doi.org/10.3390/info16070591", "journal": "Information", "cited_by_count": 11, "pdf_url": "https://www.mdpi.com/2078-2489/16/7/591/pdf?version=1752112909", "pdf_urls": ["https://www.mdpi.com/2078-2489/16/7/591/pdf?version=1752112909"], "landing_page_url": "https://doi.org/10.3390/info16070591", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 open access", "work": {"openalex_id": "https://openalex.org/W4411143162", "openalex_id_short": "W4411143162", "title": "Vision-Language Modeling Meets Remote Sensing: <i>Models, datasets, and perspectives</i>", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "published": "2025-06-09", "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.", "doi": "https://doi.org/10.1109/mgrs.2025.3572702", "journal": "IEEE Geoscience and Remote Sensing Magazine", "cited_by_count": 5, "pdf_url": "https://arxiv.org/pdf/2505.14361", "pdf_urls": ["https://arxiv.org/pdf/2505.14361"], "landing_page_url": "http://arxiv.org/abs/2505.14361", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2505.14361"}}
{"query": "iccv 2025 open access", "work": {"openalex_id": "https://openalex.org/W4411208469", "openalex_id_short": "W4411208469", "title": "A Survey of Deep Learning-Driven 3D Object Detection: Sensor Modalities, Technical Architectures, and Applications", "authors": ["Xiang Zhang", "Hai Wang", "Haoran Dong"], "published": "2025-06-11", "abstract": "This review presents a comprehensive survey on deep learning-driven 3D object detection, focusing on the synergistic innovation between sensor modalities and technical architectures. Through a dual-axis ‚Äúsensor modality‚Äìtechnical architecture‚Äù classification framework, it systematically analyzes detection methods based on RGB cameras, LiDAR, and multimodal fusion. From the sensor perspective, the study reveals the evolutionary paths of monocular depth estimation optimization, LiDAR point cloud processing from voxel-based to pillar-based modeling, and three-level cross-modal fusion paradigms (data-level alignment, feature-level interaction, and result-level verification). Regarding technical architectures, the paper examines structured representation optimization in traditional convolutional networks, spatiotemporal modeling breakthroughs in bird‚Äôs-eye view (BEV) methods, voxel-level modeling advantages of occupancy networks for irregular objects, and dynamic scene understanding capabilities of temporal fusion architectures. The applications in autonomous driving and agricultural robotics are discussed, highlighting future directions including depth perception enhancement, open-scene modeling, and lightweight deployment to advance 3D perception systems toward higher accuracy and stronger generalization.", "doi": "https://doi.org/10.3390/s25123668", "journal": "Sensors", "cited_by_count": 5, "pdf_url": "https://www.mdpi.com/1424-8220/25/12/3668/pdf?version=1749654582", "pdf_urls": ["https://www.mdpi.com/1424-8220/25/12/3668/pdf?version=1749654582"], "landing_page_url": "https://doi.org/10.3390/s25123668", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 paper code", "work": {"openalex_id": "https://openalex.org/W4410070872", "openalex_id_short": "W4410070872", "title": "YOLOv8n-SMMP: A Lightweight YOLO Forest Fire Detection Model", "authors": ["Nan Zhou", "Demin Gao", "Zhengli Zhu"], "published": "2025-05-03", "abstract": "Global warming has driven a marked increase in forest fire occurrences, underscoring the critical need for timely and accurate detection to mitigate fire-related losses. Existing forest fire detection algorithms face limitations in capturing flame and smoke features in complex natural environments, coupled with high computational complexity and inadequate lightweight design for practical deployment. To address these challenges, this paper proposes an enhanced forest fire detection model, YOLOv8n-SMMP (SlimNeck‚ÄìMCA‚ÄìMPDIoU‚ÄìPruned), based on the YOLO framework. Key innovations include the following: introducing the SlimNeck solution to streamline the neck network by replacing conventional convolutions with Group Shuffling Convolution (GSConv) and substituting the Cross-convolution with 2 filters (C2f) module with the lightweight VoV-based Group Shuffling Cross-Stage Partial Network (VoV-GSCSP) feature extraction module; integrating the Multi-dimensional Collaborative Attention (MCA) mechanism between the neck and head networks to enhance focus on fire-related regions; adopting the Minimum Point Distance Intersection over Union (MPDIoU) loss function to optimize bounding box regression during training; and implementing selective channel pruning tailored to the modified network architecture. The experimental results reveal that, relative to the baseline model, the optimized lightweight model achieves a 3.3% enhancement in detection accuracy (mAP@0.5), slashes the parameter count by 31%, and reduces computational overhead by 33%. These advancements underscore the model‚Äôs superior performance in real-time forest fire detection, outperforming other mainstream lightweight YOLO models in both accuracy and efficiency.", "doi": "https://doi.org/10.3390/fire8050183", "journal": "Fire", "cited_by_count": 9, "pdf_url": "https://www.mdpi.com/2571-6255/8/5/183/pdf?version=1746280182", "pdf_urls": ["https://www.mdpi.com/2571-6255/8/5/183/pdf?version=1746280182"], "landing_page_url": "https://doi.org/10.3390/fire8050183", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 paper code", "work": {"openalex_id": "https://openalex.org/W4409904069", "openalex_id_short": "W4409904069", "title": "A Survey of the Multi-Sensor Fusion Object Detection Task in Autonomous Driving", "authors": ["Hai Wang", "Junhao Liu", "Haoran Dong", "Zheng Shao"], "published": "2025-04-29", "abstract": "Multi-sensor fusion object detection is an advanced method that improves object recognition and tracking accuracy by integrating data from different types of sensors. As it can overcome the limitations of a single sensor in complex environments, the method has been widely applied in fields such as autonomous driving, intelligent monitoring, robot navigation, drone flight and so on. In the field of autonomous driving, multi-sensor fusion object detection has become a hot research topic. To further explore the future development trends of multi-sensor fusion object detection, we introduce the mainstream framework Transformer model of the multi-sensor fusion object detection algorithm, and we also provide a comprehensive summary of the feature fusion algorithms used in multi-sensor fusion object detection, specifically focusing on the fusion of camera and LiDAR data. This article provides an overview of feature fusion‚Äôs development into feature-level fusion and proposal-level fusion, and it specifically reviews multiple related algorithms. We discuss the application of current multi-sensor object detection algorithms. In the future, with the continuous advancement of sensor technology and the development of artificial intelligence algorithms, multi-sensor fusion object detection will show great potential in more fields.", "doi": "https://doi.org/10.3390/s25092794", "journal": "Sensors", "cited_by_count": 10, "pdf_url": "https://www.mdpi.com/1424-8220/25/9/2794/pdf?version=1745915382", "pdf_urls": ["https://www.mdpi.com/1424-8220/25/9/2794/pdf?version=1745915382"], "landing_page_url": "https://doi.org/10.3390/s25092794", "is_oa": true, "oa_status": "gold"}}
{"query": "iccv 2025 paper code", "work": {"openalex_id": "https://openalex.org/W4411143162", "openalex_id_short": "W4411143162", "title": "Vision-Language Modeling Meets Remote Sensing: <i>Models, datasets, and perspectives</i>", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "published": "2025-06-09", "abstract": "Vision-language modeling (VLM) aims to bridge the information gap between images and natural language. Under the new paradigm of first pre-training on massive image-text pairs and then fine-tuning on task-specific data, VLM in the remote sensing domain has made significant progress. The resulting models benefit from the absorption of extensive general knowledge and demonstrate strong performance across a variety of remote sensing data analysis tasks. Moreover, they are capable of interacting with users in a conversational manner. In this paper, we aim to provide the remote sensing community with a timely and comprehensive review of the developments in VLM using the two-stage paradigm. Specifically, we first cover a taxonomy of VLM in remote sensing: contrastive learning, visual instruction tuning, and text-conditioned image generation. For each category, we detail the commonly used network architecture and pre-training objectives. Second, we conduct a thorough review of existing works, examining foundation models and task-specific adaptation methods in contrastive-based VLM, architectural upgrades, training strategies and model capabilities in instruction-based VLM, as well as generative foundation models with their representative downstream applications. Third, we summarize datasets used for VLM pre-training, fine-tuning, and evaluation, with an analysis of their construction methodologies (including image sources and caption generation) and key properties, such as scale and task adaptability. Finally, we conclude this survey with insights and discussions on future research directions: cross-modal representation alignment, vague requirement comprehension, explanation-driven model reliability, continually scalable model capabilities, and large-scale datasets featuring richer modalities and greater challenges.", "doi": "https://doi.org/10.1109/mgrs.2025.3572702", "journal": "IEEE Geoscience and Remote Sensing Magazine", "cited_by_count": 5, "pdf_url": "https://arxiv.org/pdf/2505.14361", "pdf_urls": ["https://arxiv.org/pdf/2505.14361"], "landing_page_url": "http://arxiv.org/abs/2505.14361", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2505.14361"}}
{"query": "iccv 2025 paper code", "work": {"openalex_id": "https://openalex.org/W4409578551", "openalex_id_short": "W4409578551", "title": "Trajectory prediction via proposal guided transformer with out way attention", "authors": ["Wei Xu", "Ruochen Li", "Xiaodong Du", "Bingjie Li", "Lei Xing"], "published": "2025-04-19", "abstract": "The accurate prediction of the behavior of surrounding agents is crucial for the safe operation of autonomous vehicles. Currently, the dominant approach involves manually defining rules, which often fail to cover all potential scenarios. To address the rigidity and challenges of generalizing these rules to real-world driving contexts, we introduce a novel attention mechanism called \"Out Way Attention\". This mechanism improves the model's capacity to dynamically adapt to various driving situations by incorporating attention exits (out way) into the attention framework. Additionally, we present a new trajectory prediction framework that includes a learnable proposal matrix and permutation-invariant positional encoding. This matrix aids in forecasting future multimodal trajectories for multiple interacting agents in dynamic settings. The permutation-invariant positional encoding ensures that the processing sequence of agents at the same time does not influence the prediction outcomes. By integrating the proposed methods into the Transformer architecture, our approach reduces human intervention and significantly enhances the model's adaptability, as well as improving training and inference efficiency. We have validated the effectiveness of our model on three public datasets, Argoverse, Trajnet++, and ETH/UCY. The results confirm that our method sustains robust performance in complex, highly dynamic environments with multiple interacting agents.", "doi": "https://doi.org/10.1038/s41598-025-97244-4", "journal": "Scientific Reports", "cited_by_count": 4, "pdf_url": "https://www.nature.com/articles/s41598-025-97244-4.pdf", "pdf_urls": ["https://www.nature.com/articles/s41598-025-97244-4.pdf"], "landing_page_url": "https://doi.org/10.1038/s41598-025-97244-4", "is_oa": true, "oa_status": "gold", "downloaded_pdf_url": "https://www.nature.com/articles/s41598-025-97244-4.pdf"}}
{"query": "iccv 2025 paper code", "work": {"openalex_id": "https://openalex.org/W4411208469", "openalex_id_short": "W4411208469", "title": "A Survey of Deep Learning-Driven 3D Object Detection: Sensor Modalities, Technical Architectures, and Applications", "authors": ["Xiang Zhang", "Hai Wang", "Haoran Dong"], "published": "2025-06-11", "abstract": "This review presents a comprehensive survey on deep learning-driven 3D object detection, focusing on the synergistic innovation between sensor modalities and technical architectures. Through a dual-axis ‚Äúsensor modality‚Äìtechnical architecture‚Äù classification framework, it systematically analyzes detection methods based on RGB cameras, LiDAR, and multimodal fusion. From the sensor perspective, the study reveals the evolutionary paths of monocular depth estimation optimization, LiDAR point cloud processing from voxel-based to pillar-based modeling, and three-level cross-modal fusion paradigms (data-level alignment, feature-level interaction, and result-level verification). Regarding technical architectures, the paper examines structured representation optimization in traditional convolutional networks, spatiotemporal modeling breakthroughs in bird‚Äôs-eye view (BEV) methods, voxel-level modeling advantages of occupancy networks for irregular objects, and dynamic scene understanding capabilities of temporal fusion architectures. The applications in autonomous driving and agricultural robotics are discussed, highlighting future directions including depth perception enhancement, open-scene modeling, and lightweight deployment to advance 3D perception systems toward higher accuracy and stronger generalization.", "doi": "https://doi.org/10.3390/s25123668", "journal": "Sensors", "cited_by_count": 5, "pdf_url": "https://www.mdpi.com/1424-8220/25/12/3668/pdf?version=1749654582", "pdf_urls": ["https://www.mdpi.com/1424-8220/25/12/3668/pdf?version=1749654582"], "landing_page_url": "https://doi.org/10.3390/s25123668", "is_oa": true, "oa_status": "gold"}}
