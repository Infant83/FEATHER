{"arxiv_id": "2601.05567v1", "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature", "authors": ["Tengxiao Liu", "Deepak Nathani", "Zekun Li", "Kevin Yang", "William Yang Wang"], "published": "2026-01-09T06:35:23+00:00", "updated": "2026-01-09T06:35:23+00:00", "summary": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.", "primary_category": "cs.AI", "categories": ["cs.AI", "cs.CL"], "doi": null, "pdf_url": "https://arxiv.org/pdf/2601.05567v1", "entry_id": "http://arxiv.org/abs/2601.05567v1"}
