<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>archive/arxiv/text/2101.00001v1.txt</title>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    body { font-family: "Iowan Old Style", Georgia, serif; margin: 0; color: #1d1c1a; }
    header { padding: 16px 20px; border-bottom: 1px solid #e7dfd2; background: #f7f4ee; }
    header h1 { margin: 0; font-size: 1.1rem; }
    main { padding: 20px; }
    .meta-block { background: #fdf7ea; border: 1px solid #e7dfd2; padding: 12px 14px; margin-bottom: 16px; }
    .meta-block p { margin: 0 0 6px 0; }
    .meta-block p:last-child { margin-bottom: 0; }
    pre { white-space: pre-wrap; font-family: "SFMono-Regular", Consolas, monospace; font-size: 0.95rem; }
    code { font-family: "SFMono-Regular", Consolas, monospace; }
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #e7dfd2; padding: 8px 10px; text-align: left; }
    th { background: #f6f1e8; }
  </style>
</head>
<body>
  <header><h1>archive/arxiv/text/2101.00001v1.txt</h1></header>
  <main><p><em>Truncated view for readability.</em></p><div class="meta-block"><p><strong>Title:</strong> Etat de l&#x27;art sur l&#x27;application des bandits multi-bras</p><p><strong>Authors:</strong> Djallel Bouneffouf</p><p><strong>Published:</strong> 2021-01-04T18:12:28+00:00</p><p><strong>Source:</strong> <a href="http://arxiv.org/abs/2101.00001v1">http://arxiv.org/abs/2101.00001v1</a></p><p><strong>PDF:</strong> <a href="../archive/arxiv/pdf/2101.00001v1.pdf">./archive/arxiv/pdf/2101.00001v1.pdf</a></p><p><strong>Summary:</strong><br />The Multi-armed bandit offer the advantage to learn and exploit the already learnt knowledge at the same time. This capability allows this approach to be applied in different domains, going from clinical trials where the goal is investigating the effects of different experimental treatments while minimizing patient losses, to adaptive routing where the goal is to minimize the delays in a network. This article provides a review of the recent results on applying bandit to real-life scenario and summarize the state of the art for each of these fields. Different techniques has been proposed to solve this problem setting, like epsilon-greedy, Upper confident bound (UCB) and Thompson Sampling (TS). We are showing here how this algorithms were adapted to solve the different problems of exploration exploitation.</p></div><pre>

===== PAGE 1 =====
arXiv:2101.00001v1  [cs.LG]  4 Jan 2021
Etat de l’art sur l’application des bandits
multi-bras
1st Djallel Bouneffouf
IBM Research
Djallel.bouneffouf@ibm.com
New York, USA
Abstract—Le domaine des bandits multi-bras connaˆıt
actuellement une renaissance, alors que de nouveaux
param`etres de probl`emes et des algorithmes motiv´es par di-
verses applications pratiques sont introduits, en s’ajoutant
au probl`eme classique des bandits. Cet article vise `a
fournir un examen complet des principaux d´eveloppements
r´ecents dans de multiples applications r´eelles des bandits.
Plus pr´ecis´ement, nous introduisons une taxonomie des
applications communes bas´ees sur le MAB et r´esumons
l’´etat de l’art pour chacun de ces domaines. De plus,
nous identiﬁons les tendances actuelles importantes et
fournissons de nouvelles perspectives concernant l’avenir
de ce domaine en plein essor.
Index Terms—bandit
I. INTRODUCTION
De nombreuses applications pratiques n´ecessitent des
probl`emes de prise de d´ecision s´equentiels, o`u un agent
doit choisir la meilleure action parmi plusieurs alterna-
tives. Des exemples de telles applications incluent les
essais cliniques [1], syst`emes de recommandation [2]–
[11], [11]–[19], [19]–[45], [45]–[92] et la d´etection
d’anomalies [93]. Dans certains cas, des informations
secondaires ou un contexte sont associ´es `a chaque
action (par exemple, le proﬁl d’un utilisateur), et le
retour d’information, ou r´ecompense, est limit´e `a l’option
choisie. Par exemple, dans les essais cliniques, le con-
texte est le dossier m´edical du patient (par exemple,
´etat de sant´e, ant´ec´edents familiaux, etc.), les actions
correspondent aux options de traitement compar´ees et la
r´ecompense repr´esente le r´esultat du traitement propos´e
(par exemple, succ`es ou ´echec). Un aspect important
affectant le succ`es `a long terme dans de tels contextes est
de trouver un bon compromis entre l’exploration (par ex-
emple, essayer une nouveau traitement) et l’exploitation
(choisir le traitement le plus connue `a ce jour).
Ce
compromis
inh´erent
entre
l’exploration
et
l’exploitation existe dans de nombreux probl`emes de
prise de d´ecision s´equentiels, et est traditionnellement
Identify applicable funding agency here. If none, delete this.
formul´e comme le probl`eme des bandit, qui se pr´esente
comme suit:
´Etant donn´e K actions possibles, ou
”bras”, chacun associ´e `a une distribution de probabilit´e
de r´ecompense ﬁxe mais inconnue [94], [95], `a chaque
it´eration, un agent s´electionne un bras `a jouer et rec¸oit
une r´ecompense, ´echantillonn´ee `a partir de la distribution
de probabilit´e du bras respectif ind´ependamment des
actions pr´ec´edentes. La tˆache d’un agent est d’apprendre
`a choisir ses actions aﬁn que les r´ecompenses cumul´ees
au ﬁl du temps soient maximis´ees.
Notez que l’agent doit essayer diff´erentes bras pour
apprendre leurs r´ecompenses (c’est-`a-dire explorer le
gain), et ´egalement utiliser ces informations apprises aﬁn
de recevoir le meilleur gain (exploiter les gains appris).
Il existe un compromis naturel entre l’exploration et
l’exploitation. Par exemple, essayer chaque bras exacte-
ment une fois, puis jouer le meilleur d’entre. Cette
approche est souvent susceptible de conduire `a des
solutions tr`es sous-optimales lorsque les r´ecompenses
des bras sont incertaines. Diff´erentes solutions ont ´et´e
propos´ees pour ce probl`eme, bas´ees sur une formulation
stochastique
[94]–[96] et une formulation bay´esienne
[97]; cependant, ces approches ne tenaient pas compte du
contexte ou des informations secondaires dont disposait
l’agent.
Il est `a not´e que le probl`eme des bandits peut ˆetre
vu comme la forme la plus simple d’apprentissage
par renforcement, dans laquelle l’agent est sans ´etat.
Lorsque le syst`eme a des ´etats, les actions provoquent
des changements d’´etats et les r´ecompenses d´ependent
´egalement des ´etats. Par cons´equent, dans l’apprentissage
par renforcement, les r´ecompenses `a diff´erentes ´etapes
ne sont pas ind´ependantes les unes des autres. En
fait, les algorithmes classiques pour l’apprentissage par
renforcement (avec ´etats) utilisent souvent des solu-
tions au probl`eme des bandits multi-bras comme sous-
programmes pour d´eﬁnir des politiques exploration ex-
ploitation dans l’apprentissage par renforcement. Par ex-


===== PAGE 2 =====
emple, il est bien connu ǫ-greedy L’algorithme de bandits
multi-bras est souvent combin´e avec l’algorithme de pro-
grammation dynamique de Bellman pour l’apprentissage
par renforcement aﬁn de d´eﬁnir les choix d’actions.
En outre, de nombreux algorithmes d’apprentissage par
renforcement, lorsqu’ils sont appliqu´es `a des syst`emes
sans ´etat, se r´eduisent `a des algorithmes de bandit multi-
bras.
Une version particuli`erement utile du MAB est le
contextual multi-arm bandit (CMAB), ou simplement le
contextual bandit, o`u `a chaque it´eration, avant de choisir
un bras, l’agent observe un N-dimensions du contexte,
ou vecteur de features. L’agent utilise ce contexte, ainsi
que les r´ecompenses des bras jou´ees dans le pass´e,
pour choisir quel bras jouer dans l’it´eration actuelle.
Au ﬁl du temps, le but de l’agent est de collecter
sufﬁsamment d’informations sur la relation entre les
vecteurs de contexte et les r´ecompenses, aﬁn qu’il puisse
pr´edire le prochain meilleur bras `a jouer en regardant
le contexte actuel [98], [99]. Diff´erents algorithmes ont
´et´e propos´es pour le cas g´en´eral, dont LINUCB [100],
Neural Bandit [101] et Contextual Thompson Sampling
(CTS) [99], o`u une d´ependance lin´eaire est g´en´eralement
suppos´ee entre la r´ecompense attendue d’une action et
son contexte.
Nous allons maintenant fournir un aperc¸u des di-
verses applications du bandit au probl`emes de la vie
r´eelle (sant´e, r´eseau informatique, ﬁnance, et au-del`a),
ainsi qu’en apprentissage automatique. En particulier,
lorsque les approches bandit peuvent aider `a am´eliorer
le r´eglage des hyperparam`etres et d’autres choix al-
gorithmiques importants dans l’apprentissage supervis´e,
l’apprentissage actif et l’apprentissage par renforcement.
II. APPLICATIONS DES BANDITS
Le bandit stochastique aborde les d´eﬁs associ´es `a
la pr´esence d’incertitude dans la prise de d´ecision
s´equentielle. Ce type d’incertitude a une interaction
complexe avec le dilemme de l’exploration exploitation
et fournit donc un formalisme naturel pour la plupart des
probl`emes de prise de d´ecision.
A. Sant´e
Essais cliniques.
La collecte de donn´ees pour
´evaluer l’efﬁcacit´e du traitement sur des animaux pen-
dant tous les stades de la maladie peut ˆetre difﬁ-
cile lors de l’utilisation de proc´edures conventionnelles
d’allocation de traitement al´eatoire, car de mauvais
choix de traitements peuvent entraˆıner une d´et´erioration
de la sant´e du sujet. Les auteurs de [1] visent `a
concevoir une strat´egie d’allocation adaptative pour
am´eliorer l’efﬁcacit´e de la collecte de donn´ees en al-
louant plus d’´echantillons pour explorer des traitements
prometteurs. Ils pr´esentent cette application comme un
probl`eme de bandit contextuel et introduisent un algo-
rithme pratique d’exploration exploitation dans ce cadre.
Le travail repose sur le sous-´echantillonnage pour com-
parer les options de traitement en utilisant une quantit´e
´equivalente d’informations. Ils ´etendent la strat´egie de
sous-´echantillonnage au contexte de bandit contextuel
en appliquant un sous-´echantillonnage dans la r´egression
avec processus gaussien.
Warfarine est l’anticoagulant oral le plus utilis´e dans
le monde; cependant, l’administration d’un dosage pr´ecis
reste un d´eﬁimportant, car le dosage appropri´e peut
ˆetre tr`es variable entre les individus en raison de divers
facteurs cliniques, d´emographiques et g´en´etiques. Les
m´edecins suivent actuellement une strat´egie `a dose ﬁxe:
les patients commencent avec une dose de 5 mg / jour
(ce qui est la posologie appropri´ee pour la majorit´e
des patients) et ajustent lentement la dose au cours de
quelques semaines en suivant les taux d’anticoagulant
du patient. Cependant, une posologie initiale incorrecte
peut entraˆıner des cons´equences tr`es n´efastes telles qu’un
accident vasculaire c´er´ebral (si la dose initiale est trop
faible) ou une h´emorragie interne (si la dose initiale
est trop ´elev´ee). Ainsi, les auteurs de [102] abordent
le probl`eme de l’apprentissage et de l’attribution d’un
dosage initial appropri´e aux patients en mod´elisant le
probl`eme comme un bandit avec des covariables de haute
dimension, et proposent un nouvel algorithme de bandit
efﬁcace bas´e sur l’estimateur LASSO.
Mod´elisation du cerveau et du comportement.
S’inspirant des ´etudes comportementales de la prise
de d´ecision humaine chez les patients souffrant de
diff´erents troubles mentaux, les auteurs de [43] proposent
un cadre param´etrique g´en´eral pour le probl`eme des
bandits qui ´etend l’approche standard d’´echantillonnage
de Thompson pour incorporer les biais de traitement
des r´ecompenses associ´es `a plusieurs conditions neu-
rologiques et psychiatriques, y compris les maladies
de Parkinson et d’Alzheimer, le trouble de d´eﬁcit de
l’attention/ hyperactivit´e (TDAH), la d´ependance et la
douleur chronique. Ils d´emontrent empiriquement, du
point de vue de la mod´elisation comportementale, que
leur model peut ˆetre consid´er´e comme une premi`ere
´etape vers un mod`ele de calcul uniﬁcateur capturant les
anomalies du traitement des r´ecompenses dans plusieurs
conditions mentales.
2


===== PAGE 3 =====
B. La ﬁnance
Ces derni`eres ann´ees, la s´election s´equentielle de
portefeuilles a suscit´e un int´erˆet croissant `a l’intersection
de l’apprentissage automatique et de la ﬁnance quantita-
tive. Le compromis entre l’exploration et l’exploitation,
dans le but de maximiser la r´ecompense cumulative,
est une formulation naturelle des probl`emes de choix
de portefeuille. Dans [103], les auteurs ont propos´e un
algorithme de bandit pour faire des choix de portefeuille
en ligne en exploitant les corr´elations entre plusieurs
bras. En construisant des portefeuilles orthogonaux `a
partir de plusieurs actifs et en int´egrant leur approche
au cadre des bandits, les auteurs d´erivent la strat´egie
de portefeuille optimale repr´esentant une combinaison
d’investissements passifs et actifs selon une fonction de
r´ecompense ajust´ee au risque. Dans [104], les auteurs
int`egrent la conscience du risque dans le cadre classique
du bandit et introduisent un nouvel algorithme pour
la construction de portefeuille. En ﬁltrant les actifs en
fonction de la structure topologique du march´e ﬁnancier
et en combinant la politique optimale de bandit avec la
minimisation d’une mesure de risque, ils parviennent `a
un ´equilibre entre le risque et le rendement.
C. Tariﬁcation dynamique
Les entreprises de vente en ligne sont souvent
confront´ees au probl`eme de tariﬁcation dynamique:
l’entreprise doit d´ecider des prix en temps r´eel pour
chacun de ses multiples produits. L’entreprise peut mener
des exp´eriences de prix (faire des changements de prix
fr´equents) pour se renseigner sur la demande et max-
imiser les proﬁts `a long terme. Les auteurs de [105]
proposent une politique d’exp´erimentation dynamique
des prix, o`u l’entreprise ne dispose que d’informations
incompl`etes sur la demande. Pour ce param`etre g´en´eral,
les auteurs d´erivent un algorithme de tariﬁcation qui
´equilibre le fait de gagner un proﬁt imm´ediat par rapport
`a l’apprentissage pour les b´en´eﬁces futurs. L’approche
combine un bandit avec une identiﬁcation partielle de
la demande des consommateurs `a partir de la th´eorie
´economique. Semblable `a [105], les auteurs de [106]
consid`erent la tariﬁcation multi-produits dynamique de
haute dimension avec un mod`ele de demande lin´eaire de
faible dimension ´evolutif. Ils montrent que le probl`eme
de maximisation des revenus se r´eduit `a une optimi-
sation convexe de bandit en ligne avec des informa-
tions secondaires donn´ees par les demandes observ´ees.
L’approche applique un algorithme d’optimisation con-
vexe de bandit dans un espace projet´e de faible di-
mension couvert par les caract´eristiques du produit la-
tent, tout en apprenant simultan´ement cette dur´ee via
la d´ecomposition en valeur singuli`ere en ligne d’une
matrice contenant les demandes observ´ees.
D. Syst`emes de recommandation
Les syst`emes de recommandation sont fr´equemment
utilis´es dans diverses applications pour pr´edire les
pr´ef´erences
de
l’utilisateur.
Cependant,
ils
sont
´egalement
confront´es
au
dilemme
exploration-
exploitation lorsqu’ils font une recommandation, car ils
doivent exploiter leurs connaissances sur les ´el´ements
pr´ec´edemment choisis qui int´eressent l’utilisateur, tout
en explorant de nouveaux ´el´ements susceptibles de
plaire `a l’utilisateur. Les auteurs de [107] abordent ce
d´eﬁen utilisant le param`etre bandit, en particulier pour
les syst`emes de recommandation `a grande ´echelle qui
ont un nombre vraiment grand ou inﬁni d’´el´ements. Ils
proposent deux approches de bandit `a grande ´echelle
dans des situations o`u aucune information pr´ealable
n’est disponible. Une exploration continue de leurs
approches peut r´esoudre le probl`eme du d´emarrage
`a froid dans les syst`emes de recommandation. Dans
les
syst`emes
de
recommandation
contextuels,
la
plupart des approches existantes se concentrent sur la
recommandation d’´el´ements pertinents aux utilisateurs,
en tenant compte des informations contextuelles, telles
que l’heure, le lieu ou les aspects sociaux. Cependant,
aucune de ces approches n’a pris en compte le probl`eme
de l’´evolution du contenu des utilisateurs. Dans [87],
les auteurs introduisent un algorithme qui prend en
compte cette dynamique. Il est bas´e sur une exploration
/ exploitation dynamique et peut ´equilibrer de mani`ere
adaptative les deux aspects, en d´ecidant quelle situation
est la plus pertinente pour l’exploration ou l’exploitation.
En ce sens, [27] propose d’´etudier la ”fraˆıcheur” du
contenu de l’utilisateur
`a travers le probl`eme du
bandit. Ils introduisent l’algorithme Freshness-Aware
Thompson
Sampling
pour
la
recommandation
de
nouveaux documents.
E. Maximisation de l’inﬂuence
Les auteurs de [108] consid`erent la maximisation
de l’inﬂuence (IM) dans les r´eseaux sociaux, qui est
le probl`eme de maximiser le nombre d’utilisateurs qui
prennent conscience d’un produit en s´electionnant un
ensemble d’utilisateurs auxquels exposer le produit. Ils
proposent une nouvelle param´etrisation qui rend non
seulement le cadre ind´ependant du mod`ele de diffusion
sous-jacent, mais aussi statistiquement efﬁcace pour ap-
prendre des donn´ees.
Ils donnent une fonction de substitution monotone et
submodulaire correspondante, et montrent qu’il s’agit
3


===== PAGE 4 =====
d’une bonne approximation de l’objectif original de
la MI. Ils consid`erent ´egalement le cas d’un nouveau
marketeur cherchant `a exploiter un r´eseau social ex-
istant, tout en apprenant simultan´ement les facteurs
r´egissant la propagation de l’information. Pour cela, ils
d´eveloppent un algorithme de bandit bas´e sur LinUCB.
Les auteurs de [109] ´etudient ´egalement le probl`eme de
maximisation de l’inﬂuence en ligne dans les r´eseaux
sociaux mais sous le mod`ele de cascade ind´ependant.
Plus pr´ecis´ement, ils essaient d’apprendre l’ensemble
des ”meilleures graines ou inﬂuenceurs” dans un r´eseau
social en ligne tout en interagissant `a plusieurs reprises
avec lui. Ils abordent les d´eﬁs de l’espace d’action
combinatoire, car le nombre d’ensembles d’inﬂuenceurs
r´ealisables augmente de mani`ere exponentielle avec le
nombre maximum d’inﬂuenceurs et un retour limit´e, car
seule la partie inﬂuenc´ee du r´eseau est observ´ee.
F. R´ecup´eration de l’information
Les auteurs de [110] soutiennent que le proces-
sus de s´election it´erative de recherche d’informations
peut ˆetre naturellement mod´elis´e comme un probl`eme
de bandit contextuel. Le mo</pre></main>
  <script>
    document.querySelectorAll('a').forEach((link) => {
      link.setAttribute('target', '_blank');
      link.setAttribute('rel', 'noopener');
    });
  </script>
</body>
</html>
