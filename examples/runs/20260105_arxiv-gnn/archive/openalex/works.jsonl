{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7116276642", "openalex_id_short": "W7116276642", "title": "Graph Neural Networks for Interferometer Simulations", "authors": ["Kannan, Sidharth", "Goodarzi, Pooyan", "Papalexakis, Evangelos E.", "Richardson, Jonathan W."], "published": "2025-12-18", "abstract": "In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.", "doi": "https://doi.org/10.48550/arxiv.2512.16051", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": null, "pdf_urls": [], "landing_page_url": "https://doi.org/10.48550/arxiv.2512.16051", "is_oa": true, "oa_status": "green"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W4417529673", "openalex_id_short": "W4417529673", "title": "Graph Neural Networks for Interferometer Simulations", "authors": ["S. Kannan", "P. Goodarzi", "Evangelos E. Papalexakis", "Jonathan W. Richardson"], "published": "2025-12-18", "abstract": "In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.", "doi": "https://doi.org/10.48550/arxiv.2512.16051", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2512.16051", "pdf_urls": ["https://arxiv.org/pdf/2512.16051"], "landing_page_url": "http://arxiv.org/abs/2512.16051", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2512.16051"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7116584439", "openalex_id_short": "W7116584439", "title": "Graph Neural Networks for Interferometer Simulations", "authors": ["Kannan, Sidharth", "Goodarzi, Pooyan", "Papalexakis, Evangelos E.", "Richardson, Jonathan W."], "published": "2025-12-18", "abstract": "In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.", "doi": null, "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2512.16051", "pdf_urls": ["https://arxiv.org/pdf/2512.16051"], "landing_page_url": "http://arxiv.org/abs/2512.16051", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2512.16051"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7115590550", "openalex_id_short": "W7115590550", "title": "chutongjia/CLGNN: Contrastive learning graph neural network", "authors": ["chutongjia"], "published": "2025-12-15", "abstract": "A Contrastive Learning Graph Neural Network (CLGNN) model to predict the interaction coefficients between fatty acids (drugs) and PDAC through multi-scale graph embeddings from CDKG.", "doi": "https://doi.org/10.5281/zenodo.17935355", "journal": "Zenodo (CERN European Organization for Nuclear Research)", "cited_by_count": 0, "pdf_url": null, "pdf_urls": [], "landing_page_url": "https://doi.org/10.5281/zenodo.17935355", "is_oa": true, "oa_status": "green"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7115601538", "openalex_id_short": "W7115601538", "title": "chutongjia/CLGNN: Contrastive learning graph neural network", "authors": ["chutongjia"], "published": "2025-12-15", "abstract": "A Contrastive Learning Graph Neural Network (CLGNN) model to predict the interaction coefficients between fatty acids (drugs) and PDAC through multi-scale graph embeddings from CDKG.", "doi": "https://doi.org/10.5281/zenodo.17935354", "journal": "Zenodo (CERN European Organization for Nuclear Research)", "cited_by_count": 0, "pdf_url": null, "pdf_urls": [], "landing_page_url": "https://doi.org/10.5281/zenodo.17935354", "is_oa": true, "oa_status": "green"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W4417330266", "openalex_id_short": "W4417330266", "title": "Flavour Tagging with Graph Neural Network with the ATLAS Detector", "authors": ["Helena Santos", "ATLAS Collaboration"], "published": "2025-12-15", "abstract": "The identification of jets containing b-hadrons is key to many physics analyses at the Large Hadron Collider, including measurements involving Higgs bosons or top quarks, and searches for physics beyond the Standard Model. In this contribution, the most recent enhancements in the capability of ATLAS to separate b-jets from jets stemming from lighter quarks will be presented. The improved performance originates from the usage of state-of-the-art machine learning algorithms based on graph neural networks. A factor of more than two to reject light- and c-quark-initiated jets is observed compared to the current performance. Perspectives for the High-Luminosity LHC will be discussed.", "doi": "https://doi.org/10.22323/1.512.0080", "journal": null, "cited_by_count": 0, "pdf_url": "https://pos.sissa.it/512/080/pdf", "pdf_urls": ["https://pos.sissa.it/512/080/pdf"], "landing_page_url": "https://doi.org/10.22323/1.512.0080", "is_oa": true, "oa_status": "gold", "downloaded_pdf_url": "https://pos.sissa.it/512/080/pdf"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7119235243", "openalex_id_short": "W7119235243", "title": "Learning from Historical Activations in Graph Neural Networks", "authors": ["Yaniv Galron", "Hadar Sinai", "Haggai Maron", "Moshe Eliasof"], "published": "2026-01-03", "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.", "doi": null, "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": "https://arxiv.org/pdf/2601.01123", "pdf_urls": ["https://arxiv.org/pdf/2601.01123"], "landing_page_url": "http://arxiv.org/abs/2601.01123", "is_oa": true, "oa_status": "green", "downloaded_pdf_url": "https://arxiv.org/pdf/2601.01123"}}
{"query": "graph neural networks", "work": {"openalex_id": "https://openalex.org/W7119230132", "openalex_id_short": "W7119230132", "title": "Learning from Historical Activations in Graph Neural Networks", "authors": ["Yaniv Galron", "Hadar Sinai", "Haggai Maron", "Moshe Eliasof"], "published": "2026-01-03", "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.", "doi": "https://doi.org/10.48550/arxiv.2601.01123", "journal": "arXiv (Cornell University)", "cited_by_count": 0, "pdf_url": null, "pdf_urls": [], "landing_page_url": "https://doi.org/10.48550/arxiv.2601.01123", "is_oa": true, "oa_status": "green"}}
