#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from pathlib import Path

from federlicht import report as report_mod
from federlicht.quality_profiles import (
    normalize_quality_profile,
    quality_profile_choices,
    resolve_quality_gate_targets,
)


def _to_float(value: object, default: float = 0.0) -> float:
    try:
        return float(value)
    except Exception:
        return default


def _average(rows: list[dict], key: str) -> float:
    if not rows:
        return 0.0
    return sum(_to_float(row.get(key), 0.0) for row in rows) / float(len(rows))


def _summary(rows: list[dict]) -> dict[str, float]:
    return {
        "overall": _average(rows, "overall"),
        "claim_support_ratio": _average(rows, "claim_support_ratio"),
        "unsupported_claim_count": _average(rows, "unsupported_claim_count"),
        "section_coherence_score": _average(rows, "section_coherence_score"),
    }


def _load_rows(payload: object) -> list[dict]:
    if isinstance(payload, list):
        return [row for row in payload if isinstance(row, dict)]
    if isinstance(payload, dict):
        rows = payload.get("rows")
        if isinstance(rows, list):
            return [row for row in rows if isinstance(row, dict)]
        summary = payload.get("summary")
        if isinstance(summary, dict):
            return [summary]
    return []


def run_gate(
    rows: list[dict],
    *,
    min_overall: float,
    min_claim_support: float,
    max_unsupported: float,
    min_section_coherence: float,
) -> tuple[bool, list[str]]:
    summary = _summary(rows)
    errors = report_mod.quality_gate_failures(
        summary,
        min_overall=min_overall,
        min_claim_support=min_claim_support,
        max_unsupported=max_unsupported,
        min_section_coherence=min_section_coherence,
    )
    errors = [f"avg_{item}" for item in errors]
    return (len(errors) == 0), errors


def main() -> int:
    parser = argparse.ArgumentParser(description="Quality regression gate for benchmark JSON output.")
    parser.add_argument("--input", required=True, help="Benchmark JSON path generated by report_quality_benchmark.py")
    parser.add_argument(
        "--quality-profile",
        default="baseline",
        type=normalize_quality_profile,
        choices=list(quality_profile_choices()),
        help=(
            "Gate threshold preset: none/smoke/baseline/professional/deep_research "
            "(default: baseline)."
        ),
    )
    parser.add_argument("--min-overall", type=float, default=0.0)
    parser.add_argument("--min-claim-support", type=float, default=0.0)
    parser.add_argument("--max-unsupported", type=float, default=-1.0)
    parser.add_argument("--min-section-coherence", type=float, default=0.0)
    args = parser.parse_args()

    input_path = Path(args.input)
    payload = json.loads(input_path.read_text(encoding="utf-8"))
    rows = _load_rows(payload)
    if not rows:
        raise ValueError("input JSON must contain row list or {'rows': [...]} bundle")
    policy = resolve_quality_gate_targets(
        profile=args.quality_profile,
        min_overall=float(args.min_overall),
        min_claim_support=float(args.min_claim_support),
        max_unsupported=float(args.max_unsupported),
        min_section_coherence=float(args.min_section_coherence),
    )
    targets = dict(policy.get("thresholds") or {})
    ok, errors = run_gate(
        rows,
        min_overall=float(targets.get("min_overall", 0.0)),
        min_claim_support=float(targets.get("min_claim_support", 0.0)),
        max_unsupported=float(targets.get("max_unsupported", -1.0)),
        min_section_coherence=float(targets.get("min_section_coherence", 0.0)),
    )
    summary = _summary(rows)
    print(
        "gate-policy | "
        f"profile={policy.get('profile')} | "
        f"effective_band={policy.get('effective_band')} | "
        f"source={policy.get('source')} | "
        f"targets={targets}"
    )
    print(
        "gate-summary | "
        f"overall={summary['overall']:.2f} | "
        f"claim_support={summary['claim_support_ratio']:.2f} | "
        f"unsupported={summary['unsupported_claim_count']:.2f} | "
        f"section_coherence={summary['section_coherence_score']:.2f}"
    )
    if ok:
        print("gate-result | PASS")
        return 0
    print("gate-result | FAIL")
    for item in errors:
        print(f"- {item}")
    return 2


if __name__ == "__main__":
    raise SystemExit(main())
